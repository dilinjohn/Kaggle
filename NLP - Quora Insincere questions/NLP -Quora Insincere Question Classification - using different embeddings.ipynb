{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/wikinews300d1mvec/wiki-news-300d-1M.vec\n",
      "/kaggle/input/paragram-300-sl999/paragram_300_sl999.txt\n",
      "/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\n",
      "/kaggle/input/quora-insincere-questions-classification/sample_submission.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/embeddings.zip\n",
      "/kaggle/input/quora-insincere-questions-classification/test.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/train.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1306122, 3)\n",
      "(375806, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\n",
    "test_df = pd.read_csv('../input/quora-insincere-questions-classification/test.csv')\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Check for data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5b6e983350>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(train_df.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qid              0\n",
       "question_text    0\n",
       "target           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qid              0\n",
       "question_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Do the following in the next steps:\n",
    "\n",
    "* Split the train into train and validation sets. We will not do cross valdiation as it is time consuming\n",
    "* No missing values identified, if any, replace them with 'na'\n",
    "* Tokenize the `text` column and convert them to vector sequences\n",
    "* Pad or truncate the sequences as required - truncation happens when sequence length exceeds `max_len` and if less, they will be padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 291 ms, sys: 14.8 ms, total: 306 ms\n",
      "Wall time: 313 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# split into train and validation sets\n",
    "\n",
    "train_df, valid_df = train_test_split(train_df, test_size=0.1, random_state=2020)\n",
    "\n",
    "# some config values for tokenization and vector sequences\n",
    "\n",
    "max_len = 100        # max number of words in the question that will be used\n",
    "max_features = 50000 # max number of features or unique words we will use from the entire corpus (the same as number of rows in the embedding matrix)\n",
    "embed_size = 300     # size of each embedding or word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 19s, sys: 898 ms, total: 1min 20s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# substitute missing values in text\n",
    "train_X = train_df['question_text'].fillna(\"_na_\")\n",
    "valid_X = valid_df['question_text'].fillna(\"_na_\")\n",
    "test_X = test_df['question_text'].fillna(\"_na_\")\n",
    "\n",
    "# tokenize the sentences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_features)\n",
    "\n",
    "# fit on train_X. fit_on_texts() expects a list of text as input\n",
    "tokenizer.fit_on_texts(train_X.values.tolist())\n",
    "\n",
    "# use fit_to_sequences method to encode the tokens or words into interger sequences\n",
    "train_X = tokenizer.texts_to_sequences(train_X.values)\n",
    "valid_X = tokenizer.texts_to_sequences(valid_X.values)\n",
    "test_X = tokenizer.texts_to_sequences(test_X.values)\n",
    "\n",
    "# pad the sentences, according to the max_len\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "train_X = pad_sequences(train_X, maxlen = max_len)\n",
    "valid_X = pad_sequences(valid_X, maxlen = max_len)\n",
    "test_X = pad_sequences(test_X, maxlen = max_len)\n",
    "\n",
    "# store the target values\n",
    "train_y = train_df['target'].values\n",
    "valid_y = valid_df['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Without using any of the trained embeddings provided:\n",
    "We will learn the embeddings from scratch without using any of the trained embeddings as the first step. For this we will train a Bidirectional GRU model. We will use the CUDA based NN model from Nvidia known as th CuDNNGRU, which is GPU based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Embedding, CuDNNGRU, Dense, Dropout, LSTM, Conv1D, Activation, GRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor flow version in use:  2.2.0\n"
     ]
    }
   ],
   "source": [
    "# tensorflow version check\n",
    "import tensorflow as tf\n",
    "print('tensor flow version in use: ', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Using the CuDNNGRU is not importable as it seems to have been deprecated in recent versions of TF and I keep getting error 'ModuleNotFoundError: No module named 'tensorflow.contrib'', this is because this model is removed in the current version.\n",
    "\n",
    "The better way is to simply use the GRU and or LSTM as it is, which will default to `CuDNNGRU` and `CuDNNLSTM`, if certain conditions are met. Refer [here](https://stackoverflow.com/questions/60468385/is-there-cudnnlstm-or-cudnngru-alternative-in-tensorflow-2-0)\n",
    "\n",
    "Refer to other links on how to import a `GRU` model:\n",
    "1. https://www.tensorflow.org/api_docs/python/tf/compat/v1/keras/layers/CuDNNGRU\n",
    "2. https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 128)          140160    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 15,142,241\n",
      "Trainable params: 15,142,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = max_features, output_dim = embed_size, input_length = max_len)) # input_shape is optional\n",
    "model.add(Bidirectional(GRU(64, return_sequences=True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Train the model on the given train split and monitor the metric on the validation set. Run for 1 epochs for now. Changing the epochs, batch_size, model parameters could give a better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1175509 samples, validate on 130613 samples\n",
      "Epoch 1/1\n",
      "1175509/1175509 [==============================] - 1069s 909us/step - loss: 0.1241 - accuracy: 0.9533 - val_loss: 0.1084 - val_accuracy: 0.9565\n",
      "CPU times: user 26min 17s, sys: 4min 8s, total: 30min 25s\n",
      "Wall time: 17min 50s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f5a987e58d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# train the model\n",
    "model.fit(train_X, train_y, batch_size=512, epochs =1 , validation_data = (valid_X, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open('bidirectional_gru_keras_no_pretrain_embed.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "*let's looks at the validation sample predictions and decide the best threshold for `F1` score. The actual values are `0` and `1`. Based on threshold, the predictions will vary.*\n",
    "\n",
    "Here we are learning embeddings and we don't use any of the pretrained embeddings that have been provided along with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130613/130613 [==============================] - 5s 38us/step\n",
      "F1 score at threshold 0.1 is 0.565\n",
      "F1 score at threshold 0.11 is 0.5724\n",
      "F1 score at threshold 0.12 is 0.5788\n",
      "F1 score at threshold 0.13 is 0.5846\n",
      "F1 score at threshold 0.14 is 0.5895\n",
      "F1 score at threshold 0.15 is 0.5956\n",
      "F1 score at threshold 0.16 is 0.5992\n",
      "F1 score at threshold 0.17 is 0.6042\n",
      "F1 score at threshold 0.18 is 0.6071\n",
      "F1 score at threshold 0.19 is 0.6103\n",
      "F1 score at threshold 0.2 is 0.6131\n",
      "F1 score at threshold 0.21 is 0.6158\n",
      "F1 score at threshold 0.22 is 0.618\n",
      "F1 score at threshold 0.23 is 0.6212\n",
      "F1 score at threshold 0.24 is 0.6235\n",
      "F1 score at threshold 0.25 is 0.6256\n",
      "F1 score at threshold 0.26 is 0.6285\n",
      "F1 score at threshold 0.27 is 0.631\n",
      "F1 score at threshold 0.28 is 0.6326\n",
      "F1 score at threshold 0.29 is 0.6344\n",
      "F1 score at threshold 0.3 is 0.6377\n",
      "F1 score at threshold 0.31 is 0.6402\n",
      "F1 score at threshold 0.32 is 0.6414\n",
      "F1 score at threshold 0.33 is 0.6422\n",
      "F1 score at threshold 0.34 is 0.6422\n",
      "F1 score at threshold 0.35 is 0.6428\n",
      "F1 score at threshold 0.36 is 0.6434\n",
      "F1 score at threshold 0.37 is 0.6446\n",
      "F1 score at threshold 0.38 is 0.6455\n",
      "F1 score at threshold 0.39 is 0.6444\n",
      "F1 score at threshold 0.4 is 0.644\n",
      "F1 score at threshold 0.41 is 0.6436\n",
      "F1 score at threshold 0.42 is 0.6451\n",
      "F1 score at threshold 0.43 is 0.6449\n",
      "F1 score at threshold 0.44 is 0.6438\n",
      "F1 score at threshold 0.45 is 0.6438\n",
      "F1 score at threshold 0.46 is 0.642\n",
      "F1 score at threshold 0.47 is 0.6399\n",
      "F1 score at threshold 0.48 is 0.6377\n",
      "F1 score at threshold 0.49 is 0.636\n"
     ]
    }
   ],
   "source": [
    "prediction_noembed_val_y = model.predict([valid_X], batch_size=1024, verbose=1)\n",
    "\n",
    "for thresh in np.arange(0.1, 0.5, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    score = round(metrics.f1_score(valid_y, (prediction_noembed_val_y > thresh).astype(int)),4)\n",
    "    print('F1 score at threshold {} is {}'.format(thresh, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375806/375806 [==============================] - 14s 37us/step\n"
     ]
    }
   ],
   "source": [
    "# predictions on test(no pretrained embedding in use)\n",
    "pred_noembed_y = model.predict([test_X], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "\n",
    "# pause execution for 10s after gc to allow gc to complete\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Imported Glove840b300d embedding file from kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Examine the contents of the embedding file. Each `line` represents a `token` and the correspoding `300D vector embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", -0.082752 0.67204 -0.14987 -0.064983 0.056491 0.40228 0.0027747 -0.3311 -0.30691 2.0817 0.031819 0.013643 0.30265 0.0071297 -0.5819 -0.2774 -0.062254 1.1451 -0.24232 0.1235 -0.12243 0.33152 -0.006162 -0.30541 -0.13057 -0.054601 0.037083 -0.070552 0.5893 -0.30385 0.2898 -0.14653 -0.27052 0.37161 0.32031 -0.29125 0.0052483 -0.13212 -0.052736 0.087349 -0.26668 -0.16897 0.015162 -0.0083746 -0.14871 0.23413 -0.20719 -0.091386 0.40075 -0.17223 0.18145 0.37586 -0.28682 0.37289 -0.16185 0.18008 0.3032 -0.13216 0.18352 0.095759 0.094916 0.008289 0.11761 0.34046 0.03677 -0.29077 0.058303 -0.027814 0.082941 0.1862 -0.031494 0.27985 -0.074412 -0.13762 -0.21866 0.18138 0.040855 -0.113 0.24107 0.3657 -0.27525 -0.05684 0.34872 0.011884 0.14517 -0.71395 0.48497 0.14807 0.62287 0.20599 0.58379 -0.13438 0.40207 0.18311 0.28021 -0.42349 -0.25626 0.17715 -0.54095 0.16596 -0.036058 0.08499 -0.64989 0.075549 -0.28831 0.40626 -0.2802 0.094062 0.32406 0.28437 -0.26341 0.11553 0.071918 -0.47215 -0.18366 -0.34709 0.29964 -0.66514 0.002516 -0.42333 0.27512 0.36012 0.16311 0.23964 -0.05923 0.3261 0.20559 0.038677 -0.045816 0.089764 0.43151 -0.15954 0.08532 -0.26572 -0.15001 0.084286 -0.16714 -0.43004 0.060807 0.13121 -0.24112 0.66554 0.4453 -0.18019 -0.13919 0.56252 0.21457 -0.46443 -0.012211 0.029988 -0.051094 -0.20135 0.80788 0.47377 -0.057647 0.46216 0.16084 -0.20954 -0.05452 0.15572 -0.13712 0.12972 -0.011936 -0.003378 -0.13595 -0.080711 0.20065 0.054056 0.046816 0.059539 0.046265 0.17754 -0.31094 0.28119 -0.24355 0.085252 -0.21011 -0.19472 0.0027297 -0.46341 0.14789 -0.31517 -0.065939 0.036106 0.42903 -0.33759 0.16432 0.32568 -0.050392 -0.054297 0.24074 0.41923 0.13012 -0.17167 -0.37808 -0.23089 -0.019477 -0.29291 -0.30824 0.30297 -0.22659 0.081574 -0.18516 -0.21408 0.40616 -0.28974 0.074174 -0.17795 0.28595 -0.039626 -0.2339 -0.36054 -0.067503 -0.091065 0.23438 -0.0041331 0.003232 0.0072134 0.008697 0.21614 0.049904 0.35582 0.13748 0.073361 0.14166 0.2412 -0.013322 0.15613 0.083381 0.088146 -0.019357 0.43795 0.083961 0.45309 -0.50489 -0.10865 -0.2527 -0.18251 0.20441 0.13319 0.1294 0.050594 -0.15612 -0.39543 0.12538 0.24881 -0.1927 -0.31847 -0.12719 0.4341 0.31177 -0.0040946 -0.2094 -0.079961 0.1161 -0.050794 0.015266 -0.2803 -0.12486 0.23587 0.2339 -0.14023 0.028462 0.56923 -0.1649 -0.036429 0.010051 -0.17107 -0.042608 0.044965 -0.4393 -0.26137 0.30088 -0.060772 -0.45312 -0.19076 -0.20288 0.27694 -0.060888 0.11944 0.62206 -0.19343 0.47849 -0.30113 0.059389 0.074901 0.061068 -0.4662 0.40054 -0.19099 -0.14331 0.018267 -0.18643 0.20709 -0.35598 0.05338 -0.050821 -0.1918 -0.37846 -0.06589\n",
      "\n",
      ". 0.012001 0.20751 -0.12578 -0.59325 0.12525 0.15975 0.13748 -0.33157 -0.13694 1.7893 -0.47094 0.70434 0.26673 -0.089961 -0.18168 0.067226 0.053347 1.5595 -0.2541 0.038413 -0.01409 0.056774 0.023434 0.024042 0.31703 0.19025 -0.37505 0.035603 0.1181 0.012032 -0.037566 -0.5046 -0.049261 0.092351 0.11031 -0.073062 0.33994 0.28239 0.13413 0.070128 -0.022099 -0.28103 0.49607 -0.48693 -0.090964 -0.1538 -0.38011 -0.014228 -0.19392 -0.11068 -0.014088 -0.17906 0.24509 -0.16878 -0.15351 -0.13808 0.02151 0.13699 0.0068061 -0.14915 -0.38169 0.12727 0.44007 0.32678 -0.46117 0.068687 0.34747 0.18827 -0.31837 0.4447 -0.2095 -0.26987 0.48945 0.15388 0.05295 -0.049831 0.11207 0.14881 -0.37003 0.30777 -0.33865 0.045149 -0.18987 0.26634 -0.26401 -0.47556 0.68381 -0.30653 0.24606 0.31611 -0.071098 0.030417 0.088119 0.045025 0.20125 -0.21618 -0.36371 -0.25948 -0.42398 -0.14305 -0.10208 0.21498 -0.21924 -0.17935 0.21546 0.13801 0.24504 -0.2559 0.054815 0.21307 0.2564 -0.25673 0.17961 -0.47638 -0.25181 -0.0091498 -0.054362 -0.21007 0.12597 -0.40795 -0.021164 0.20585 0.18925 -0.0051896 -0.51394 0.28862 -0.077748 -0.27676 0.46567 -0.14225 -0.17879 -0.4357 -0.32481 0.15034 -0.058367 0.49652 0.20472 0.019866 0.13326 0.12823 -1.0177 0.29007 0.28995 0.029994 -0.10763 0.28665 -0.24387 0.22905 -0.26249 -0.069269 -0.17889 0.21936 0.15146 0.04567 -0.050497 0.071482 -0.1027 -0.080705 0.30296 0.031302 0.26613 -0.0060951 0.10313 -0.39987 -0.043945 -0.057625 0.08702 -0.098152 0.22835 -0.005211 0.038075 0.01591 -0.20622 0.021853 0.0040426 -0.043063 -0.002294 -0.26097 -0.25802 -0.28158 -0.23118 -0.010404 -0.30102 -0.4042 0.014653 -0.10445 0.30377 -0.20957 0.3119 0.068272 0.1008 0.010423 0.54011 0.29865 0.12653 0.013761 0.21738 -0.39521 0.066633 0.50327 0.14913 -0.11554 0.010042 0.095698 0.16607 -0.18808 0.055019 0.026715 -0.3164 -0.046583 -0.051591 0.023475 -0.11007 0.085642 0.28394 0.040497 0.071986 0.14157 -0.021199 0.44718 0.20088 -0.12964 -0.067183 0.47614 0.13394 -0.17287 -0.37324 -0.17285 0.02683 -0.1316 0.09116 -0.46487 0.1274 -0.090159 -0.10552 0.068006 -0.13381 0.17056 0.089509 -0.23133 -0.27572 0.061534 -0.051646 0.28377 0.25286 -0.24139 -0.19905 0.12049 -0.1011 0.27392 0.27843 0.26449 -0.18292 -0.048961 0.19198 0.17192 0.33659 -0.20184 -0.34305 -0.24553 -0.15399 0.3945 0.22839 -0.25753 -0.25675 -0.37332 -0.23884 -0.048816 0.78323 0.18851 -0.26477 0.096566 0.062658 -0.30668 -0.43334 0.10006 0.21136 0.039459 -0.11077 0.24421 0.60942 -0.46646 0.086385 -0.39702 -0.23363 0.021307 -0.10778 -0.2281 0.50803 0.11567 0.16165 -0.066737 -0.29556 0.022612 -0.28135 0.0635 0.14019 0.13871 -0.36049 -0.035\n",
      "\n",
      "the 0.27204 -0.06203 -0.1884 0.023225 -0.018158 0.0067192 -0.13877 0.17708 0.17709 2.5882 -0.35179 -0.17312 0.43285 -0.10708 0.15006 -0.19982 -0.19093 1.1871 -0.16207 -0.23538 0.003664 -0.19156 -0.085662 0.039199 -0.066449 -0.04209 -0.19122 0.011679 -0.37138 0.21886 0.0011423 0.4319 -0.14205 0.38059 0.30654 0.020167 -0.18316 -0.0065186 -0.0080549 -0.12063 0.027507 0.29839 -0.22896 -0.22882 0.14671 -0.076301 -0.1268 -0.0066651 -0.052795 0.14258 0.1561 0.05551 -0.16149 0.09629 -0.076533 -0.049971 -0.010195 -0.047641 -0.16679 -0.2394 0.0050141 -0.049175 0.013338 0.41923 -0.10104 0.015111 -0.077706 -0.13471 0.119 0.10802 0.21061 -0.051904 0.18527 0.17856 0.041293 -0.014385 -0.082567 -0.035483 -0.076173 -0.045367 0.089281 0.33672 -0.22099 -0.0067275 0.23983 -0.23147 -0.88592 0.091297 -0.012123 0.013233 -0.25799 -0.02972 0.016754 0.01369 0.32377 0.039546 0.042114 -0.088243 0.30318 0.087747 0.16346 -0.40485 -0.043845 -0.040697 0.20936 -0.77795 0.2997 0.2334 0.14891 -0.39037 -0.053086 0.062922 0.065663 -0.13906 0.094193 0.10344 -0.2797 0.28905 -0.32161 0.020687 0.063254 -0.23257 -0.4352 -0.017049 -0.32744 -0.047064 -0.075149 -0.18788 -0.015017 0.029342 -0.3527 -0.044278 -0.13507 -0.11644 -0.1043 0.1392 0.0039199 0.37603 0.067217 -0.37992 -1.1241 -0.057357 -0.16826 0.03941 0.2604 -0.023866 0.17963 0.13553 0.2139 0.052633 -0.25033 -0.11307 0.22234 0.066597 -0.11161 0.062438 -0.27972 0.19878 -0.36262 -1.0006e-05 -0.17262 0.29166 -0.15723 0.054295 0.06101 -0.39165 0.2766 0.057816 0.39709 0.025229 0.24672 -0.08905 0.15683 -0.2096 -0.22196 0.052394 -0.01136 0.050417 -0.14023 -0.042825 -0.031931 -0.21336 -0.20402 -0.23272 0.07449 0.088202 -0.11063 -0.33526 -0.014028 -0.29429 -0.086911 -0.1321 -0.43616 0.20513 0.0079362 0.48505 0.064237 0.14261 -0.43711 0.12783 -0.13111 0.24673 -0.27496 0.15896 0.43314 0.090286 0.24662 0.066463 -0.20099 0.1101 0.03644 0.17359 -0.15689 -0.086328 -0.17316 0.36975 -0.40317 -0.064814 -0.034166 -0.013773 0.062854 -0.17183 -0.12366 -0.034663 -0.22793 -0.23172 0.239 0.27473 0.15332 0.10661 -0.060982 -0.024805 -0.13478 0.17932 -0.37374 -0.02893 -0.11142 -0.08389 -0.055932 0.068039 -0.10783 0.1465 0.094617 -0.084554 0.067429 -0.3291 0.034082 -0.16747 -0.25997 -0.22917 0.020159 -0.02758 0.16136 -0.18538 0.037665 0.57603 0.20684 0.27941 0.16477 -0.018769 0.12062 0.069648 0.059022 -0.23154 0.24095 -0.3471 0.04854 -0.056502 0.41566 -0.43194 0.4823 -0.051759 -0.27285 -0.25893 0.16555 -0.1831 -0.06734 0.42457 0.010346 0.14237 0.25939 0.17123 -0.13821 -0.066846 0.015981 -0.30193 0.043579 -0.043102 0.35025 -0.19681 -0.4281 0.16899 0.22511 -0.28557 -0.1028 -0.018168 0.11407 0.13015 -0.18317 0.1323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_FILE = '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "\n",
    "with open(EMBEDDING_FILE, 'r', encoding='utf8') as fp:\n",
    "    for i in range(3):\n",
    "        print(fp.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "For details on how what the `*` operator means, refer [here](https://stackoverflow.com/questions/11315010/what-do-and-before-a-variable-name-mean-in-a-function-signature)\n",
    "\n",
    "Inside a function header:\n",
    "\n",
    "* `*` unpacks a list or tuple into position arguments.\n",
    "\n",
    "* `**` unpacks a dictionary into keyword arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "**I got infinity when computing the `standard deviation` after stacking the embedding values when using dtype as 'float16', but if using 'float32', this does not happen, in the below function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the embeddings and the token from the text file\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". 0.012001 0.20751 -0.12578 -0.59325 0.12525 0.15975 0.13748 -0.33157 -0.13694 1.7893 -0.47094 0.70434 0.26673 -0.089961 -0.18168 0.067226 0.053347 1.5595 -0.2541 0.038413 -0.01409 0.056774 0.023434 0.024042 0.31703 0.19025 -0.37505 0.035603 0.1181 0.012032 -0.037566 -0.5046 -0.049261 0.092351 0.11031 -0.073062 0.33994 0.28239 0.13413 0.070128 -0.022099 -0.28103 0.49607 -0.48693 -0.090964 -0.1538 -0.38011 -0.014228 -0.19392 -0.11068 -0.014088 -0.17906 0.24509 -0.16878 -0.15351 -0.13808 0.02151 0.13699 0.0068061 -0.14915 -0.38169 0.12727 0.44007 0.32678 -0.46117 0.068687 0.34747 0.18827 -0.31837 0.4447 -0.2095 -0.26987 0.48945 0.15388 0.05295 -0.049831 0.11207 0.14881 -0.37003 0.30777 -0.33865 0.045149 -0.18987 0.26634 -0.26401 -0.47556 0.68381 -0.30653 0.24606 0.31611 -0.071098 0.030417 0.088119 0.045025 0.20125 -0.21618 -0.36371 -0.25948 -0.42398 -0.14305 -0.10208 0.21498 -0.21924 -0.17935 0.21546 0.13801 0.24504 -0.2559 0.054815 0.21307 0.2564 -0.25673 0.17961 -0.47638 -0.25181 -0.0091498 -0.054362 -0.21007 0.12597 -0.40795 -0.021164 0.20585 0.18925 -0.0051896 -0.51394 0.28862 -0.077748 -0.27676 0.46567 -0.14225 -0.17879 -0.4357 -0.32481 0.15034 -0.058367 0.49652 0.20472 0.019866 0.13326 0.12823 -1.0177 0.29007 0.28995 0.029994 -0.10763 0.28665 -0.24387 0.22905 -0.26249 -0.069269 -0.17889 0.21936 0.15146 0.04567 -0.050497 0.071482 -0.1027 -0.080705 0.30296 0.031302 0.26613 -0.0060951 0.10313 -0.39987 -0.043945 -0.057625 0.08702 -0.098152 0.22835 -0.005211 0.038075 0.01591 -0.20622 0.021853 0.0040426 -0.043063 -0.002294 -0.26097 -0.25802 -0.28158 -0.23118 -0.010404 -0.30102 -0.4042 0.014653 -0.10445 0.30377 -0.20957 0.3119 0.068272 0.1008 0.010423 0.54011 0.29865 0.12653 0.013761 0.21738 -0.39521 0.066633 0.50327 0.14913 -0.11554 0.010042 0.095698 0.16607 -0.18808 0.055019 0.026715 -0.3164 -0.046583 -0.051591 0.023475 -0.11007 0.085642 0.28394 0.040497 0.071986 0.14157 -0.021199 0.44718 0.20088 -0.12964 -0.067183 0.47614 0.13394 -0.17287 -0.37324 -0.17285 0.02683 -0.1316 0.09116 -0.46487 0.1274 -0.090159 -0.10552 0.068006 -0.13381 0.17056 0.089509 -0.23133 -0.27572 0.061534 -0.051646 0.28377 0.25286 -0.24139 -0.19905 0.12049 -0.1011 0.27392 0.27843 0.26449 -0.18292 -0.048961 0.19198 0.17192 0.33659 -0.20184 -0.34305 -0.24553 -0.15399 0.3945 0.22839 -0.25753 -0.25675 -0.37332 -0.23884 -0.048816 0.78323 0.18851 -0.26477 0.096566 0.062658 -0.30668 -0.43334 0.10006 0.21136 0.039459 -0.11077 0.24421 0.60942 -0.46646 0.086385 -0.39702 -0.23363 0.021307 -0.10778 -0.2281 0.50803 0.11567 0.16165 -0.066737 -0.29556 0.022612 -0.28135 0.0635 0.14019 0.13871 -0.36049 -0.035\n",
      "\n",
      "('.', array([ 0.012001 ,  0.20751  , -0.12578  , -0.59325  ,  0.12525  ,\n",
      "        0.15975  ,  0.13748  , -0.33157  , -0.13694  ,  1.7893   ,\n",
      "       -0.47094  ,  0.70434  ,  0.26673  , -0.089961 , -0.18168  ,\n",
      "        0.067226 ,  0.053347 ,  1.5595   , -0.2541   ,  0.038413 ,\n",
      "       -0.01409  ,  0.056774 ,  0.023434 ,  0.024042 ,  0.31703  ,\n",
      "        0.19025  , -0.37505  ,  0.035603 ,  0.1181   ,  0.012032 ,\n",
      "       -0.037566 , -0.5046   , -0.049261 ,  0.092351 ,  0.11031  ,\n",
      "       -0.073062 ,  0.33994  ,  0.28239  ,  0.13413  ,  0.070128 ,\n",
      "       -0.022099 , -0.28103  ,  0.49607  , -0.48693  , -0.090964 ,\n",
      "       -0.1538   , -0.38011  , -0.014228 , -0.19392  , -0.11068  ,\n",
      "       -0.014088 , -0.17906  ,  0.24509  , -0.16878  , -0.15351  ,\n",
      "       -0.13808  ,  0.02151  ,  0.13699  ,  0.0068061, -0.14915  ,\n",
      "       -0.38169  ,  0.12727  ,  0.44007  ,  0.32678  , -0.46117  ,\n",
      "        0.068687 ,  0.34747  ,  0.18827  , -0.31837  ,  0.4447   ,\n",
      "       -0.2095   , -0.26987  ,  0.48945  ,  0.15388  ,  0.05295  ,\n",
      "       -0.049831 ,  0.11207  ,  0.14881  , -0.37003  ,  0.30777  ,\n",
      "       -0.33865  ,  0.045149 , -0.18987  ,  0.26634  , -0.26401  ,\n",
      "       -0.47556  ,  0.68381  , -0.30653  ,  0.24606  ,  0.31611  ,\n",
      "       -0.071098 ,  0.030417 ,  0.088119 ,  0.045025 ,  0.20125  ,\n",
      "       -0.21618  , -0.36371  , -0.25948  , -0.42398  , -0.14305  ,\n",
      "       -0.10208  ,  0.21498  , -0.21924  , -0.17935  ,  0.21546  ,\n",
      "        0.13801  ,  0.24504  , -0.2559   ,  0.054815 ,  0.21307  ,\n",
      "        0.2564   , -0.25673  ,  0.17961  , -0.47638  , -0.25181  ,\n",
      "       -0.0091498, -0.054362 , -0.21007  ,  0.12597  , -0.40795  ,\n",
      "       -0.021164 ,  0.20585  ,  0.18925  , -0.0051896, -0.51394  ,\n",
      "        0.28862  , -0.077748 , -0.27676  ,  0.46567  , -0.14225  ,\n",
      "       -0.17879  , -0.4357   , -0.32481  ,  0.15034  , -0.058367 ,\n",
      "        0.49652  ,  0.20472  ,  0.019866 ,  0.13326  ,  0.12823  ,\n",
      "       -1.0177   ,  0.29007  ,  0.28995  ,  0.029994 , -0.10763  ,\n",
      "        0.28665  , -0.24387  ,  0.22905  , -0.26249  , -0.069269 ,\n",
      "       -0.17889  ,  0.21936  ,  0.15146  ,  0.04567  , -0.050497 ,\n",
      "        0.071482 , -0.1027   , -0.080705 ,  0.30296  ,  0.031302 ,\n",
      "        0.26613  , -0.0060951,  0.10313  , -0.39987  , -0.043945 ,\n",
      "       -0.057625 ,  0.08702  , -0.098152 ,  0.22835  , -0.005211 ,\n",
      "        0.038075 ,  0.01591  , -0.20622  ,  0.021853 ,  0.0040426,\n",
      "       -0.043063 , -0.002294 , -0.26097  , -0.25802  , -0.28158  ,\n",
      "       -0.23118  , -0.010404 , -0.30102  , -0.4042   ,  0.014653 ,\n",
      "       -0.10445  ,  0.30377  , -0.20957  ,  0.3119   ,  0.068272 ,\n",
      "        0.1008   ,  0.010423 ,  0.54011  ,  0.29865  ,  0.12653  ,\n",
      "        0.013761 ,  0.21738  , -0.39521  ,  0.066633 ,  0.50327  ,\n",
      "        0.14913  , -0.11554  ,  0.010042 ,  0.095698 ,  0.16607  ,\n",
      "       -0.18808  ,  0.055019 ,  0.026715 , -0.3164   , -0.046583 ,\n",
      "       -0.051591 ,  0.023475 , -0.11007  ,  0.085642 ,  0.28394  ,\n",
      "        0.040497 ,  0.071986 ,  0.14157  , -0.021199 ,  0.44718  ,\n",
      "        0.20088  , -0.12964  , -0.067183 ,  0.47614  ,  0.13394  ,\n",
      "       -0.17287  , -0.37324  , -0.17285  ,  0.02683  , -0.1316   ,\n",
      "        0.09116  , -0.46487  ,  0.1274   , -0.090159 , -0.10552  ,\n",
      "        0.068006 , -0.13381  ,  0.17056  ,  0.089509 , -0.23133  ,\n",
      "       -0.27572  ,  0.061534 , -0.051646 ,  0.28377  ,  0.25286  ,\n",
      "       -0.24139  , -0.19905  ,  0.12049  , -0.1011   ,  0.27392  ,\n",
      "        0.27843  ,  0.26449  , -0.18292  , -0.048961 ,  0.19198  ,\n",
      "        0.17192  ,  0.33659  , -0.20184  , -0.34305  , -0.24553  ,\n",
      "       -0.15399  ,  0.3945   ,  0.22839  , -0.25753  , -0.25675  ,\n",
      "       -0.37332  , -0.23884  , -0.048816 ,  0.78323  ,  0.18851  ,\n",
      "       -0.26477  ,  0.096566 ,  0.062658 , -0.30668  , -0.43334  ,\n",
      "        0.10006  ,  0.21136  ,  0.039459 , -0.11077  ,  0.24421  ,\n",
      "        0.60942  , -0.46646  ,  0.086385 , -0.39702  , -0.23363  ,\n",
      "        0.021307 , -0.10778  , -0.2281   ,  0.50803  ,  0.11567  ,\n",
      "        0.16165  , -0.066737 , -0.29556  ,  0.022612 , -0.28135  ,\n",
      "        0.0635   ,  0.14019  ,  0.13871  , -0.36049  , -0.035    ],\n",
      "      dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# explore how we can use the * operator to unpack the list into a tuple\n",
    "# do the same for the rest of tokens in each line of the file\n",
    "\n",
    "with open(EMBEDDING_FILE, 'r', encoding='utf8') as fp:\n",
    "    firstline = fp.readline()\n",
    "    secondline = fp.readline()\n",
    "    print(secondline)\n",
    "    print(get_coefs(*secondline.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(',', array([-0.082752 ,  0.67204  , -0.14987  , -0.064983 ,  0.056491 ,\n",
      "        0.40228  ,  0.0027747, -0.3311   , -0.30691  ,  2.0817   ,\n",
      "        0.031819 ,  0.013643 ,  0.30265  ,  0.0071297, -0.5819   ,\n",
      "       -0.2774   , -0.062254 ,  1.1451   , -0.24232  ,  0.1235   ,\n",
      "       -0.12243  ,  0.33152  , -0.006162 , -0.30541  , -0.13057  ,\n",
      "       -0.054601 ,  0.037083 , -0.070552 ,  0.5893   , -0.30385  ,\n",
      "        0.2898   , -0.14653  , -0.27052  ,  0.37161  ,  0.32031  ,\n",
      "       -0.29125  ,  0.0052483, -0.13212  , -0.052736 ,  0.087349 ,\n",
      "       -0.26668  , -0.16897  ,  0.015162 , -0.0083746, -0.14871  ,\n",
      "        0.23413  , -0.20719  , -0.091386 ,  0.40075  , -0.17223  ,\n",
      "        0.18145  ,  0.37586  , -0.28682  ,  0.37289  , -0.16185  ,\n",
      "        0.18008  ,  0.3032   , -0.13216  ,  0.18352  ,  0.095759 ,\n",
      "        0.094916 ,  0.008289 ,  0.11761  ,  0.34046  ,  0.03677  ,\n",
      "       -0.29077  ,  0.058303 , -0.027814 ,  0.082941 ,  0.1862   ,\n",
      "       -0.031494 ,  0.27985  , -0.074412 , -0.13762  , -0.21866  ,\n",
      "        0.18138  ,  0.040855 , -0.113    ,  0.24107  ,  0.3657   ,\n",
      "       -0.27525  , -0.05684  ,  0.34872  ,  0.011884 ,  0.14517  ,\n",
      "       -0.71395  ,  0.48497  ,  0.14807  ,  0.62287  ,  0.20599  ,\n",
      "        0.58379  , -0.13438  ,  0.40207  ,  0.18311  ,  0.28021  ,\n",
      "       -0.42349  , -0.25626  ,  0.17715  , -0.54095  ,  0.16596  ,\n",
      "       -0.036058 ,  0.08499  , -0.64989  ,  0.075549 , -0.28831  ,\n",
      "        0.40626  , -0.2802   ,  0.094062 ,  0.32406  ,  0.28437  ,\n",
      "       -0.26341  ,  0.11553  ,  0.071918 , -0.47215  , -0.18366  ,\n",
      "       -0.34709  ,  0.29964  , -0.66514  ,  0.002516 , -0.42333  ,\n",
      "        0.27512  ,  0.36012  ,  0.16311  ,  0.23964  , -0.05923  ,\n",
      "        0.3261   ,  0.20559  ,  0.038677 , -0.045816 ,  0.089764 ,\n",
      "        0.43151  , -0.15954  ,  0.08532  , -0.26572  , -0.15001  ,\n",
      "        0.084286 , -0.16714  , -0.43004  ,  0.060807 ,  0.13121  ,\n",
      "       -0.24112  ,  0.66554  ,  0.4453   , -0.18019  , -0.13919  ,\n",
      "        0.56252  ,  0.21457  , -0.46443  , -0.012211 ,  0.029988 ,\n",
      "       -0.051094 , -0.20135  ,  0.80788  ,  0.47377  , -0.057647 ,\n",
      "        0.46216  ,  0.16084  , -0.20954  , -0.05452  ,  0.15572  ,\n",
      "       -0.13712  ,  0.12972  , -0.011936 , -0.003378 , -0.13595  ,\n",
      "       -0.080711 ,  0.20065  ,  0.054056 ,  0.046816 ,  0.059539 ,\n",
      "        0.046265 ,  0.17754  , -0.31094  ,  0.28119  , -0.24355  ,\n",
      "        0.085252 , -0.21011  , -0.19472  ,  0.0027297, -0.46341  ,\n",
      "        0.14789  , -0.31517  , -0.065939 ,  0.036106 ,  0.42903  ,\n",
      "       -0.33759  ,  0.16432  ,  0.32568  , -0.050392 , -0.054297 ,\n",
      "        0.24074  ,  0.41923  ,  0.13012  , -0.17167  , -0.37808  ,\n",
      "       -0.23089  , -0.019477 , -0.29291  , -0.30824  ,  0.30297  ,\n",
      "       -0.22659  ,  0.081574 , -0.18516  , -0.21408  ,  0.40616  ,\n",
      "       -0.28974  ,  0.074174 , -0.17795  ,  0.28595  , -0.039626 ,\n",
      "       -0.2339   , -0.36054  , -0.067503 , -0.091065 ,  0.23438  ,\n",
      "       -0.0041331,  0.003232 ,  0.0072134,  0.008697 ,  0.21614  ,\n",
      "        0.049904 ,  0.35582  ,  0.13748  ,  0.073361 ,  0.14166  ,\n",
      "        0.2412   , -0.013322 ,  0.15613  ,  0.083381 ,  0.088146 ,\n",
      "       -0.019357 ,  0.43795  ,  0.083961 ,  0.45309  , -0.50489  ,\n",
      "       -0.10865  , -0.2527   , -0.18251  ,  0.20441  ,  0.13319  ,\n",
      "        0.1294   ,  0.050594 , -0.15612  , -0.39543  ,  0.12538  ,\n",
      "        0.24881  , -0.1927   , -0.31847  , -0.12719  ,  0.4341   ,\n",
      "        0.31177  , -0.0040946, -0.2094   , -0.079961 ,  0.1161   ,\n",
      "       -0.050794 ,  0.015266 , -0.2803   , -0.12486  ,  0.23587  ,\n",
      "        0.2339   , -0.14023  ,  0.028462 ,  0.56923  , -0.1649   ,\n",
      "       -0.036429 ,  0.010051 , -0.17107  , -0.042608 ,  0.044965 ,\n",
      "       -0.4393   , -0.26137  ,  0.30088  , -0.060772 , -0.45312  ,\n",
      "       -0.19076  , -0.20288  ,  0.27694  , -0.060888 ,  0.11944  ,\n",
      "        0.62206  , -0.19343  ,  0.47849  , -0.30113  ,  0.059389 ,\n",
      "        0.074901 ,  0.061068 , -0.4662   ,  0.40054  , -0.19099  ,\n",
      "       -0.14331  ,  0.018267 , -0.18643  ,  0.20709  , -0.35598  ,\n",
      "        0.05338  , -0.050821 , -0.1918   , -0.37846  , -0.06589  ],\n",
      "      dtype=float32))\n",
      "CPU times: user 4.16 ms, sys: 11 µs, total: 4.17 ms\n",
      "Wall time: 3.94 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EMBEDDING_FILE = '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "\n",
    "with open(EMBEDDING_FILE, 'r', encoding='utf8') as fp:\n",
    "    for o in fp:\n",
    "        print(get_coefs(*o.split()))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 32s, sys: 5.61 s, total: 4min 38s\n",
      "Wall time: 4min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EMBEDDING_FILE = '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "with open(EMBEDDING_FILE, 'r') as fp:\n",
    "    embedding_index = dict(get_coefs(*o.split(\" \")) for o in fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    " Since all_embeds.std() is coming to 'inf' or Infinity, I hard code the values. \n",
    " \n",
    " For mean, there is no issue  emb_mean,emb_std = -0.005838499,0.48782197"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the embedding index:  2196017\n",
      "-0.005838503 0.48782194\n",
      "embedding matrix shape (2196017, 300)\n",
      "new value for embed_std is:  0.48782194\n",
      "CPU times: user 5.9 s, sys: 3.68 s, total: 9.58 s\n",
      "Wall time: 9.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Size of the embedding index: ', len(embedding_index))\n",
    "\n",
    "# stack the embedding values to calculate the mean and std for a normal distribution\n",
    "all_embeds = np.stack(list(embedding_index.values()))\n",
    "embed_mean, embed_std = np.mean(all_embeds), np.std(all_embeds) # or all_embeds.mean(), all_embeds.std() will do\n",
    "embed_size = all_embeds.shape[1]\n",
    "print(embed_mean, embed_std)\n",
    "print('embedding matrix shape', all_embeds.shape)\n",
    "if embed_std == 'inf':\n",
    "    embed_std = 0.48782197\n",
    "print('new value for embed_std is: ', embed_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Lets look at the tokens from our training data and use the learned glove embeddings to extract the embeddings for those tokens only.\n",
    "Before extracting, the embeddings are normalized. This is because, in the glove embeddings, there are some tokens in lower and upper case, which has different embeddings, though there are quite similar, while others don't have this. So it is better to normalize them before using the learned embeddings for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the vocabulary is : 209442\n",
      "Found embedding for 45959 words from the embeddings, from a total of 50000\n",
      "Number of missing tokens are:  4041\n",
      "Size of the embedding matrix is:  (50001, 300)\n",
      "CPU times: user 726 ms, sys: 24.9 ms, total: 751 ms\n",
      "Wall time: 748 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get the tokens from our training data. Remember the Keras tokenizer has already beeen fit on the train text.\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "# vocab length should be 1 more to account for tokens which has embeddings missing\n",
    "print('Number of unique words in the vocabulary is :', len(word_index) +1)\n",
    "\n",
    "vocab_length = len(word_index)+1\n",
    "\n",
    "# select minimum of tokens tokenized words from train and max_features we want to learn\n",
    "nb_words = min(len(word_index), max_features)\n",
    "\n",
    "''' Use the below when not going to normalize the distribution'''\n",
    "embedding_matrix = np.zeros((len(word_index)+1, embed_size))\n",
    "\n",
    "# normalize the distribution fo embeddings and store them in a matrix, the tokens will be added as keys in the next step\n",
    "embedding_matrix = np.random.normal(embed_mean, embed_std, (nb_words+1, embed_size))\n",
    "\n",
    "count_found = nb_words\n",
    "missing_tokens =[]\n",
    "\n",
    "\n",
    "for word, index in word_index.items():\n",
    "    if index >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embedding_index.get(word) # get the embedding vector for the word\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        count_found -=1\n",
    "        missing_tokens.append(word)\n",
    "\n",
    "print('Found embedding for {} words from the embeddings, from a total of {}'.format(count_found, nb_words))\n",
    "       \n",
    "print('Number of missing tokens are: ', len(missing_tokens))        \n",
    "print('Size of the embedding matrix is: ', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Now lets define a Neural network model to use the above as inputs.\n",
    "\n",
    "The embedding layer can be seeded with the GloVe word embedding weights. Finally, we do not want to update the `learned word weights` in this model, therefore we will set the `trainable` attribute for the model to be `False`\n",
    "\n",
    "The number of rows in the `embedding_matrix` should be the same as the `input_dim` for the `Embedding layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del all_embeds, embedding_index, embedding_vector, missing_tokens\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 300)          15000300  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 100, 128)          140160    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 15,142,541\n",
      "Trainable params: 142,241\n",
      "Non-trainable params: 15,000,300\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#model.add(Embedding(input_dim = max_features, output_dim = embed_size, input_length = max_len, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Embedding(input_dim = max_features+1, output_dim = embed_size, input_length = max_len, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Bidirectional(GRU(64, return_sequences=True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss ='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1175509 samples, validate on 130613 samples\n",
      "Epoch 1/1\n",
      "1175509/1175509 [==============================] - 1060s 902us/step - loss: 0.1391 - accuracy: 0.9482 - val_loss: 0.1202 - val_accuracy: 0.9523\n",
      "CPU times: user 26min 27s, sys: 4min 6s, total: 30min 34s\n",
      "Wall time: 17min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f5695a1ec50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(train_X, train_y, batch_size=512, epochs=1, validation_data=[valid_X, valid_y])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "**Prediction using Glove embedding trained NN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130613/130613 [==============================] - 5s 36us/step\n",
      "F1_score at threshold 0.1 is 0.526\n",
      "F1_score at threshold 0.11 is 0.537\n",
      "F1_score at threshold 0.12 is 0.545\n",
      "F1_score at threshold 0.13 is 0.553\n",
      "F1_score at threshold 0.13999999999999999 is 0.56\n",
      "F1_score at threshold 0.14999999999999997 is 0.566\n",
      "F1_score at threshold 0.15999999999999998 is 0.571\n",
      "F1_score at threshold 0.16999999999999998 is 0.576\n",
      "F1_score at threshold 0.17999999999999997 is 0.581\n",
      "F1_score at threshold 0.18999999999999995 is 0.585\n",
      "F1_score at threshold 0.19999999999999996 is 0.588\n",
      "F1_score at threshold 0.20999999999999996 is 0.59\n",
      "F1_score at threshold 0.21999999999999995 is 0.592\n",
      "F1_score at threshold 0.22999999999999995 is 0.596\n",
      "F1_score at threshold 0.23999999999999994 is 0.598\n",
      "F1_score at threshold 0.24999999999999992 is 0.601\n",
      "F1_score at threshold 0.2599999999999999 is 0.602\n",
      "F1_score at threshold 0.2699999999999999 is 0.603\n",
      "F1_score at threshold 0.2799999999999999 is 0.603\n",
      "F1_score at threshold 0.2899999999999999 is 0.604\n",
      "F1_score at threshold 0.29999999999999993 is 0.605\n",
      "F1_score at threshold 0.30999999999999994 is 0.605\n",
      "F1_score at threshold 0.3199999999999999 is 0.604\n",
      "F1_score at threshold 0.32999999999999985 is 0.605\n",
      "F1_score at threshold 0.33999999999999986 is 0.605\n",
      "F1_score at threshold 0.34999999999999987 is 0.604\n",
      "F1_score at threshold 0.3599999999999999 is 0.604\n",
      "F1_score at threshold 0.3699999999999999 is 0.602\n",
      "F1_score at threshold 0.3799999999999999 is 0.601\n",
      "F1_score at threshold 0.3899999999999999 is 0.601\n",
      "F1_score at threshold 0.3999999999999998 is 0.598\n",
      "F1_score at threshold 0.4099999999999998 is 0.597\n",
      "F1_score at threshold 0.4199999999999998 is 0.596\n",
      "F1_score at threshold 0.4299999999999998 is 0.594\n",
      "F1_score at threshold 0.43999999999999984 is 0.59\n",
      "F1_score at threshold 0.44999999999999984 is 0.588\n",
      "F1_score at threshold 0.45999999999999985 is 0.584\n",
      "F1_score at threshold 0.46999999999999986 is 0.581\n",
      "F1_score at threshold 0.47999999999999976 is 0.577\n",
      "F1_score at threshold 0.48999999999999977 is 0.574\n"
     ]
    }
   ],
   "source": [
    "pred_glove_val_y = model.predict([valid_X], batch_size=1024, verbose=1)\n",
    "\n",
    "# check the F1_score at different thresholds\n",
    "for thresh in np.arange(0.1, 0.5, 0.01):\n",
    "    score = metrics.f1_score(valid_y, (pred_glove_val_y > thresh).astype(int))\n",
    "    print('F1_score at threshold {} is {}'.format(thresh, round(score, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375806/375806 [==============================] - 14s 38us/step\n"
     ]
    }
   ],
   "source": [
    "pred_glove_test_y = model.predict([test_X], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del embedding_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## **Try with FastText Embeddings - wikinews300d1m**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wiki-news-300d-1M.vec']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../input/wikinews300d1mvec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", 0.1073 0.0089 0.0006 0.0055 -0.0646 -0.0600 0.0450 -0.0133 -0.0357 0.0430 -0.0356 -0.0032 0.0073 -0.0001 0.0258 -0.0166 0.0075 0.0686 0.0392 0.0753 0.0115 -0.0087 0.0421 0.0265 -0.0601 0.2420 0.0199 -0.0739 -0.0031 -0.0263 -0.0062 0.0168 -0.0357 -0.0249 0.0190 -0.0184 -0.0537 0.1420 0.0600 0.0226 -0.0038 -0.0675 -0.0036 -0.0080 0.0570 0.0208 0.0223 -0.0256 -0.0153 0.0022 -0.0482 0.0131 -0.6016 -0.0088 0.0106 0.0229 0.0336 0.0071 0.0887 0.0237 -0.0290 -0.0405 -0.0125 0.0147 0.0475 0.0647 0.0474 0.0199 0.0408 0.0322 0.0036 0.0350 -0.0723 -0.0305 0.0184 -0.0026 0.0240 -0.0160 -0.0308 0.0434 0.0147 -0.0457 -0.0267 -0.1703 -0.0099 0.0417 0.0235 -0.0260 -0.1519 -0.0116 -0.0306 -0.0413 0.0330 0.0723 0.0365 -0.0001 0.0042 0.0346 0.0277 -0.0305 0.0784 -0.0404 0.0187 -0.0225 -0.0206 -0.0179 -0.2428 0.0669 0.0523 0.0527 0.0149 -0.0708 -0.0987 0.0263 -0.0611 0.0302 0.0216 0.0313 -0.0140 -0.2495 -0.0346 -0.0480 0.0250 0.2130 -0.0330 -0.1553 -0.0292 -0.0346 0.1074 0.0010 -0.0117 -0.0057 -0.1280 -0.0038 0.0130 -0.1157 -0.0108 0.0275 0.0158 -0.0169 0.0070 0.0247 0.0510 1.0292 -0.0283 -0.0310 -0.0026 -0.0343 0.0578 0.0444 0.0812 -0.0211 -0.0872 0.0169 0.0499 0.0485 0.0227 -0.0323 -0.0035 0.0435 -0.0275 0.0154 0.0135 -0.0484 -0.0699 -0.0502 0.2745 -0.0003 -0.0371 0.0517 -0.0908 0.0013 0.0360 0.0280 0.0839 0.0980 -0.0490 -0.2423 -0.0142 0.0024 -0.0207 0.0012 0.0088 -0.0143 -0.0197 0.0515 -0.0085 0.0257 0.2154 0.0301 0.0211 0.0530 -0.0005 0.0177 0.0016 -0.0053 -0.0162 -0.0223 -0.1862 0.0398 0.0658 -0.0962 -0.0076 -0.0075 -0.0342 -0.0265 0.0420 0.0522 -0.0266 0.0201 -0.1331 -0.0367 0.0351 0.0518 -0.0087 0.0599 -0.1086 -0.0188 0.0481 0.0105 -0.0060 0.0151 -0.0031 0.0077 -0.0276 -0.0373 -0.0203 0.0472 0.0246 0.1440 0.0542 -0.0225 0.2495 0.1617 0.0038 0.1119 -0.0230 -0.0785 0.0250 -0.0616 -0.0485 0.0225 0.0281 0.0041 0.0112 0.0172 0.0291 -0.0282 0.0026 0.4055 0.0392 0.0088 0.0228 0.0299 0.1195 0.0545 -0.0020 0.0020 0.0490 0.0145 -0.0086 0.0098 -0.0236 0.0171 -0.0765 -0.0400 0.0128 0.0011 0.0042 0.0244 0.0075 0.0200 0.0201 0.0196 -0.0377 -0.0432 -0.0073 -0.0021 0.0183 0.0076 0.1805 -0.0551 0.0075 -0.0516 0.0420 -0.0068 -0.0711 -0.1408 0.0504 0.0276 0.0470 0.0323 -0.0219 0.0010 0.0089 0.0276 0.0186 0.0050 0.1173 -0.0400 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examine file content\n",
    "\n",
    "EMBEDDING_FILE = '../input/wikinews300d1mvec/wiki-news-300d-1M.vec'\n",
    "\n",
    "with open(EMBEDDING_FILE) as fp:\n",
    "    next(fp)  # skip header\n",
    "    print(fp.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "There was a problem when reading from the embedding file and converting it into a dictionary. The dict() method was not able to understand the inputs as token as vectors and hence used the `setdefault()` method of the dictionary, where we explicity mention what is the key and value pairs. More details on how to use it can be referred [here](https://www.geeksforgeeks.org/python-convert-list-tuples-dictionary/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '0.1073 0.0089 0.0006 0.0055 -0.0646 -0.0600 0.0450 -0.0133 -0.0357 0.0430 -0.0356 -0.0032 0.0073 -0.0001 0.0258 -0.0166 0.0075 0.0686 0.0392 0.0753 0.0115 -0.0087 0.0421 0.0265 -0.0601 0.2420 0.0199 -0.0739 -0.0031 -0.0263 -0.0062 0.0168 -0.0357 -0.0249 0.0190 -0.0184 -0.0537 0.1420 0.0600 0.0226 -0.0038 -0.0675 -0.0036 -0.0080 0.0570 0.0208 0.0223 -0.0256 -0.0153 0.0022 -0.0482 0.0131 -0.6016 -0.0088 0.0106 0.0229 0.0336 0.0071 0.0887 0.0237 -0.0290 -0.0405 -0.0125 0.0147 0.0475 0.0647 0.0474 0.0199 0.0408 0.0322 0.0036 0.0350 -0.0723 -0.0305 0.0184 -0.0026 0.0240 -0.0160 -0.0308 0.0434 0.0147 -0.0457 -0.0267 -0.1703 -0.0099 0.0417 0.0235 -0.0260 -0.1519 -0.0116 -0.0306 -0.0413 0.0330 0.0723 0.0365 -0.0001 0.0042 0.0346 0.0277 -0.0305 0.0784 -0.0404 0.0187 -0.0225 -0.0206 -0.0179 -0.2428 0.0669 0.0523 0.0527 0.0149 -0.0708 -0.0987 0.0263 -0.0611 0.0302 0.0216 0.0313 -0.0140 -0.2495 -0.0346 -0.0480 0.0250 0.2130 -0.0330 -0.1553 -0.0292 -0.0346 0.1074 0.0010 -0.0117 -0.0057 -0.1280 -0.0038 0.0130 -0.1157 -0.0108 0.0275 0.0158 -0.0169 0.0070 0.0247 0.0510 1.0292 -0.0283 -0.0310 -0.0026 -0.0343 0.0578 0.0444 0.0812 -0.0211 -0.0872 0.0169 0.0499 0.0485 0.0227 -0.0323 -0.0035 0.0435 -0.0275 0.0154 0.0135 -0.0484 -0.0699 -0.0502 0.2745 -0.0003 -0.0371 0.0517 -0.0908 0.0013 0.0360 0.0280 0.0839 0.0980 -0.0490 -0.2423 -0.0142 0.0024 -0.0207 0.0012 0.0088 -0.0143 -0.0197 0.0515 -0.0085 0.0257 0.2154 0.0301 0.0211 0.0530 -0.0005 0.0177 0.0016 -0.0053 -0.0162 -0.0223 -0.1862 0.0398 0.0658 -0.0962 -0.0076 -0.0075 -0.0342 -0.0265 0.0420 0.0522 -0.0266 0.0201 -0.1331 -0.0367 0.0351 0.0518 -0.0087 0.0599 -0.1086 -0.0188 0.0481 0.0105 -0.0060 0.0151 -0.0031 0.0077 -0.0276 -0.0373 -0.0203 0.0472 0.0246 0.1440 0.0542 -0.0225 0.2495 0.1617 0.0038 0.1119 -0.0230 -0.0785 0.0250 -0.0616 -0.0485 0.0225 0.0281 0.0041 0.0112 0.0172 0.0291 -0.0282 0.0026 0.4055 0.0392 0.0088 0.0228 0.0299 0.1195 0.0545 -0.0020 0.0020 0.0490 0.0145 -0.0086 0.0098 -0.0236 0.0171 -0.0765 -0.0400 0.0128 0.0011 0.0042 0.0244 0.0075 0.0200 0.0201 0.0196 -0.0377 -0.0432 -0.0073 -0.0021 0.0183 0.0076 0.1805 -0.0551 0.0075 -0.0516 0.0420 -0.0068 -0.0711 -0.1408 0.0504 0.0276 0.0470 0.0323 -0.0219 0.0010 0.0089 0.0276 0.0186 0.0050 0.1173 -0.0400']\n",
      "{',': '0.1073 0.0089 0.0006 0.0055 -0.0646 -0.0600 0.0450 -0.0133 -0.0357 0.0430 -0.0356 -0.0032 0.0073 -0.0001 0.0258 -0.0166 0.0075 0.0686 0.0392 0.0753 0.0115 -0.0087 0.0421 0.0265 -0.0601 0.2420 0.0199 -0.0739 -0.0031 -0.0263 -0.0062 0.0168 -0.0357 -0.0249 0.0190 -0.0184 -0.0537 0.1420 0.0600 0.0226 -0.0038 -0.0675 -0.0036 -0.0080 0.0570 0.0208 0.0223 -0.0256 -0.0153 0.0022 -0.0482 0.0131 -0.6016 -0.0088 0.0106 0.0229 0.0336 0.0071 0.0887 0.0237 -0.0290 -0.0405 -0.0125 0.0147 0.0475 0.0647 0.0474 0.0199 0.0408 0.0322 0.0036 0.0350 -0.0723 -0.0305 0.0184 -0.0026 0.0240 -0.0160 -0.0308 0.0434 0.0147 -0.0457 -0.0267 -0.1703 -0.0099 0.0417 0.0235 -0.0260 -0.1519 -0.0116 -0.0306 -0.0413 0.0330 0.0723 0.0365 -0.0001 0.0042 0.0346 0.0277 -0.0305 0.0784 -0.0404 0.0187 -0.0225 -0.0206 -0.0179 -0.2428 0.0669 0.0523 0.0527 0.0149 -0.0708 -0.0987 0.0263 -0.0611 0.0302 0.0216 0.0313 -0.0140 -0.2495 -0.0346 -0.0480 0.0250 0.2130 -0.0330 -0.1553 -0.0292 -0.0346 0.1074 0.0010 -0.0117 -0.0057 -0.1280 -0.0038 0.0130 -0.1157 -0.0108 0.0275 0.0158 -0.0169 0.0070 0.0247 0.0510 1.0292 -0.0283 -0.0310 -0.0026 -0.0343 0.0578 0.0444 0.0812 -0.0211 -0.0872 0.0169 0.0499 0.0485 0.0227 -0.0323 -0.0035 0.0435 -0.0275 0.0154 0.0135 -0.0484 -0.0699 -0.0502 0.2745 -0.0003 -0.0371 0.0517 -0.0908 0.0013 0.0360 0.0280 0.0839 0.0980 -0.0490 -0.2423 -0.0142 0.0024 -0.0207 0.0012 0.0088 -0.0143 -0.0197 0.0515 -0.0085 0.0257 0.2154 0.0301 0.0211 0.0530 -0.0005 0.0177 0.0016 -0.0053 -0.0162 -0.0223 -0.1862 0.0398 0.0658 -0.0962 -0.0076 -0.0075 -0.0342 -0.0265 0.0420 0.0522 -0.0266 0.0201 -0.1331 -0.0367 0.0351 0.0518 -0.0087 0.0599 -0.1086 -0.0188 0.0481 0.0105 -0.0060 0.0151 -0.0031 0.0077 -0.0276 -0.0373 -0.0203 0.0472 0.0246 0.1440 0.0542 -0.0225 0.2495 0.1617 0.0038 0.1119 -0.0230 -0.0785 0.0250 -0.0616 -0.0485 0.0225 0.0281 0.0041 0.0112 0.0172 0.0291 -0.0282 0.0026 0.4055 0.0392 0.0088 0.0228 0.0299 0.1195 0.0545 -0.0020 0.0020 0.0490 0.0145 -0.0086 0.0098 -0.0236 0.0171 -0.0765 -0.0400 0.0128 0.0011 0.0042 0.0244 0.0075 0.0200 0.0201 0.0196 -0.0377 -0.0432 -0.0073 -0.0021 0.0183 0.0076 0.1805 -0.0551 0.0075 -0.0516 0.0420 -0.0068 -0.0711 -0.1408 0.0504 0.0276 0.0470 0.0323 -0.0219 0.0010 0.0089 0.0276 0.0186 0.0050 0.1173 -0.0400'}\n"
     ]
    }
   ],
   "source": [
    "dict_1 = dict()\n",
    "\n",
    "i=1\n",
    "with open(EMBEDDING_FILE, 'r', encoding='utf8') as fp:\n",
    "    next(fp)\n",
    "    for o in fp:\n",
    "        temp = o.rstrip().split(\" \", 1)\n",
    "        print(temp)\n",
    "        dict_1.setdefault(temp[0], temp[1])\n",
    "        i-=1\n",
    "        if i==0:\n",
    "            break\n",
    "print(dict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{',': array([ 1.0730e-01,  8.9000e-03,  6.0000e-04,  5.5000e-03, -6.4600e-02,\n",
      "       -6.0000e-02,  4.5000e-02, -1.3300e-02, -3.5700e-02,  4.3000e-02,\n",
      "       -3.5600e-02, -3.2000e-03,  7.3000e-03, -1.0000e-04,  2.5800e-02,\n",
      "       -1.6600e-02,  7.5000e-03,  6.8600e-02,  3.9200e-02,  7.5300e-02,\n",
      "        1.1500e-02, -8.7000e-03,  4.2100e-02,  2.6500e-02, -6.0100e-02,\n",
      "        2.4200e-01,  1.9900e-02, -7.3900e-02, -3.1000e-03, -2.6300e-02,\n",
      "       -6.2000e-03,  1.6800e-02, -3.5700e-02, -2.4900e-02,  1.9000e-02,\n",
      "       -1.8400e-02, -5.3700e-02,  1.4200e-01,  6.0000e-02,  2.2600e-02,\n",
      "       -3.8000e-03, -6.7500e-02, -3.6000e-03, -8.0000e-03,  5.7000e-02,\n",
      "        2.0800e-02,  2.2300e-02, -2.5600e-02, -1.5300e-02,  2.2000e-03,\n",
      "       -4.8200e-02,  1.3100e-02, -6.0160e-01, -8.8000e-03,  1.0600e-02,\n",
      "        2.2900e-02,  3.3600e-02,  7.1000e-03,  8.8700e-02,  2.3700e-02,\n",
      "       -2.9000e-02, -4.0500e-02, -1.2500e-02,  1.4700e-02,  4.7500e-02,\n",
      "        6.4700e-02,  4.7400e-02,  1.9900e-02,  4.0800e-02,  3.2200e-02,\n",
      "        3.6000e-03,  3.5000e-02, -7.2300e-02, -3.0500e-02,  1.8400e-02,\n",
      "       -2.6000e-03,  2.4000e-02, -1.6000e-02, -3.0800e-02,  4.3400e-02,\n",
      "        1.4700e-02, -4.5700e-02, -2.6700e-02, -1.7030e-01, -9.9000e-03,\n",
      "        4.1700e-02,  2.3500e-02, -2.6000e-02, -1.5190e-01, -1.1600e-02,\n",
      "       -3.0600e-02, -4.1300e-02,  3.3000e-02,  7.2300e-02,  3.6500e-02,\n",
      "       -1.0000e-04,  4.2000e-03,  3.4600e-02,  2.7700e-02, -3.0500e-02,\n",
      "        7.8400e-02, -4.0400e-02,  1.8700e-02, -2.2500e-02, -2.0600e-02,\n",
      "       -1.7900e-02, -2.4280e-01,  6.6900e-02,  5.2300e-02,  5.2700e-02,\n",
      "        1.4900e-02, -7.0800e-02, -9.8700e-02,  2.6300e-02, -6.1100e-02,\n",
      "        3.0200e-02,  2.1600e-02,  3.1300e-02, -1.4000e-02, -2.4950e-01,\n",
      "       -3.4600e-02, -4.8000e-02,  2.5000e-02,  2.1300e-01, -3.3000e-02,\n",
      "       -1.5530e-01, -2.9200e-02, -3.4600e-02,  1.0740e-01,  1.0000e-03,\n",
      "       -1.1700e-02, -5.7000e-03, -1.2800e-01, -3.8000e-03,  1.3000e-02,\n",
      "       -1.1570e-01, -1.0800e-02,  2.7500e-02,  1.5800e-02, -1.6900e-02,\n",
      "        7.0000e-03,  2.4700e-02,  5.1000e-02,  1.0292e+00, -2.8300e-02,\n",
      "       -3.1000e-02, -2.6000e-03, -3.4300e-02,  5.7800e-02,  4.4400e-02,\n",
      "        8.1200e-02, -2.1100e-02, -8.7200e-02,  1.6900e-02,  4.9900e-02,\n",
      "        4.8500e-02,  2.2700e-02, -3.2300e-02, -3.5000e-03,  4.3500e-02,\n",
      "       -2.7500e-02,  1.5400e-02,  1.3500e-02, -4.8400e-02, -6.9900e-02,\n",
      "       -5.0200e-02,  2.7450e-01, -3.0000e-04, -3.7100e-02,  5.1700e-02,\n",
      "       -9.0800e-02,  1.3000e-03,  3.6000e-02,  2.8000e-02,  8.3900e-02,\n",
      "        9.8000e-02, -4.9000e-02, -2.4230e-01, -1.4200e-02,  2.4000e-03,\n",
      "       -2.0700e-02,  1.2000e-03,  8.8000e-03, -1.4300e-02, -1.9700e-02,\n",
      "        5.1500e-02, -8.5000e-03,  2.5700e-02,  2.1540e-01,  3.0100e-02,\n",
      "        2.1100e-02,  5.3000e-02, -5.0000e-04,  1.7700e-02,  1.6000e-03,\n",
      "       -5.3000e-03, -1.6200e-02, -2.2300e-02, -1.8620e-01,  3.9800e-02,\n",
      "        6.5800e-02, -9.6200e-02, -7.6000e-03, -7.5000e-03, -3.4200e-02,\n",
      "       -2.6500e-02,  4.2000e-02,  5.2200e-02, -2.6600e-02,  2.0100e-02,\n",
      "       -1.3310e-01, -3.6700e-02,  3.5100e-02,  5.1800e-02, -8.7000e-03,\n",
      "        5.9900e-02, -1.0860e-01, -1.8800e-02,  4.8100e-02,  1.0500e-02,\n",
      "       -6.0000e-03,  1.5100e-02, -3.1000e-03,  7.7000e-03, -2.7600e-02,\n",
      "       -3.7300e-02, -2.0300e-02,  4.7200e-02,  2.4600e-02,  1.4400e-01,\n",
      "        5.4200e-02, -2.2500e-02,  2.4950e-01,  1.6170e-01,  3.8000e-03,\n",
      "        1.1190e-01, -2.3000e-02, -7.8500e-02,  2.5000e-02, -6.1600e-02,\n",
      "       -4.8500e-02,  2.2500e-02,  2.8100e-02,  4.1000e-03,  1.1200e-02,\n",
      "        1.7200e-02,  2.9100e-02, -2.8200e-02,  2.6000e-03,  4.0550e-01,\n",
      "        3.9200e-02,  8.8000e-03,  2.2800e-02,  2.9900e-02,  1.1950e-01,\n",
      "        5.4500e-02, -2.0000e-03,  2.0000e-03,  4.9000e-02,  1.4500e-02,\n",
      "       -8.6000e-03,  9.8000e-03, -2.3600e-02,  1.7100e-02, -7.6500e-02,\n",
      "       -4.0000e-02,  1.2800e-02,  1.1000e-03,  4.2000e-03,  2.4400e-02,\n",
      "        7.5000e-03,  2.0000e-02,  2.0100e-02,  1.9600e-02, -3.7700e-02,\n",
      "       -4.3200e-02, -7.3000e-03, -2.1000e-03,  1.8300e-02,  7.6000e-03,\n",
      "        1.8050e-01, -5.5100e-02,  7.5000e-03, -5.1600e-02,  4.2000e-02,\n",
      "       -6.8000e-03, -7.1100e-02, -1.4080e-01,  5.0400e-02,  2.7600e-02,\n",
      "        4.7000e-02,  3.2300e-02, -2.1900e-02,  1.0000e-03,  8.9000e-03,\n",
      "        2.7600e-02,  1.8600e-02,  5.0000e-03,  1.1730e-01, -4.0000e-02],\n",
      "      dtype=float32), 'the': array([ 8.970e-02,  1.600e-02, -5.710e-02,  4.050e-02, -6.960e-02,\n",
      "       -1.237e-01,  3.010e-02,  2.480e-02, -3.030e-02,  1.740e-02,\n",
      "        6.300e-03,  1.840e-02,  2.170e-02, -2.570e-02,  3.500e-02,\n",
      "       -2.420e-02,  2.900e-03,  1.880e-02, -5.700e-02,  2.520e-02,\n",
      "       -2.100e-02, -8.000e-04,  3.600e-02, -7.290e-02, -6.650e-02,\n",
      "        9.890e-02,  6.760e-02,  8.520e-02, -8.900e-03,  3.130e-02,\n",
      "       -6.900e-03, -3.200e-03, -4.620e-02,  4.970e-02,  2.610e-02,\n",
      "        2.680e-02, -3.100e-02, -1.361e-01, -6.200e-03,  3.750e-02,\n",
      "       -3.200e-02, -1.060e-02,  5.340e-02, -1.870e-02,  6.380e-02,\n",
      "        9.400e-03,  4.700e-03, -5.300e-02,  9.300e-03, -8.700e-03,\n",
      "        4.000e-04,  4.930e-02, -6.296e-01,  2.220e-02,  1.900e-02,\n",
      "        2.680e-02, -4.260e-02,  5.700e-03, -1.683e-01,  2.440e-02,\n",
      "       -2.130e-02, -1.810e-02,  4.210e-02, -3.090e-02, -8.900e-03,\n",
      "        3.200e-03,  1.080e-02, -4.900e-03,  2.580e-02,  2.780e-02,\n",
      "       -1.630e-02,  2.000e-02,  1.640e-02, -9.540e-02, -3.200e-03,\n",
      "        4.300e-03,  1.040e-02, -8.800e-03,  7.000e-04,  3.500e-02,\n",
      "       -2.060e-02, -8.300e-03, -1.140e-02, -1.869e-01,  2.580e-02,\n",
      "        1.000e-03,  8.500e-03,  1.510e-02,  2.125e-01,  7.100e-03,\n",
      "        3.190e-02, -4.820e-02,  6.210e-02,  6.260e-02,  1.590e-02,\n",
      "       -1.300e-03,  8.700e-03,  6.860e-02, -3.400e-03,  2.380e-02,\n",
      "       -4.520e-02, -1.980e-02,  1.120e-02,  1.090e-02, -1.022e-01,\n",
      "       -2.720e-02,  2.337e-01, -4.650e-02,  1.592e-01, -4.070e-02,\n",
      "       -1.029e-01, -4.870e-02, -6.760e-02,  6.760e-02, -3.280e-02,\n",
      "        3.230e-02,  7.700e-03,  1.900e-02,  1.700e-03, -2.974e-01,\n",
      "        1.100e-03, -3.560e-02,  6.930e-02, -4.800e-02, -8.210e-02,\n",
      "       -6.440e-02, -2.840e-02, -1.910e-02, -2.330e-02,  3.530e-02,\n",
      "       -4.630e-02,  6.560e-02,  1.900e-03, -2.120e-02, -3.090e-02,\n",
      "       -3.534e-01, -3.090e-02,  7.600e-03, -4.190e-02,  4.570e-02,\n",
      "       -3.060e-02,  3.570e-02,  6.670e-02,  3.659e-01,  1.490e-02,\n",
      "       -4.430e-02,  6.800e-03, -3.780e-02,  1.460e-02,  2.150e-02,\n",
      "        1.081e-01,  1.240e-02, -4.370e-02, -4.300e-02,  2.580e-02,\n",
      "        2.130e-02, -3.090e-02, -1.800e-03, -6.700e-03,  1.720e-02,\n",
      "        8.900e-03, -1.710e-02,  2.750e-02, -5.180e-02, -1.840e-01,\n",
      "       -1.300e-02, -2.410e-02,  5.260e-02, -2.800e-02,  5.100e-03,\n",
      "        1.630e-02, -1.650e-02,  1.610e-02,  1.237e-01,  8.040e-02,\n",
      "       -7.890e-02,  3.860e-02, -3.892e-01,  1.570e-02, -2.460e-02,\n",
      "        4.770e-02, -4.500e-03, -2.140e-02,  1.730e-02, -1.910e-02,\n",
      "       -1.382e-01, -1.110e-02,  7.120e-02,  1.514e-01,  2.910e-02,\n",
      "        5.550e-02, -3.900e-03,  2.800e-03, -2.770e-02, -2.750e-02,\n",
      "       -1.770e-02, -3.380e-02, -3.720e-02,  2.071e-01,  4.600e-02,\n",
      "       -2.940e-02,  4.350e-02, -1.690e-02, -1.210e-02,  2.530e-02,\n",
      "        1.980e-02,  9.180e-02,  1.930e-02,  6.680e-02,  2.880e-02,\n",
      "        4.000e-03, -4.390e-02, -3.020e-02,  6.400e-03,  3.640e-02,\n",
      "        5.430e-02, -3.380e-02,  1.590e-02,  6.170e-02, -9.410e-02,\n",
      "       -8.600e-03, -9.200e-03,  3.000e-02, -2.410e-02, -3.500e-02,\n",
      "       -6.210e-02,  1.750e-02,  3.740e-02,  3.400e-03,  3.440e-02,\n",
      "        1.286e-01, -2.670e-02,  1.861e-01,  4.890e-02, -3.200e-03,\n",
      "        1.800e-02, -2.280e-02,  2.414e-01, -9.350e-02,  6.120e-02,\n",
      "       -2.090e-02,  1.360e-02,  3.920e-02, -1.350e-02, -2.530e-02,\n",
      "        3.350e-02,  9.500e-03,  4.190e-02,  7.600e-03,  4.522e-01,\n",
      "       -1.880e-02,  2.330e-02, -4.740e-02,  1.590e-02, -9.000e-03,\n",
      "        2.650e-02,  3.360e-02,  2.210e-02,  4.720e-02,  4.800e-03,\n",
      "        9.620e-02,  3.440e-02, -5.150e-02, -8.700e-03, -9.800e-02,\n",
      "       -2.880e-02,  3.770e-02,  2.020e-02, -2.979e-01, -3.870e-02,\n",
      "       -1.980e-02, -1.610e-02, -4.500e-03,  8.700e-03, -3.870e-02,\n",
      "        4.210e-02,  3.830e-02,  2.580e-02,  6.900e-03, -2.980e-02,\n",
      "       -1.980e-02, -1.520e-02,  3.300e-03,  7.500e-03,  3.580e-02,\n",
      "       -1.550e-02, -1.110e-02,  7.600e-02, -4.520e-02,  6.970e-02,\n",
      "        2.990e-02, -2.900e-03, -3.480e-02, -2.700e-02,  3.510e-02,\n",
      "        5.590e-02,  5.910e-02,  1.559e-01, -2.540e-02, -2.590e-02],\n",
      "      dtype=float32), '.': array([ 4.000e-04,  3.200e-03, -2.040e-02,  4.790e-02, -4.500e-02,\n",
      "       -1.165e-01,  1.420e-02,  6.800e-03, -3.340e-02, -5.040e-02,\n",
      "        2.240e-02, -2.900e-03, -2.580e-02,  2.650e-02,  5.900e-03,\n",
      "       -4.590e-02,  7.530e-02,  4.220e-02,  2.690e-02, -2.830e-02,\n",
      "       -1.013e-01,  9.920e-02, -1.140e-02,  5.830e-02, -1.547e-01,\n",
      "       -1.972e-01, -2.820e-02, -1.391e-01, -2.880e-02, -2.830e-02,\n",
      "        2.730e-02,  1.890e-02,  2.750e-02, -5.400e-02,  4.580e-02,\n",
      "        3.060e-02, -1.580e-02,  2.338e-01,  2.060e-02, -8.100e-03,\n",
      "       -1.800e-02, -5.900e-03,  1.045e-01,  4.090e-02,  3.520e-02,\n",
      "       -3.800e-03,  4.030e-02, -1.290e-02, -7.400e-03,  3.000e-04,\n",
      "       -4.840e-02,  4.120e-02, -5.999e-01,  2.240e-02, -1.530e-02,\n",
      "        2.960e-02,  1.100e-03,  6.400e-02, -1.061e-01,  9.000e-04,\n",
      "       -3.800e-03, -1.970e-02,  1.980e-02, -5.600e-03, -2.870e-02,\n",
      "        1.570e-02, -2.620e-02, -3.000e-04, -3.300e-03, -7.000e-04,\n",
      "       -4.210e-02,  3.670e-02, -2.400e-02, -5.190e-02, -9.800e-03,\n",
      "        2.970e-02,  2.510e-02, -1.100e-02, -5.900e-03, -4.200e-03,\n",
      "        1.910e-02,  9.120e-02,  1.420e-02, -4.690e-02,  4.700e-03,\n",
      "       -4.610e-02, -7.000e-04, -2.420e-02, -1.023e-01,  2.210e-02,\n",
      "       -5.500e-03, -2.460e-02,  2.350e-02,  1.175e-01,  5.270e-02,\n",
      "       -1.300e-03,  6.900e-03,  7.500e-03,  6.530e-02,  7.390e-02,\n",
      "       -8.520e-02, -1.700e-02, -1.020e-02, -2.250e-02, -3.273e-01,\n",
      "       -4.000e-03, -2.590e-02,  3.740e-02, -1.285e-01, -2.600e-02,\n",
      "        5.120e-02,  2.950e-02, -6.480e-02,  8.000e-03,  1.000e-02,\n",
      "       -8.880e-02,  2.680e-02,  2.090e-02,  1.720e-02, -2.961e-01,\n",
      "        1.170e-02, -1.024e-01, -6.710e-02, -1.541e-01,  1.400e-03,\n",
      "        8.950e-02, -9.000e-03, -1.170e-02,  2.300e-03,  1.970e-02,\n",
      "        5.130e-02,  5.140e-02, -8.700e-03, -1.600e-03, -1.870e-02,\n",
      "       -1.328e-01, -3.090e-02,  9.300e-03, -1.600e-02, -3.280e-02,\n",
      "        1.230e-02, -1.350e-02,  7.070e-02, -4.418e-01, -2.930e-02,\n",
      "        3.210e-02,  7.250e-02, -1.500e-02, -2.410e-02, -3.080e-02,\n",
      "        1.423e-01,  2.050e-02, -4.430e-02, -1.640e-02, -4.000e-03,\n",
      "        4.100e-02,  3.110e-02,  2.910e-02, -1.440e-02,  2.900e-03,\n",
      "        1.101e-01,  3.050e-02,  5.590e-02, -1.322e-01, -2.437e-01,\n",
      "       -4.960e-02,  1.666e-01, -3.710e-02, -2.550e-02, -1.380e-02,\n",
      "       -2.298e-01, -6.000e-03,  2.060e-02,  4.590e-02, -1.113e-01,\n",
      "       -3.650e-02, -2.480e-02, -3.067e-01,  1.660e-02,  3.340e-02,\n",
      "        2.100e-03, -1.630e-02,  2.370e-02, -2.500e-02,  1.080e-02,\n",
      "       -1.783e-01,  3.010e-02, -6.560e-02,  1.937e-01,  2.270e-02,\n",
      "        1.420e-02, -3.090e-02, -3.130e-02,  5.920e-02,  1.570e-02,\n",
      "       -1.460e-02,  6.910e-02, -3.550e-02,  2.422e-01,  3.300e-03,\n",
      "        9.400e-03,  9.250e-02, -2.800e-02, -8.400e-03,  1.211e-01,\n",
      "        5.300e-03, -8.200e-03,  1.110e-02, -6.280e-02, -2.730e-02,\n",
      "        6.800e-03,  1.780e-02, -3.970e-02,  7.900e-03,  1.300e-02,\n",
      "       -1.390e-02, -1.617e-01, -3.500e-02, -5.900e-02, -5.960e-02,\n",
      "        9.800e-03,  4.810e-02,  2.070e-02, -1.050e-02,  4.660e-02,\n",
      "        2.175e-01,  1.480e-02,  2.070e-02, -1.740e-02, -1.542e-01,\n",
      "        3.220e-02, -1.490e-02,  6.264e-01,  1.360e-02, -6.700e-03,\n",
      "        2.430e-01, -6.440e-02, -1.055e-01, -1.890e-01, -4.200e-03,\n",
      "       -4.240e-02, -3.190e-02,  4.190e-02,  7.800e-03, -4.860e-02,\n",
      "       -5.190e-02, -1.940e-02,  3.200e-02,  1.810e-02,  6.150e-02,\n",
      "       -3.050e-02, -8.000e-04, -2.810e-02,  6.420e-02,  5.690e-02,\n",
      "        5.120e-02, -6.890e-02, -1.000e-02,  3.390e-02, -1.000e-03,\n",
      "       -2.400e-03,  8.370e-02,  3.200e-03, -3.120e-02, -1.129e-01,\n",
      "        8.100e-03, -3.200e-02,  6.500e-03,  9.680e-02, -2.630e-02,\n",
      "       -4.710e-02, -2.560e-02, -3.000e-04,  1.880e-02, -3.970e-02,\n",
      "        4.750e-02, -8.110e-02, -4.300e-02, -1.170e-02,  4.140e-02,\n",
      "       -2.800e-03,  5.240e-02,  2.160e-02,  8.200e-02,  1.140e-02,\n",
      "       -1.730e-02, -3.620e-02, -6.700e-03, -1.180e-02,  4.350e-02,\n",
      "        6.370e-02,  2.200e-03, -9.600e-03, -3.600e-02, -1.679e-01,\n",
      "        3.040e-02,  2.900e-02,  2.070e-01,  6.890e-02, -4.670e-02],\n",
      "      dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "dict_1 = dict()\n",
    "\n",
    "i=3\n",
    "with open(EMBEDDING_FILE, 'r', encoding='utf8') as fp:\n",
    "    next(fp)\n",
    "    for o in fp:\n",
    "        temp = get_coefs(*o.rstrip().split(\" \"))\n",
    "        #print(temp)\n",
    "        dict_1.setdefault(temp[0], temp[1])\n",
    "            \n",
    "#         print(temp[0])\n",
    "#         print(temp[1])\n",
    "\n",
    "#         print(dict(temp))\n",
    "        i-=1\n",
    "        if i==0:\n",
    "            break\n",
    "print(dict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed mean : -0.003347060875967145 and std: 0.10985559225082397\n",
      "Emebdding index size (1000000, 300)\n",
      "new value for embed_std is:  0.10985559\n",
      "Number of unique words in the vocabulary is : 209442\n",
      "Found embedding for 42748 words from the embeddings, from a total of 50000\n",
      "Number of missing tokens are:  7252\n",
      "Size of the embedding matrix is:  (50001, 300)\n",
      "CPU times: user 2min 9s, sys: 2.08 s, total: 2min 11s\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "EMBEDDING_FILE = '../input/wikinews300d1mvec/wiki-news-300d-1M.vec'\n",
    "\n",
    "embedding_index = dict()\n",
    "\n",
    "with open(EMBEDDING_FILE, 'r', encoding='utf8') as fp:\n",
    "    next(fp)                                          # skip header\n",
    "    for o in fp:\n",
    "        temp = get_coefs(*o.rstrip().split(\" \"))\n",
    "        #print(temp)\n",
    "        embedding_index.setdefault(temp[0], temp[1])\n",
    "        \n",
    "# stack all the values\n",
    "all_embed = np.stack(list(embedding_index.values()))\n",
    "embed_mean, embed_std = all_embed.mean(), all_embed.std()\n",
    "embed_size = all_embed.shape[1]\n",
    "\n",
    "print('Embed mean : {} and std: {}'.format(embed_mean, embed_std))\n",
    "print('Emebdding index size', all_embed.shape)\n",
    "\n",
    "if embed_std == 'inf': # I got infinity when using dtype as 'float16', but if using 'float32', this does not happen\n",
    "    embed_std = 0.48782197\n",
    "print('new value for embed_std is: ', embed_std)\n",
    "\n",
    "# get the tokens from our training data. Remember the Keras tokenizer has already beeen fit on the train text.\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "# vocab length should be 1 more to account for tokens which has embeddings missing\n",
    "print('Number of unique words in the vocabulary is :', len(word_index) +1)\n",
    "\n",
    "vocab_length = len(word_index)+1\n",
    "\n",
    "# select minimum of tokens tokenized words from train and max_features we want to learn\n",
    "nb_words = min(len(word_index), max_features)\n",
    "\n",
    "''' Use the below when not going to normalize the distribution'''\n",
    "embedding_matrix = np.zeros((len(word_index)+1, embed_size))\n",
    "\n",
    "# normalize the distribution fo embeddings and store them in a matrix, the tokens will be added as keys in the next step\n",
    "embedding_matrix = np.random.normal(embed_mean, embed_std, (nb_words+1, embed_size))\n",
    "\n",
    "count_found = nb_words\n",
    "missing_tokens =[]\n",
    "\n",
    "\n",
    "for word, index in word_index.items():\n",
    "    if index >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embedding_index.get(word) # get the embedding vector for the word\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        count_found -=1\n",
    "        missing_tokens.append(word)\n",
    "\n",
    "print('Found embedding for {} words from the embeddings, from a total of {}'.format(count_found, nb_words))\n",
    "       \n",
    "print('Number of missing tokens are: ', len(missing_tokens))        \n",
    "print('Size of the embedding matrix is: ', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "del word_index, embedding_index, missing_tokens, all_embed\n",
    "gc.collect()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 300)          15000300  \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 100, 128)          140160    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 15,142,541\n",
      "Trainable params: 142,241\n",
      "Non-trainable params: 15,000,300\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(input_dim =nb_words+1, output_dim = embed_size, input_length = max_len, weights = [embedding_matrix], trainable=False))\n",
    "model.add(Bidirectional(GRU(64, return_sequences=True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss ='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Running for only 1 epoch as each epoch takes time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1175509 samples, validate on 130613 samples\n",
      "Epoch 1/1\n",
      "1175509/1175509 [==============================] - 1048s 892us/step - loss: 0.1451 - accuracy: 0.9479 - val_loss: 0.1242 - val_accuracy: 0.9523\n",
      "CPU times: user 26min 3s, sys: 4min 4s, total: 30min 7s\n",
      "Wall time: 17min 29s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f5a75c1e8d0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(train_X, train_y, epochs=1, batch_size=512, validation_data=[valid_X, valid_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130613/130613 [==============================] - 5s 38us/step\n",
      "F1_score at threshold 0.1 is 0.526\n",
      "F1_score at threshold 0.11 is 0.537\n",
      "F1_score at threshold 0.12 is 0.545\n",
      "F1_score at threshold 0.13 is 0.553\n",
      "F1_score at threshold 0.13999999999999999 is 0.56\n",
      "F1_score at threshold 0.14999999999999997 is 0.566\n",
      "F1_score at threshold 0.15999999999999998 is 0.571\n",
      "F1_score at threshold 0.16999999999999998 is 0.576\n",
      "F1_score at threshold 0.17999999999999997 is 0.581\n",
      "F1_score at threshold 0.18999999999999995 is 0.585\n",
      "F1_score at threshold 0.19999999999999996 is 0.588\n",
      "F1_score at threshold 0.20999999999999996 is 0.59\n",
      "F1_score at threshold 0.21999999999999995 is 0.592\n",
      "F1_score at threshold 0.22999999999999995 is 0.596\n",
      "F1_score at threshold 0.23999999999999994 is 0.598\n",
      "F1_score at threshold 0.24999999999999992 is 0.601\n",
      "F1_score at threshold 0.2599999999999999 is 0.602\n",
      "F1_score at threshold 0.2699999999999999 is 0.603\n",
      "F1_score at threshold 0.2799999999999999 is 0.603\n",
      "F1_score at threshold 0.2899999999999999 is 0.604\n",
      "F1_score at threshold 0.29999999999999993 is 0.605\n",
      "F1_score at threshold 0.30999999999999994 is 0.605\n",
      "F1_score at threshold 0.3199999999999999 is 0.604\n",
      "F1_score at threshold 0.32999999999999985 is 0.605\n",
      "F1_score at threshold 0.33999999999999986 is 0.605\n",
      "F1_score at threshold 0.34999999999999987 is 0.604\n",
      "F1_score at threshold 0.3599999999999999 is 0.604\n",
      "F1_score at threshold 0.3699999999999999 is 0.602\n",
      "F1_score at threshold 0.3799999999999999 is 0.601\n",
      "F1_score at threshold 0.3899999999999999 is 0.601\n",
      "F1_score at threshold 0.3999999999999998 is 0.598\n",
      "F1_score at threshold 0.4099999999999998 is 0.597\n",
      "F1_score at threshold 0.4199999999999998 is 0.596\n",
      "F1_score at threshold 0.4299999999999998 is 0.594\n",
      "F1_score at threshold 0.43999999999999984 is 0.59\n",
      "F1_score at threshold 0.44999999999999984 is 0.588\n",
      "F1_score at threshold 0.45999999999999985 is 0.584\n",
      "F1_score at threshold 0.46999999999999986 is 0.581\n",
      "F1_score at threshold 0.47999999999999976 is 0.577\n",
      "F1_score at threshold 0.48999999999999977 is 0.574\n"
     ]
    }
   ],
   "source": [
    "pred_fasttext_val_y = model.predict([valid_X], batch_size=1024, verbose=1)\n",
    "\n",
    "for thresh in np.arange(0.1, 0.5, 0.01): \n",
    "    score = metrics.f1_score(valid_y, (pred_glove_val_y > thresh).astype(int))\n",
    "    print('F1_score at threshold {} is {}'.format(thresh, round(score, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375806/375806 [==============================] - 13s 35us/step\n"
     ]
    }
   ],
   "source": [
    "pred_fasttext_test_y = model.predict([test_X], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del embedding_matrix, model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## **Using Paragram embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{',': array([-2.57125795e-01,  8.09895635e-01,  3.52934725e-03, -2.61192143e-01,\n",
      "        4.75149117e-02,  3.14476728e-01,  2.82113045e-01, -3.08695018e-01,\n",
      "       -3.07101041e-01,  1.92263556e+00, -2.22033486e-01,  2.48930290e-01,\n",
      "        2.38734439e-01, -1.71986103e-01, -5.13283551e-01,  6.16461970e-03,\n",
      "       -1.09530471e-01,  1.55904317e+00, -6.22845329e-02,  5.42424917e-02,\n",
      "       -5.87660596e-02,  5.11817969e-02, -4.36753384e-04, -3.93734515e-01,\n",
      "        1.13699988e-01, -2.06368506e-01,  1.11778513e-01,  9.43049863e-02,\n",
      "        5.27331889e-01, -1.84620008e-01,  2.28430703e-01,  4.16477844e-02,\n",
      "       -1.15934730e-01,  2.82809436e-01,  5.84579766e-01, -4.75815423e-02,\n",
      "        2.18655720e-01, -1.59711149e-02, -1.84448272e-01,  2.91061491e-01,\n",
      "       -6.23817332e-02, -3.31140310e-01,  8.33015218e-02,  6.00650813e-03,\n",
      "       -1.18001312e-01,  1.14101283e-02, -3.01397681e-01,  1.55064821e-01,\n",
      "        2.47375429e-01,  1.04703279e-02,  1.17448777e-01,  2.93554544e-01,\n",
      "       -8.49206299e-02,  6.28167391e-02, -4.84696448e-01,  5.73283620e-03,\n",
      "        2.35655442e-01, -1.68853298e-01,  2.69412309e-01,  1.29659712e-01,\n",
      "       -9.23916847e-02,  9.12989900e-02,  2.74665833e-01,  1.61878809e-01,\n",
      "        1.26215937e-02, -2.61250824e-01,  4.01077196e-02, -9.85990986e-02,\n",
      "       -1.70548216e-01,  5.01676440e-01,  1.58649627e-02,  1.53560430e-01,\n",
      "        7.23533183e-02, -1.11088723e-01, -2.21821833e-02, -2.33006794e-02,\n",
      "       -1.82922274e-01, -2.81393766e-01, -2.06720084e-02,  5.52844763e-01,\n",
      "       -5.05420327e-01,  1.17939733e-01,  4.02824849e-01,  8.61706510e-02,\n",
      "        2.87466086e-02, -5.62466562e-01,  6.15370512e-01, -9.62910950e-02,\n",
      "        5.97365558e-01,  3.70516628e-01,  3.16369534e-01, -4.92592808e-03,\n",
      "        2.84990698e-01,  2.23917365e-01, -6.05854429e-02, -3.74979109e-01,\n",
      "       -6.03199959e-01, -1.30521566e-01, -4.54154223e-01, -1.05947323e-01,\n",
      "       -1.39568031e-01,  3.15564364e-01, -4.78238434e-01, -1.85250223e-01,\n",
      "       -1.47022292e-01,  7.39198625e-01, -3.95243436e-01, -2.92153866e-03,\n",
      "        1.57927066e-01,  1.98375314e-01,  2.02339310e-02, -1.11572243e-01,\n",
      "        2.14093447e-01, -4.67021555e-01, -2.79860288e-01, -3.06741297e-02,\n",
      "        1.15464158e-01, -7.95712411e-01, -3.04413121e-02, -3.98879915e-01,\n",
      "        1.46741003e-01,  2.78129250e-01,  1.40431270e-01,  2.57860035e-01,\n",
      "       -2.85826653e-01,  1.52656436e-01,  1.51505709e-01,  1.34732097e-01,\n",
      "        2.48444170e-01, -1.17136516e-01,  1.77648246e-01, -4.33827847e-01,\n",
      "       -7.43784010e-03, -1.47656381e-01, -1.18499644e-01,  3.22664291e-01,\n",
      "       -2.25593090e-01, -1.84992567e-01, -1.53535649e-01,  8.78499970e-02,\n",
      "       -5.16256392e-01,  8.61399472e-01,  7.33572662e-01,  9.30007081e-03,\n",
      "       -2.61377335e-01,  3.19702715e-01,  2.87338436e-01, -1.49302065e-01,\n",
      "       -2.79114574e-01, -1.83404852e-02, -8.67057592e-02,  3.03699374e-02,\n",
      "        5.66069424e-01,  3.03689599e-01,  3.16642344e-01,  1.53821215e-01,\n",
      "        2.87508905e-01, -1.91798449e-01,  1.17286474e-01, -8.52014963e-03,\n",
      "        7.91468471e-02,  1.27255976e-01, -8.59855339e-02, -1.07645124e-01,\n",
      "       -2.10657343e-01,  2.07046285e-01, -7.39137977e-02, -8.61972347e-02,\n",
      "        2.10717499e-01,  4.52347100e-01, -1.13193825e-01,  8.35543647e-02,\n",
      "       -6.61283076e-01,  2.81016827e-01, -1.75844952e-01, -8.96361843e-02,\n",
      "       -1.63406014e-01, -3.70537460e-01,  1.43802119e-02, -6.81347191e-01,\n",
      "        1.86705310e-02, -2.50426948e-01, -2.53757834e-01, -2.23165393e-01,\n",
      "        2.49482080e-01, -4.06552732e-01,  2.11726353e-01,  1.00451738e-01,\n",
      "        1.71530664e-01,  1.82964548e-01,  7.67005607e-02,  1.17695056e-01,\n",
      "        4.07552421e-01, -1.57414675e-01, -3.52671832e-01, -3.57697964e-01,\n",
      "       -6.32567555e-02, -5.02070367e-01,  7.48574287e-02,  3.72752964e-01,\n",
      "       -2.55725622e-01,  7.55656660e-02, -1.97395697e-01, -2.00460210e-01,\n",
      "        2.56995648e-01, -4.34121996e-01,  1.24909990e-01, -5.92614934e-02,\n",
      "        2.25615595e-02, -1.81063533e-01, -4.73740220e-01, -2.64733940e-01,\n",
      "        6.03373908e-02,  5.87557517e-02,  3.25305611e-01, -2.02013925e-01,\n",
      "        2.06839919e-01, -7.62498705e-03,  8.26275162e-03,  7.39805028e-02,\n",
      "       -4.65018228e-02,  2.61489868e-01, -8.91087651e-02,  2.49249339e-01,\n",
      "        3.29827428e-01,  1.68509558e-01, -2.59331286e-01,  2.36015379e-01,\n",
      "       -4.84940074e-02, -6.35391707e-03, -5.34992516e-02,  1.35479569e-01,\n",
      "        1.62377581e-01,  2.53484994e-01, -4.73191589e-01,  2.27742329e-01,\n",
      "       -2.47795999e-01, -1.29868671e-01, -2.14547031e-02,  3.99298817e-02,\n",
      "        8.08735192e-02, -1.92016855e-01,  2.07209006e-01, -4.17103082e-01,\n",
      "        2.73412287e-01,  4.78071831e-02, -3.08101028e-01, -2.06720173e-01,\n",
      "       -7.24969152e-03,  6.83252513e-01,  2.26906583e-01,  8.13156739e-03,\n",
      "       -3.80268365e-01, -5.06003164e-02, -2.60247476e-02,  1.88281432e-01,\n",
      "        4.88237431e-03, -2.30988935e-01, -2.79773623e-01,  2.00696886e-01,\n",
      "       -9.19539854e-02, -7.03264400e-03,  2.71720767e-01,  4.63453561e-01,\n",
      "       -1.36245266e-01, -2.46522024e-01, -1.32801190e-01, -3.21483344e-01,\n",
      "        3.42869669e-01,  1.62904158e-01, -8.19709957e-01, -2.94458240e-01,\n",
      "        6.08258620e-02,  2.32246220e-02, -6.88628733e-01,  1.02211125e-02,\n",
      "       -3.66000384e-02,  1.34315938e-01, -8.37789103e-02,  4.35700804e-01,\n",
      "        9.02601838e-01, -4.44801927e-01,  4.45655346e-01, -4.09887791e-01,\n",
      "       -9.34736207e-02,  3.60555910e-02,  1.82083771e-01, -2.25611970e-01,\n",
      "        3.78065318e-01, -2.98776239e-01, -2.16396555e-01, -2.49021068e-01,\n",
      "       -1.84113681e-01, -6.51995391e-02, -2.50193834e-01, -2.57118773e-02,\n",
      "        9.87512320e-02, -3.63280475e-02, -2.55567729e-01,  8.68508741e-02],\n",
      "      dtype=float32), '.': array([-1.08199976e-01,  5.14792800e-01, -3.14560230e-03, -5.21876335e-01,\n",
      "        9.85066593e-02,  2.74737597e-01,  4.22257036e-01, -2.96034753e-01,\n",
      "       -2.43487373e-01,  1.46654010e+00, -5.06000161e-01,  5.75832129e-01,\n",
      "        2.45367765e-01, -4.77039814e-02, -2.29290411e-01,  4.97925393e-02,\n",
      "       -4.46200110e-02,  1.76948202e+00, -1.12913720e-01, -6.13961406e-02,\n",
      "        1.76702976e-01, -1.97725371e-01,  2.48176828e-02, -2.25409657e-01,\n",
      "        4.43594158e-01,  4.01803525e-03, -1.82415500e-01,  1.98106393e-01,\n",
      "        2.00784504e-01, -2.68981773e-02, -6.91939294e-02, -2.13975281e-01,\n",
      "        5.48766628e-02,  1.69985905e-01,  4.74046856e-01, -4.35117595e-02,\n",
      "        4.31587458e-01,  1.25622556e-01, -7.57912640e-03,  5.77629060e-02,\n",
      "        9.92805138e-02, -2.74782926e-01,  3.65043759e-01, -3.60351861e-01,\n",
      "        1.39232641e-02, -1.55861065e-01, -4.50102150e-01,  2.51060903e-01,\n",
      "       -1.21972263e-01,  1.11146003e-01,  1.15427166e-01,  4.87353513e-03,\n",
      "        2.77003735e-01, -2.44322851e-01, -4.66067821e-01, -1.40654564e-01,\n",
      "        5.71941808e-02,  5.09050749e-02,  7.93575570e-02,  7.60844164e-03,\n",
      "       -2.66542703e-01,  2.45806366e-01,  4.09061849e-01,  3.29848588e-01,\n",
      "       -1.18756384e-01, -1.21724240e-01,  1.94623306e-01,  2.11212654e-02,\n",
      "       -3.91831666e-01,  4.80097085e-01,  6.97897822e-02, -6.93299621e-02,\n",
      "        3.45198363e-01,  1.10027827e-02,  6.42097741e-02, -2.56439477e-01,\n",
      "       -2.26635382e-01, -1.26071766e-01, -3.28789115e-01,  4.26840186e-01,\n",
      "       -5.08432925e-01,  9.09600034e-02,  4.99139540e-02,  2.26192906e-01,\n",
      "       -1.08755641e-01, -3.40976238e-01,  5.94368398e-01, -3.14459741e-01,\n",
      "        3.09069782e-01,  4.28758651e-01, -1.14076352e-02,  3.53660248e-02,\n",
      "        4.18665111e-02,  1.86806008e-01, -9.60356966e-02, -3.86008561e-01,\n",
      "       -6.01276219e-01, -3.23542207e-01, -4.52775478e-01, -3.01268816e-01,\n",
      "       -1.31414026e-01,  4.56856549e-01, -2.75131851e-01, -2.47110888e-01,\n",
      "        7.30857849e-02,  5.79679906e-01, -2.92155463e-02, -1.12648800e-01,\n",
      "        1.74392030e-01,  2.83467323e-01,  3.33966106e-01, -2.26406604e-01,\n",
      "        3.39830577e-01, -5.16298890e-01, -2.74569482e-01,  9.55007002e-02,\n",
      "        1.42399281e-01, -4.04717803e-01, -1.58372656e-01, -4.77498174e-01,\n",
      "       -5.94003685e-03,  2.78922588e-01,  2.70999789e-01,  2.77485430e-01,\n",
      "       -4.11515534e-01,  1.47436678e-01, -1.13321375e-02, -2.21019909e-02,\n",
      "        4.91371453e-01, -1.99281648e-01, -2.59624213e-01, -5.08471310e-01,\n",
      "       -3.80694330e-01,  6.58494756e-02, -9.70841721e-02,  4.58805948e-01,\n",
      "       -7.02239498e-02,  1.25254631e-01, -8.59306902e-02,  8.19069520e-02,\n",
      "       -1.16592002e+00,  5.97664237e-01,  5.61300397e-01,  5.88845387e-02,\n",
      "       -1.85910240e-01,  2.04237550e-01,  1.22126229e-02,  3.12164843e-01,\n",
      "       -3.33029717e-01, -3.02222967e-01, -7.31367394e-02,  3.28436613e-01,\n",
      "        2.13396549e-01,  1.35160282e-01,  2.07549930e-01, -1.84549034e-01,\n",
      "       -6.42929822e-02, -1.32965788e-01,  4.72807109e-01, -1.18665017e-01,\n",
      "        2.88580775e-01, -3.44682224e-02, -1.80631671e-02, -3.77160251e-01,\n",
      "       -9.65887029e-03,  9.17482525e-02, -1.17662095e-01, -2.91566104e-01,\n",
      "        4.15535212e-01,  4.03573662e-01, -1.45762727e-01,  1.08922906e-02,\n",
      "       -5.11622012e-01,  1.32847860e-01, -1.13313630e-01, -1.99098811e-01,\n",
      "       -4.54131700e-02, -3.24582040e-01, -1.94289342e-01, -4.00654942e-01,\n",
      "       -1.07674174e-01, -1.33992314e-01, -3.87601763e-01, -5.00924766e-01,\n",
      "       -8.30279291e-02, -2.77476341e-01,  2.26273954e-01, -8.09304789e-02,\n",
      "        2.66568422e-01,  1.08699724e-01, -9.59996656e-02, -9.66520980e-02,\n",
      "        5.34416974e-01,  2.14351043e-01, -6.93973228e-02, -5.57664223e-02,\n",
      "        1.29350960e-01, -5.15218616e-01,  2.97844648e-01,  5.04108787e-01,\n",
      "       -1.32728219e-01, -7.31108040e-02, -7.89096355e-02,  3.72725651e-02,\n",
      "        3.24532092e-02, -3.40323716e-01,  1.90030545e-01,  1.26629934e-01,\n",
      "       -1.10022493e-01, -2.65196413e-01, -2.00210810e-01, -8.77207518e-02,\n",
      "       -5.73257655e-02,  8.18290189e-03,  4.84912723e-01,  1.64712649e-02,\n",
      "        1.70802534e-01,  4.67330478e-02, -1.01451211e-01,  1.94942668e-01,\n",
      "       -1.28381345e-02,  1.56837821e-01, -9.04648826e-02,  5.07586718e-01,\n",
      "        3.53519946e-01,  1.13272235e-01, -4.82731611e-01,  1.56843126e-01,\n",
      "       -1.30441085e-01, -4.16141562e-02, -6.64866120e-02, -4.53756362e-01,\n",
      "        7.58711398e-02, -1.72108747e-02, -1.86284631e-01,  3.50293785e-01,\n",
      "       -2.13254899e-01,  1.88758120e-01, -5.16663902e-02, -1.46702290e-01,\n",
      "       -1.64214775e-01, -1.87305957e-01,  2.30012909e-01, -5.20587154e-02,\n",
      "        3.35590541e-01, -2.25577295e-01, -4.21351552e-01, -8.29178244e-02,\n",
      "        4.86035384e-02,  4.84097838e-01,  3.12441766e-01,  3.03199857e-01,\n",
      "       -4.40230042e-01,  2.36031488e-01,  6.11384138e-02,  1.61626622e-01,\n",
      "        1.54618084e-01, -1.90593004e-01, -3.95183235e-01, -1.40090749e-01,\n",
      "       -3.87002885e-01,  3.99599761e-01,  2.90053070e-01, -2.34548963e-04,\n",
      "       -2.32654393e-01, -3.91791970e-01, -1.66708395e-01, -2.39903610e-02,\n",
      "        6.52766228e-01,  1.89309448e-01, -5.92090607e-01,  5.08999676e-02,\n",
      "       -5.22376038e-03, -2.10036948e-01, -7.25096166e-01,  9.18846875e-02,\n",
      "        9.07683671e-02, -2.17783940e-03, -1.91495717e-02,  4.69401062e-01,\n",
      "        8.66919518e-01, -3.70408356e-01,  1.07837267e-01, -4.42950547e-01,\n",
      "       -1.66303977e-01, -1.36508107e-01,  1.15773395e-01, -1.02446154e-01,\n",
      "        5.77672601e-01, -7.94590563e-02, -6.61133677e-02, -1.47927508e-01,\n",
      "       -2.37177715e-01, -4.15030792e-02, -1.29535481e-01,  7.58646801e-02,\n",
      "        2.33247727e-01,  1.33113816e-01, -1.81757614e-01,  5.78336865e-02],\n",
      "      dtype=float32), 'the': array([ 2.49915466e-01, -1.12669170e-01, -1.75746799e-01, -1.41901985e-01,\n",
      "        4.14641052e-02,  3.49641740e-01,  5.76852322e-01, -8.69857222e-02,\n",
      "        3.32831353e-01,  1.87939739e+00, -6.94813311e-01,  1.44522756e-01,\n",
      "        7.10852623e-01, -7.72599339e-01,  5.48628986e-01,  5.92769742e-01,\n",
      "       -8.40233490e-02,  1.10436177e+00, -5.27610958e-01, -7.12453723e-01,\n",
      "        1.93836614e-01, -1.16054028e-01, -3.11651468e-01, -1.73923761e-01,\n",
      "       -5.57321589e-04, -6.86451867e-02, -1.05996525e+00,  3.78565520e-01,\n",
      "        6.22425020e-01,  5.17876334e-02,  8.32265392e-02, -4.24527116e-02,\n",
      "        4.99945208e-02,  8.09962571e-01,  3.35674018e-01, -2.05139026e-01,\n",
      "       -1.66426063e-01, -1.57357961e-01, -3.50601822e-01, -1.30022720e-01,\n",
      "       -5.98689258e-01,  4.63923365e-01, -1.67771176e-01,  1.35032358e-02,\n",
      "        1.54381678e-01, -2.69392401e-01,  3.90799135e-01,  4.91074435e-02,\n",
      "        1.88878328e-01,  2.29749009e-01, -2.19324172e-01, -2.69701153e-01,\n",
      "        2.64721572e-01,  1.97188452e-01,  9.56704915e-02,  2.52529178e-02,\n",
      "        2.72513121e-01,  1.13746308e-01,  2.05231369e-01,  2.11658198e-02,\n",
      "        1.26773953e-01,  3.40377867e-01, -4.30643499e-01,  5.74611425e-01,\n",
      "       -1.82656776e-02, -8.79503936e-02,  3.24800700e-01, -7.19427168e-01,\n",
      "        3.44189793e-01,  8.54063854e-02, -3.92835706e-01, -5.11655249e-02,\n",
      "        6.80812448e-02, -1.10064618e-01, -3.93182188e-01, -2.68731155e-02,\n",
      "        1.83149725e-01, -9.03094858e-02,  4.30780768e-01,  8.89539495e-02,\n",
      "        3.87566358e-01,  2.37792090e-01, -2.76247948e-01, -3.31650645e-01,\n",
      "        5.76178372e-01, -6.17929041e-01, -1.38064492e+00, -6.28537297e-01,\n",
      "       -6.46551967e-01,  7.25378003e-03, -2.76057333e-01,  1.43621648e-02,\n",
      "        8.41481447e-01, -6.05253398e-01,  9.88613628e-03, -3.82674411e-02,\n",
      "        5.09209514e-01, -6.05429530e-01, -9.43076760e-02,  1.73917696e-01,\n",
      "        1.80435018e-03,  4.65874642e-01, -3.72283608e-01, -3.91152613e-02,\n",
      "        7.29561269e-01,  1.27125585e+00,  2.70198405e-01,  2.01410353e-01,\n",
      "        4.96663213e-01, -3.81907254e-01, -2.31519818e-01, -5.66445351e-01,\n",
      "        3.08666617e-01,  5.93235910e-01,  1.01944156e-01, -3.21016997e-01,\n",
      "       -6.03065550e-01,  3.96192074e-01, -3.28271650e-02, -2.11935222e-01,\n",
      "        4.20197248e-01, -2.71462321e-01, -2.79037654e-01, -5.26644409e-01,\n",
      "       -4.09789145e-01, -2.97036707e-01,  2.65913010e-01,  3.67442407e-02,\n",
      "        8.98582339e-01,  1.26925886e-01, -1.18735802e+00,  2.96414584e-01,\n",
      "       -2.68456452e-02, -1.02842286e-01,  1.47528484e-01,  3.33645642e-01,\n",
      "       -3.98705333e-01, -2.85370708e-01,  4.91982773e-02, -3.28472517e-02,\n",
      "       -1.14237523e+00, -3.36120605e-01,  1.99563026e-01, -5.19322790e-02,\n",
      "        3.70669272e-03,  1.79776430e-01,  2.34361485e-01,  3.30339819e-02,\n",
      "        2.73836702e-01, -3.56950730e-01,  2.92926699e-01, -3.64547148e-02,\n",
      "        3.29783320e-01, -8.70774984e-01,  1.41317680e-01,  3.14055353e-01,\n",
      "       -2.71086872e-01,  1.92631319e-01, -4.03548747e-01, -2.72950530e-01,\n",
      "       -2.01719701e-01, -3.84406373e-02, -7.43130669e-02,  2.09612235e-01,\n",
      "       -5.91008458e-03,  4.07082826e-01, -1.79248184e-01,  6.44063294e-01,\n",
      "        7.01079488e-01, -1.62382096e-01, -3.77512008e-01, -5.60205579e-01,\n",
      "       -3.53863925e-01, -1.06029198e-01,  1.86218247e-01, -1.13019280e-01,\n",
      "        1.40117392e-01, -2.12261789e-02,  3.65805000e-01, -4.53834295e-01,\n",
      "        8.39126930e-02, -6.57748505e-02, -4.18569833e-01, -1.00173354e+00,\n",
      "        3.10639143e-01,  1.56334951e-03, -1.04156613e-01,  1.98379770e-01,\n",
      "        6.11608922e-01, -2.91434020e-01,  3.91526282e-01,  1.88149214e-01,\n",
      "       -1.12901163e+00,  9.88596141e-01,  1.70135066e-01,  6.46003544e-01,\n",
      "       -1.65379986e-01, -5.99001884e-01, -8.86279702e-01,  6.06478631e-01,\n",
      "       -3.19331050e-01,  6.35755360e-01, -3.47353071e-01,  2.95676082e-01,\n",
      "        3.48278284e-01, -1.05189180e+00,  1.43202528e-01,  6.85693383e-01,\n",
      "       -4.96557653e-01, -2.18606010e-01,  8.43281373e-02, -3.59163314e-01,\n",
      "       -2.70482898e-01, -1.69747606e-01, -1.44277029e-02,  3.83216739e-01,\n",
      "       -4.97633696e-01,  4.31078613e-01,  7.07473531e-02,  2.03898519e-01,\n",
      "        2.45414767e-02, -1.02636062e-01, -3.98008525e-01, -6.48450032e-02,\n",
      "        2.76205502e-02,  7.46144652e-01,  4.38561663e-02,  5.24367511e-01,\n",
      "       -2.08967417e-01,  2.00079400e-02,  9.24744383e-02, -1.41351864e-01,\n",
      "       -2.54382998e-01, -3.47004890e-01, -3.09826225e-01,  1.53470516e-01,\n",
      "       -5.90239950e-02,  5.96313000e-01, -4.96642143e-01,  4.54615027e-01,\n",
      "        4.82947469e-01,  8.24023485e-01, -2.87134141e-01,  8.33757222e-01,\n",
      "        2.66374230e-01,  2.58811146e-01,  1.50434613e-01, -5.62351584e-01,\n",
      "       -4.59742099e-01,  5.67092970e-02, -1.95118949e-01, -1.15426607e-01,\n",
      "       -4.81593937e-01,  1.76332921e-01,  2.75866538e-01,  5.85953891e-01,\n",
      "       -1.85888633e-01,  2.64766127e-01,  8.11776161e-01,  1.23755738e-01,\n",
      "        8.42304528e-02,  6.89480603e-02, -2.81668872e-01, -1.09133311e-01,\n",
      "        3.81432831e-01, -6.81008458e-01,  1.14959158e-01, -3.19596529e-01,\n",
      "       -3.36615354e-01, -4.20074552e-01,  3.73243183e-01, -2.94549704e-01,\n",
      "        3.20896864e-01,  4.92262810e-01,  1.85153484e-01,  3.69780838e-01,\n",
      "        3.63160744e-02,  2.11345375e-01, -1.27782911e-01,  9.04749990e-01,\n",
      "       -2.51321107e-01, -1.18210040e-01,  4.67763662e-01, -5.39137721e-01,\n",
      "        3.03359866e-01, -4.52707946e-01,  8.88284445e-01, -1.14420094e-01,\n",
      "        3.82268459e-01,  1.89149946e-01,  3.54114532e-01,  1.25213131e-01,\n",
      "        3.64439011e-01, -4.53117818e-01,  8.96800935e-01,  4.93653454e-02,\n",
      "        5.37476242e-01, -2.19689697e-01,  3.61412317e-02,  3.14833075e-01],\n",
      "      dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# experiment with file read\n",
    "EMBEDDING_FILE = '../input/paragram-300-sl999/paragram_300_sl999.txt'\n",
    "embedding_index = dict()\n",
    "i=3\n",
    "with open(EMBEDDING_FILE, 'r') as fp:\n",
    "    for o in fp:\n",
    "        temp = get_coefs(*o.split(\" \"))\n",
    "        #print(temp[0], temp[1])\n",
    "        embedding_index.setdefault(temp[0], temp[1])\n",
    "        i-=1\n",
    "        if i ==0:\n",
    "            break\n",
    "\n",
    "print(embedding_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.15 s, sys: 170 ms, total: 9.32 s\n",
      "Wall time: 9.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "EMBEDDING_FILE = '../input/paragram-300-sl999/paragram_300_sl999.txt'\n",
    "\n",
    "embedding_index = dict()\n",
    "\n",
    "# with open(EMBEDDING_FILE, 'r') as fp:\n",
    "#     embedding_index = dict(get_coefs(*o.split(\" \")) for o in fp)\n",
    "\n",
    "with open(EMBEDDING_FILE, 'r') as fp:\n",
    "    for o in fp:\n",
    "        temp = get_coefs(*o.split(\" \"))\n",
    "        #print(temp[0], temp[1])\n",
    "        embedding_index.setdefault(temp[0], temp[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "I cannot stack the values for embeddings due to some difference in array size. So I check the array size for every token in the embedding and pop the one which doesn't have an embedding vector size of 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raheem\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for k, v in embedding_index.items():\n",
    "    #print(len(v))\n",
    "    if len(v) != 300:\n",
    "        count+=1\n",
    "        print(k)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.43258283,  0.09266366,  0.25799406,  0.8843603 ,  0.27901617,\n",
       "       -0.85909986, -0.17252979,  0.60357445,  0.5192926 ,  0.30764836,\n",
       "        1.1313126 , -0.09444306,  0.5114069 , -0.4533463 , -0.32634237,\n",
       "        0.42611784,  0.20467412, -1.5356295 ,  0.52981794,  0.23863493,\n",
       "       -0.38401672,  0.42267364, -0.5657015 , -0.05632737,  0.15524974,\n",
       "       -0.4972775 ,  0.2936109 , -0.17312671,  0.5867042 ,  0.3858083 ,\n",
       "       -0.96525776, -0.18743043, -0.49245378, -0.4399429 ,  0.7486791 ,\n",
       "       -0.40318578, -0.00849412, -0.22251162, -0.1376775 ,  0.09772917,\n",
       "       -0.24      ], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross check\n",
    "embedding_index.get('raheem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# pop the token and its embedding vector from the embedding matrix\n",
    "embedding_index.pop('raheem')\n",
    "\n",
    "# now again check\n",
    "count=0\n",
    "for k, v in embedding_index.items():\n",
    "    #print(len(v))\n",
    "    if len(v) != 300:\n",
    "        count+=1\n",
    "        print(k)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed mean : -0.0004052692966070026 and std: 0.42649155855178833\n",
      "Emebdding index shape (66198, 300)\n",
      "new value for embed_std is:  0.42649156\n",
      "Number of unique tokens in the training data identified 209441\n",
      "Found embedding for 29595 words from the embeddings, from a total of 50000\n",
      "Number of missing tokens are:  13153\n",
      "Size of the embedding matrix is:  (50001, 300)\n"
     ]
    }
   ],
   "source": [
    "# stack all the values\n",
    "all_embeds = np.stack(list(embedding_index.values()))\n",
    "embed_mean, embed_std = all_embeds.mean(), all_embeds.std()\n",
    "embed_size = all_embeds.shape[1]\n",
    "\n",
    "print('Embed mean : {} and std: {}'.format(embed_mean, embed_std))\n",
    "print('Emebdding index shape', all_embeds.shape)\n",
    "\n",
    "if embed_std == 'inf':\n",
    "    embed_std = 0.48782197\n",
    "    \n",
    "print('new value for embed_std is: ', embed_std)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Number of unique tokens in the training data identified', len(word_index))\n",
    "\n",
    "vocab_length = len(word_index) + 1\n",
    "\n",
    "nb_words = min(max_features, len(word_index))\n",
    "\n",
    "# normalize the distribution\n",
    "embedding_matrix = np.zeros((nb_words+1, embed_size))\n",
    "\n",
    "embedding_matrix = np.random.normal(embed_mean, embed_std, (nb_words+1, embed_size))\n",
    "\n",
    "missing_tokens= []\n",
    "\n",
    "for word, index in word_index.items():\n",
    "    if index > max_features:\n",
    "        continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "    else:\n",
    "        count_found -=1\n",
    "        missing_tokens.append(word)\n",
    "\n",
    "print('Found embedding for {} words from the embeddings, from a total of {}'.format(count_found, nb_words))\n",
    "       \n",
    "print('Number of missing tokens are: ', len(missing_tokens))        \n",
    "print('Size of the embedding matrix is: ', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 300)          15000300  \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 100, 128)          140160    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 15,142,541\n",
      "Trainable params: 142,241\n",
      "Non-trainable params: 15,000,300\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(input_dim =nb_words+1, output_dim = embed_size, input_length = max_len, trainable=False, weights=[embedding_matrix]))\n",
    "model.add(Bidirectional(GRU(64, return_sequences=True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss ='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "**fit the model using Paragram embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1175509 samples, validate on 130613 samples\n",
      "Epoch 1/1\n",
      "1175509/1175509 [==============================] - 1048s 892us/step - loss: 0.1234 - accuracy: 0.9524 - val_loss: 0.1093 - val_accuracy: 0.9571\n",
      "CPU times: user 26min 4s, sys: 4min 4s, total: 30min 9s\n",
      "Wall time: 17min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f5a74bdccd0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(train_X, train_y, batch_size=512, epochs=1, validation_data=(valid_X, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130613/130613 [==============================] - 5s 36us/step\n",
      "F1_score at threshold 0.1 is 0.526\n",
      "F1_score at threshold 0.11 is 0.537\n",
      "F1_score at threshold 0.12 is 0.545\n",
      "F1_score at threshold 0.13 is 0.553\n",
      "F1_score at threshold 0.13999999999999999 is 0.56\n",
      "F1_score at threshold 0.14999999999999997 is 0.566\n",
      "F1_score at threshold 0.15999999999999998 is 0.571\n",
      "F1_score at threshold 0.16999999999999998 is 0.576\n",
      "F1_score at threshold 0.17999999999999997 is 0.581\n",
      "F1_score at threshold 0.18999999999999995 is 0.585\n",
      "F1_score at threshold 0.19999999999999996 is 0.588\n",
      "F1_score at threshold 0.20999999999999996 is 0.59\n",
      "F1_score at threshold 0.21999999999999995 is 0.592\n",
      "F1_score at threshold 0.22999999999999995 is 0.596\n",
      "F1_score at threshold 0.23999999999999994 is 0.598\n",
      "F1_score at threshold 0.24999999999999992 is 0.601\n",
      "F1_score at threshold 0.2599999999999999 is 0.602\n",
      "F1_score at threshold 0.2699999999999999 is 0.603\n",
      "F1_score at threshold 0.2799999999999999 is 0.603\n",
      "F1_score at threshold 0.2899999999999999 is 0.604\n",
      "F1_score at threshold 0.29999999999999993 is 0.605\n",
      "F1_score at threshold 0.30999999999999994 is 0.605\n",
      "F1_score at threshold 0.3199999999999999 is 0.604\n",
      "F1_score at threshold 0.32999999999999985 is 0.605\n",
      "F1_score at threshold 0.33999999999999986 is 0.605\n",
      "F1_score at threshold 0.34999999999999987 is 0.604\n",
      "F1_score at threshold 0.3599999999999999 is 0.604\n",
      "F1_score at threshold 0.3699999999999999 is 0.602\n",
      "F1_score at threshold 0.3799999999999999 is 0.601\n",
      "F1_score at threshold 0.3899999999999999 is 0.601\n",
      "F1_score at threshold 0.3999999999999998 is 0.598\n",
      "F1_score at threshold 0.4099999999999998 is 0.597\n",
      "F1_score at threshold 0.4199999999999998 is 0.596\n",
      "F1_score at threshold 0.4299999999999998 is 0.594\n",
      "F1_score at threshold 0.43999999999999984 is 0.59\n",
      "F1_score at threshold 0.44999999999999984 is 0.588\n",
      "F1_score at threshold 0.45999999999999985 is 0.584\n",
      "F1_score at threshold 0.46999999999999986 is 0.581\n",
      "F1_score at threshold 0.47999999999999976 is 0.577\n",
      "F1_score at threshold 0.48999999999999977 is 0.574\n"
     ]
    }
   ],
   "source": [
    "pred_paragram_val_y = model.predict([valid_X], batch_size=1024, verbose=1)\n",
    "\n",
    "for thresh in np.arange(0.1, 0.5, 0.01): \n",
    "    score = metrics.f1_score(valid_y, (pred_glove_val_y > thresh).astype(int))\n",
    "    print('F1_score at threshold {} is {}'.format(thresh, round(score, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375806/375806 [==============================] - 13s 36us/step\n"
     ]
    }
   ],
   "source": [
    "pred_paragram_test_y = model.predict([test_X], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del word_index, embedding_matrix, embedding_index, all_embeds, model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## **Blend the 3 models**\n",
    "Explored 3 models using 3 different sets of pre-trained embeddings. Though they are similar, it is possible they might capture different type of information from the data. So lets blend them in equal proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "**prediction on validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score at threshold 0.1 is 0.5489, combining all 3 models\n",
      "F1 score at threshold 0.11 is 0.5605, combining all 3 models\n",
      "F1 score at threshold 0.12 is 0.5693, combining all 3 models\n",
      "F1 score at threshold 0.13 is 0.5781, combining all 3 models\n",
      "F1 score at threshold 0.14 is 0.5863, combining all 3 models\n",
      "F1 score at threshold 0.15 is 0.5943, combining all 3 models\n",
      "F1 score at threshold 0.16 is 0.599, combining all 3 models\n",
      "F1 score at threshold 0.17 is 0.6043, combining all 3 models\n",
      "F1 score at threshold 0.18 is 0.6094, combining all 3 models\n",
      "F1 score at threshold 0.19 is 0.6128, combining all 3 models\n",
      "F1 score at threshold 0.2 is 0.6155, combining all 3 models\n",
      "F1 score at threshold 0.21 is 0.6192, combining all 3 models\n",
      "F1 score at threshold 0.22 is 0.6214, combining all 3 models\n",
      "F1 score at threshold 0.23 is 0.6245, combining all 3 models\n",
      "F1 score at threshold 0.24 is 0.6263, combining all 3 models\n",
      "F1 score at threshold 0.25 is 0.6275, combining all 3 models\n",
      "F1 score at threshold 0.26 is 0.6306, combining all 3 models\n",
      "F1 score at threshold 0.27 is 0.6317, combining all 3 models\n",
      "F1 score at threshold 0.28 is 0.6322, combining all 3 models\n",
      "F1 score at threshold 0.29 is 0.6334, combining all 3 models\n",
      "F1 score at threshold 0.3 is 0.6331, combining all 3 models\n",
      "F1 score at threshold 0.31 is 0.6332, combining all 3 models\n",
      "F1 score at threshold 0.32 is 0.6338, combining all 3 models\n",
      "F1 score at threshold 0.33 is 0.6341, combining all 3 models\n",
      "F1 score at threshold 0.34 is 0.6343, combining all 3 models\n",
      "F1 score at threshold 0.35 is 0.6337, combining all 3 models\n",
      "F1 score at threshold 0.36 is 0.6338, combining all 3 models\n",
      "F1 score at threshold 0.37 is 0.6317, combining all 3 models\n",
      "F1 score at threshold 0.38 is 0.6302, combining all 3 models\n",
      "F1 score at threshold 0.39 is 0.6263, combining all 3 models\n",
      "F1 score at threshold 0.4 is 0.6228, combining all 3 models\n",
      "F1 score at threshold 0.41 is 0.6209, combining all 3 models\n",
      "F1 score at threshold 0.42 is 0.6183, combining all 3 models\n",
      "F1 score at threshold 0.43 is 0.6142, combining all 3 models\n",
      "F1 score at threshold 0.44 is 0.6126, combining all 3 models\n",
      "F1 score at threshold 0.45 is 0.6097, combining all 3 models\n",
      "F1 score at threshold 0.46 is 0.6035, combining all 3 models\n",
      "F1 score at threshold 0.47 is 0.6012, combining all 3 models\n",
      "F1 score at threshold 0.48 is 0.5963, combining all 3 models\n",
      "F1 score at threshold 0.49 is 0.5899, combining all 3 models\n"
     ]
    }
   ],
   "source": [
    "pred_val_y_combined = 0.33*pred_glove_val_y + 0.33*pred_fasttext_val_y + 0.34*pred_paragram_val_y\n",
    "for thresh in np.arange(0.1, 0.5, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    score = round(metrics.f1_score(valid_y, (pred_val_y_combined > thresh).astype(int)),4)\n",
    "    print('F1 score at threshold {} is {}, combining all 3 models'.format(thresh, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "__prediction on test set, using a suitable threshold__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000163e3ea7c7a74cd7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00002bd4fb5d505b9161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00007756b4a147d2b0b3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000086e4b7e1c7146103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000c4c3fbe8785a3090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>000101884c19f3515c1a</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00010f62537781f44a47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00012afbd27452239059</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00014894849d00ba98a9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>000156468431f09b3cae</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  prediction\n",
       "0  0000163e3ea7c7a74cd7           1\n",
       "1  00002bd4fb5d505b9161           0\n",
       "2  00007756b4a147d2b0b3           0\n",
       "3  000086e4b7e1c7146103           0\n",
       "4  0000c4c3fbe8785a3090           0\n",
       "5  000101884c19f3515c1a           0\n",
       "6  00010f62537781f44a47           0\n",
       "7  00012afbd27452239059           0\n",
       "8  00014894849d00ba98a9           0\n",
       "9  000156468431f09b3cae           0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_test_y_combined = 0.33*pred_glove_test_y + 0.33*pred_fasttext_test_y + 0.34*pred_paragram_test_y\n",
    "pred_test_y_combined = (pred_test_y_combined > 0.35).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({'qid': test_df['qid'].values})\n",
    "submission['prediction'] = pred_test_y_combined\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "display(submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
