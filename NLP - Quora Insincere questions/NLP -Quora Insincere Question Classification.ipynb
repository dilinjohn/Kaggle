{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\n/kaggle/input/quora-insincere-questions-classification/sample_submission.csv\n/kaggle/input/quora-insincere-questions-classification/embeddings.zip\n/kaggle/input/quora-insincere-questions-classification/test.csv\n/kaggle/input/quora-insincere-questions-classification/train.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom tqdm import tqdm\nimport math\n\nimport seaborn as sns\n%matplotlib inline\n\nimport gc","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\ntest_df = pd.read_csv('../input/quora-insincere-questions-classification/test.csv')\nprint(train_df.shape)\nprint(test_df.shape)\ndisplay(train_df.head())","execution_count":3,"outputs":[{"output_type":"stream","text":"(1306122, 3)\n(375806, 2)\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"                    qid                                      question_text  \\\n0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n\n   target  \n0       0  \n1       0  \n2       0  \n3       0  \n4       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>question_text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00002165364db923c7e6</td>\n      <td>How did Quebec nationalists see their province...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000032939017120e6e44</td>\n      <td>Do you have an adopted dog, how would you enco...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0000412ca6e4628ce2cf</td>\n      <td>Why does velocity affect time? Does velocity a...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000042bf85aa498cd78e</td>\n      <td>How did Otto von Guericke used the Magdeburg h...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0000455dfa3e01eae3af</td>\n      <td>Can I convert montra helicon D to a mountain b...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Check for data imbalance"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train_df.target)","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f43c0b25e50>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"Check for missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isnull().sum()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"qid              0\nquestion_text    0\ntarget           0\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.isna().sum()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"qid              0\nquestion_text    0\ndtype: int64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Do the following in the next steps:\n\n* Split the train into train and validation sets. We will not do cross valdiation as it is time consuming\n* No missing values identified, if any, replace them with 'na'\n* Tokenize the `text` column and convert them to vector sequences\n* Pad or truncate the sequences as required - truncation happens when sequence length exceeds `max_len` and if less, they will be padded"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# split into train and validation sets\n\ntrain_df, valid_df = train_test_split(train_df, test_size=0.1, random_state=2020)\n\n# some config values for tokenization and vector sequences\n\nmax_len = 100        # max number of words in the question that will be used\nmax_features = 50000 # max number of features or unique words we will use from the entire corpus (the same as number of rows in the embedding matrix)\nembed_size = 300     # size of each embedding or word vector","execution_count":7,"outputs":[{"output_type":"stream","text":"CPU times: user 270 ms, sys: 17.8 ms, total: 288 ms\nWall time: 295 ms\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# substitute missing values in text\ntrain_X = train_df['question_text'].fillna(\"_na_\")\nvalid_X = valid_df['question_text'].fillna(\"_na_\")\ntest_X = test_df['question_text'].fillna(\"_na_\")\n\n# tokenize the sentences\nfrom keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words = max_features)\n\n# fit on train_X. fit_on_texts() expects a list of text as input\ntokenizer.fit_on_texts(train_X.values.tolist())\n\n# use fit_to_sequences method to encode the tokens or words into interger sequences\ntrain_X = tokenizer.texts_to_sequences(train_X.values)\nvalid_X = tokenizer.texts_to_sequences(valid_X.values)\ntest_X = tokenizer.texts_to_sequences(test_X.values)\n\n# pad the sentences, according to the max_len\nfrom keras.preprocessing.sequence import pad_sequences\n\ntrain_X = pad_sequences(train_X, maxlen = max_len)\nvalid_X = pad_sequences(valid_X, maxlen = max_len)\ntest_X = pad_sequences(test_X, maxlen = max_len)\n\n# store the target values\ntrain_y = train_df['target'].values\nvalid_y = valid_df['target'].values","execution_count":8,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"CPU times: user 1min 17s, sys: 864 ms, total: 1min 18s\nWall time: 1min 21s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Without using any of the trained embeddings provided:\nWe will learn the embeddings from scratch without using any of the trained embeddings as the first step. For this we will train a Bidirectional GRU model. We will use the CUDA based NN model from Nvidia known as th CuDNNGRU, which is GPU based."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model, Sequential\nfrom keras.layers import Input, Embedding, CuDNNGRU, Dense, Dropout, LSTM, Conv1D, Activation, GRU\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\n\nfrom keras import initializers, regularizers, constraints, optimizers, layers","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tensorflow version check\nimport tensorflow as tf\nprint('tensor flow version in use: ', tf.__version__)","execution_count":10,"outputs":[{"output_type":"stream","text":"tensor flow version in use:  2.2.0\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Using the CuDNNGRU is not importable as it seems to have been deprecated in recent versions of TF and I keep getting error 'ModuleNotFoundError: No module named 'tensorflow.contrib'', this is because this model is removed in the current version.\n\nThe better way is to simply use the GRU and or LSTM as it is, which will default to `CuDNNGRU` and `CuDNNLSTM`, if certain conditions are met. Refer [here](https://stackoverflow.com/questions/60468385/is-there-cudnnlstm-or-cudnngru-alternative-in-tensorflow-2-0)\n\nRefer to other links on how to import a `GRU` model:\n1. https://www.tensorflow.org/api_docs/python/tf/compat/v1/keras/layers/CuDNNGRU\n2. https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(input_dim = max_features, output_dim = embed_size, input_length = max_len)) # input_shape is optional\nmodel.add(Bidirectional(GRU(64, return_sequences=True)))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(rate=0.1))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(model.summary())","execution_count":11,"outputs":[{"output_type":"stream","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 100, 300)          15000000  \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 100, 128)          140160    \n_________________________________________________________________\nglobal_max_pooling1d_1 (Glob (None, 128)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 16)                2064      \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 16)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 15,142,241\nTrainable params: 15,142,241\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Train the model on the given train split and monitor the metric on the validation set. Run for 2 epochs for now. Changing the epochs, batch_size, model parameters could give a better model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# # train the model\n# model.fit(train_X, train_y, batch_size=512, epochs =2 , validation_data = (valid_X, valid_y))","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import pickle\n# pickle.dump(model, open('bidirectional_gru_keras_no_pretrain_embed.pkl', 'wb'))","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*let's looks at the validation sample predictions and decide the best threshold for `F1` score. The actual values are `0` and `1`. Based on threshold, the predictions will vary.*\n\nHere we are learning embeddings and we don't use any of the pretrained embeddings that have been provided along with the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction_noembed_val_y = model.predict([valid_X], batch_size=1024, verbose=1)\n\n# for thresh in np.arange(0.1, 0.5, 0.01):\n#     thresh = np.round(thresh, 2)\n#     score = round(metrics.f1_score(valid_y, (prediction_noembed_val_y > thresh).astype(int)),4)\n#     print('F1 score at threshold {} is {}'.format(thresh, score))","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # predictions on test(no pretrained embedding in use)\n# pred_noembed_y = model.predict([test_X], batch_size=1024, verbose=1)","execution_count":15,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Clear memory before going to next step**"},{"metadata":{"trusted":true},"cell_type":"code","source":"del model\ngc.collect()\n\n# pause execution for 10s after gc to allow gc to complete\ntime.sleep(10)","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Imported Glove840b300d embeddin file from kaggle"},{"metadata":{},"cell_type":"markdown","source":"Examine the contents of the embedding file. Each `line` represents a `token` and the correspoding `300D vector embedding`"},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILE = '../input/glove840b300dtxt/glove.840B.300d.txt'\n\nwith open(EMBEDDING_FILE, 'r', encoding='utf8') as fp:\n    for i in range(3):\n        print(fp.readline())","execution_count":17,"outputs":[{"output_type":"stream","text":", -0.082752 0.67204 -0.14987 -0.064983 0.056491 0.40228 0.0027747 -0.3311 -0.30691 2.0817 0.031819 0.013643 0.30265 0.0071297 -0.5819 -0.2774 -0.062254 1.1451 -0.24232 0.1235 -0.12243 0.33152 -0.006162 -0.30541 -0.13057 -0.054601 0.037083 -0.070552 0.5893 -0.30385 0.2898 -0.14653 -0.27052 0.37161 0.32031 -0.29125 0.0052483 -0.13212 -0.052736 0.087349 -0.26668 -0.16897 0.015162 -0.0083746 -0.14871 0.23413 -0.20719 -0.091386 0.40075 -0.17223 0.18145 0.37586 -0.28682 0.37289 -0.16185 0.18008 0.3032 -0.13216 0.18352 0.095759 0.094916 0.008289 0.11761 0.34046 0.03677 -0.29077 0.058303 -0.027814 0.082941 0.1862 -0.031494 0.27985 -0.074412 -0.13762 -0.21866 0.18138 0.040855 -0.113 0.24107 0.3657 -0.27525 -0.05684 0.34872 0.011884 0.14517 -0.71395 0.48497 0.14807 0.62287 0.20599 0.58379 -0.13438 0.40207 0.18311 0.28021 -0.42349 -0.25626 0.17715 -0.54095 0.16596 -0.036058 0.08499 -0.64989 0.075549 -0.28831 0.40626 -0.2802 0.094062 0.32406 0.28437 -0.26341 0.11553 0.071918 -0.47215 -0.18366 -0.34709 0.29964 -0.66514 0.002516 -0.42333 0.27512 0.36012 0.16311 0.23964 -0.05923 0.3261 0.20559 0.038677 -0.045816 0.089764 0.43151 -0.15954 0.08532 -0.26572 -0.15001 0.084286 -0.16714 -0.43004 0.060807 0.13121 -0.24112 0.66554 0.4453 -0.18019 -0.13919 0.56252 0.21457 -0.46443 -0.012211 0.029988 -0.051094 -0.20135 0.80788 0.47377 -0.057647 0.46216 0.16084 -0.20954 -0.05452 0.15572 -0.13712 0.12972 -0.011936 -0.003378 -0.13595 -0.080711 0.20065 0.054056 0.046816 0.059539 0.046265 0.17754 -0.31094 0.28119 -0.24355 0.085252 -0.21011 -0.19472 0.0027297 -0.46341 0.14789 -0.31517 -0.065939 0.036106 0.42903 -0.33759 0.16432 0.32568 -0.050392 -0.054297 0.24074 0.41923 0.13012 -0.17167 -0.37808 -0.23089 -0.019477 -0.29291 -0.30824 0.30297 -0.22659 0.081574 -0.18516 -0.21408 0.40616 -0.28974 0.074174 -0.17795 0.28595 -0.039626 -0.2339 -0.36054 -0.067503 -0.091065 0.23438 -0.0041331 0.003232 0.0072134 0.008697 0.21614 0.049904 0.35582 0.13748 0.073361 0.14166 0.2412 -0.013322 0.15613 0.083381 0.088146 -0.019357 0.43795 0.083961 0.45309 -0.50489 -0.10865 -0.2527 -0.18251 0.20441 0.13319 0.1294 0.050594 -0.15612 -0.39543 0.12538 0.24881 -0.1927 -0.31847 -0.12719 0.4341 0.31177 -0.0040946 -0.2094 -0.079961 0.1161 -0.050794 0.015266 -0.2803 -0.12486 0.23587 0.2339 -0.14023 0.028462 0.56923 -0.1649 -0.036429 0.010051 -0.17107 -0.042608 0.044965 -0.4393 -0.26137 0.30088 -0.060772 -0.45312 -0.19076 -0.20288 0.27694 -0.060888 0.11944 0.62206 -0.19343 0.47849 -0.30113 0.059389 0.074901 0.061068 -0.4662 0.40054 -0.19099 -0.14331 0.018267 -0.18643 0.20709 -0.35598 0.05338 -0.050821 -0.1918 -0.37846 -0.06589\n\n. 0.012001 0.20751 -0.12578 -0.59325 0.12525 0.15975 0.13748 -0.33157 -0.13694 1.7893 -0.47094 0.70434 0.26673 -0.089961 -0.18168 0.067226 0.053347 1.5595 -0.2541 0.038413 -0.01409 0.056774 0.023434 0.024042 0.31703 0.19025 -0.37505 0.035603 0.1181 0.012032 -0.037566 -0.5046 -0.049261 0.092351 0.11031 -0.073062 0.33994 0.28239 0.13413 0.070128 -0.022099 -0.28103 0.49607 -0.48693 -0.090964 -0.1538 -0.38011 -0.014228 -0.19392 -0.11068 -0.014088 -0.17906 0.24509 -0.16878 -0.15351 -0.13808 0.02151 0.13699 0.0068061 -0.14915 -0.38169 0.12727 0.44007 0.32678 -0.46117 0.068687 0.34747 0.18827 -0.31837 0.4447 -0.2095 -0.26987 0.48945 0.15388 0.05295 -0.049831 0.11207 0.14881 -0.37003 0.30777 -0.33865 0.045149 -0.18987 0.26634 -0.26401 -0.47556 0.68381 -0.30653 0.24606 0.31611 -0.071098 0.030417 0.088119 0.045025 0.20125 -0.21618 -0.36371 -0.25948 -0.42398 -0.14305 -0.10208 0.21498 -0.21924 -0.17935 0.21546 0.13801 0.24504 -0.2559 0.054815 0.21307 0.2564 -0.25673 0.17961 -0.47638 -0.25181 -0.0091498 -0.054362 -0.21007 0.12597 -0.40795 -0.021164 0.20585 0.18925 -0.0051896 -0.51394 0.28862 -0.077748 -0.27676 0.46567 -0.14225 -0.17879 -0.4357 -0.32481 0.15034 -0.058367 0.49652 0.20472 0.019866 0.13326 0.12823 -1.0177 0.29007 0.28995 0.029994 -0.10763 0.28665 -0.24387 0.22905 -0.26249 -0.069269 -0.17889 0.21936 0.15146 0.04567 -0.050497 0.071482 -0.1027 -0.080705 0.30296 0.031302 0.26613 -0.0060951 0.10313 -0.39987 -0.043945 -0.057625 0.08702 -0.098152 0.22835 -0.005211 0.038075 0.01591 -0.20622 0.021853 0.0040426 -0.043063 -0.002294 -0.26097 -0.25802 -0.28158 -0.23118 -0.010404 -0.30102 -0.4042 0.014653 -0.10445 0.30377 -0.20957 0.3119 0.068272 0.1008 0.010423 0.54011 0.29865 0.12653 0.013761 0.21738 -0.39521 0.066633 0.50327 0.14913 -0.11554 0.010042 0.095698 0.16607 -0.18808 0.055019 0.026715 -0.3164 -0.046583 -0.051591 0.023475 -0.11007 0.085642 0.28394 0.040497 0.071986 0.14157 -0.021199 0.44718 0.20088 -0.12964 -0.067183 0.47614 0.13394 -0.17287 -0.37324 -0.17285 0.02683 -0.1316 0.09116 -0.46487 0.1274 -0.090159 -0.10552 0.068006 -0.13381 0.17056 0.089509 -0.23133 -0.27572 0.061534 -0.051646 0.28377 0.25286 -0.24139 -0.19905 0.12049 -0.1011 0.27392 0.27843 0.26449 -0.18292 -0.048961 0.19198 0.17192 0.33659 -0.20184 -0.34305 -0.24553 -0.15399 0.3945 0.22839 -0.25753 -0.25675 -0.37332 -0.23884 -0.048816 0.78323 0.18851 -0.26477 0.096566 0.062658 -0.30668 -0.43334 0.10006 0.21136 0.039459 -0.11077 0.24421 0.60942 -0.46646 0.086385 -0.39702 -0.23363 0.021307 -0.10778 -0.2281 0.50803 0.11567 0.16165 -0.066737 -0.29556 0.022612 -0.28135 0.0635 0.14019 0.13871 -0.36049 -0.035\n\nthe 0.27204 -0.06203 -0.1884 0.023225 -0.018158 0.0067192 -0.13877 0.17708 0.17709 2.5882 -0.35179 -0.17312 0.43285 -0.10708 0.15006 -0.19982 -0.19093 1.1871 -0.16207 -0.23538 0.003664 -0.19156 -0.085662 0.039199 -0.066449 -0.04209 -0.19122 0.011679 -0.37138 0.21886 0.0011423 0.4319 -0.14205 0.38059 0.30654 0.020167 -0.18316 -0.0065186 -0.0080549 -0.12063 0.027507 0.29839 -0.22896 -0.22882 0.14671 -0.076301 -0.1268 -0.0066651 -0.052795 0.14258 0.1561 0.05551 -0.16149 0.09629 -0.076533 -0.049971 -0.010195 -0.047641 -0.16679 -0.2394 0.0050141 -0.049175 0.013338 0.41923 -0.10104 0.015111 -0.077706 -0.13471 0.119 0.10802 0.21061 -0.051904 0.18527 0.17856 0.041293 -0.014385 -0.082567 -0.035483 -0.076173 -0.045367 0.089281 0.33672 -0.22099 -0.0067275 0.23983 -0.23147 -0.88592 0.091297 -0.012123 0.013233 -0.25799 -0.02972 0.016754 0.01369 0.32377 0.039546 0.042114 -0.088243 0.30318 0.087747 0.16346 -0.40485 -0.043845 -0.040697 0.20936 -0.77795 0.2997 0.2334 0.14891 -0.39037 -0.053086 0.062922 0.065663 -0.13906 0.094193 0.10344 -0.2797 0.28905 -0.32161 0.020687 0.063254 -0.23257 -0.4352 -0.017049 -0.32744 -0.047064 -0.075149 -0.18788 -0.015017 0.029342 -0.3527 -0.044278 -0.13507 -0.11644 -0.1043 0.1392 0.0039199 0.37603 0.067217 -0.37992 -1.1241 -0.057357 -0.16826 0.03941 0.2604 -0.023866 0.17963 0.13553 0.2139 0.052633 -0.25033 -0.11307 0.22234 0.066597 -0.11161 0.062438 -0.27972 0.19878 -0.36262 -1.0006e-05 -0.17262 0.29166 -0.15723 0.054295 0.06101 -0.39165 0.2766 0.057816 0.39709 0.025229 0.24672 -0.08905 0.15683 -0.2096 -0.22196 0.052394 -0.01136 0.050417 -0.14023 -0.042825 -0.031931 -0.21336 -0.20402 -0.23272 0.07449 0.088202 -0.11063 -0.33526 -0.014028 -0.29429 -0.086911 -0.1321 -0.43616 0.20513 0.0079362 0.48505 0.064237 0.14261 -0.43711 0.12783 -0.13111 0.24673 -0.27496 0.15896 0.43314 0.090286 0.24662 0.066463 -0.20099 0.1101 0.03644 0.17359 -0.15689 -0.086328 -0.17316 0.36975 -0.40317 -0.064814 -0.034166 -0.013773 0.062854 -0.17183 -0.12366 -0.034663 -0.22793 -0.23172 0.239 0.27473 0.15332 0.10661 -0.060982 -0.024805 -0.13478 0.17932 -0.37374 -0.02893 -0.11142 -0.08389 -0.055932 0.068039 -0.10783 0.1465 0.094617 -0.084554 0.067429 -0.3291 0.034082 -0.16747 -0.25997 -0.22917 0.020159 -0.02758 0.16136 -0.18538 0.037665 0.57603 0.20684 0.27941 0.16477 -0.018769 0.12062 0.069648 0.059022 -0.23154 0.24095 -0.3471 0.04854 -0.056502 0.41566 -0.43194 0.4823 -0.051759 -0.27285 -0.25893 0.16555 -0.1831 -0.06734 0.42457 0.010346 0.14237 0.25939 0.17123 -0.13821 -0.066846 0.015981 -0.30193 0.043579 -0.043102 0.35025 -0.19681 -0.4281 0.16899 0.22511 -0.28557 -0.1028 -0.018168 0.11407 0.13015 -0.18317 0.1323\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"For details on how what the `*` operator means, refer [here](https://stackoverflow.com/questions/11315010/what-do-and-before-a-variable-name-mean-in-a-function-signature)\n\nInside a function header:\n\n* `*` unpacks a list or tuple into position arguments.\n\n* `**` unpacks a dictionary into keyword arguments."},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract the embeddings and the token from the text file\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float16')","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# explore how we can use the * operator to unpack the list into a tuple\n# do the same for the rest of tokens in each line of the file\n\nwith open(EMBEDDING_FILE, 'r', encoding='utf8') as fp:\n    firstline = fp.readline()\n    secondline = fp.readline()\n    print(secondline)\n    print(get_coefs(*secondline.split(\" \")))","execution_count":19,"outputs":[{"output_type":"stream","text":". 0.012001 0.20751 -0.12578 -0.59325 0.12525 0.15975 0.13748 -0.33157 -0.13694 1.7893 -0.47094 0.70434 0.26673 -0.089961 -0.18168 0.067226 0.053347 1.5595 -0.2541 0.038413 -0.01409 0.056774 0.023434 0.024042 0.31703 0.19025 -0.37505 0.035603 0.1181 0.012032 -0.037566 -0.5046 -0.049261 0.092351 0.11031 -0.073062 0.33994 0.28239 0.13413 0.070128 -0.022099 -0.28103 0.49607 -0.48693 -0.090964 -0.1538 -0.38011 -0.014228 -0.19392 -0.11068 -0.014088 -0.17906 0.24509 -0.16878 -0.15351 -0.13808 0.02151 0.13699 0.0068061 -0.14915 -0.38169 0.12727 0.44007 0.32678 -0.46117 0.068687 0.34747 0.18827 -0.31837 0.4447 -0.2095 -0.26987 0.48945 0.15388 0.05295 -0.049831 0.11207 0.14881 -0.37003 0.30777 -0.33865 0.045149 -0.18987 0.26634 -0.26401 -0.47556 0.68381 -0.30653 0.24606 0.31611 -0.071098 0.030417 0.088119 0.045025 0.20125 -0.21618 -0.36371 -0.25948 -0.42398 -0.14305 -0.10208 0.21498 -0.21924 -0.17935 0.21546 0.13801 0.24504 -0.2559 0.054815 0.21307 0.2564 -0.25673 0.17961 -0.47638 -0.25181 -0.0091498 -0.054362 -0.21007 0.12597 -0.40795 -0.021164 0.20585 0.18925 -0.0051896 -0.51394 0.28862 -0.077748 -0.27676 0.46567 -0.14225 -0.17879 -0.4357 -0.32481 0.15034 -0.058367 0.49652 0.20472 0.019866 0.13326 0.12823 -1.0177 0.29007 0.28995 0.029994 -0.10763 0.28665 -0.24387 0.22905 -0.26249 -0.069269 -0.17889 0.21936 0.15146 0.04567 -0.050497 0.071482 -0.1027 -0.080705 0.30296 0.031302 0.26613 -0.0060951 0.10313 -0.39987 -0.043945 -0.057625 0.08702 -0.098152 0.22835 -0.005211 0.038075 0.01591 -0.20622 0.021853 0.0040426 -0.043063 -0.002294 -0.26097 -0.25802 -0.28158 -0.23118 -0.010404 -0.30102 -0.4042 0.014653 -0.10445 0.30377 -0.20957 0.3119 0.068272 0.1008 0.010423 0.54011 0.29865 0.12653 0.013761 0.21738 -0.39521 0.066633 0.50327 0.14913 -0.11554 0.010042 0.095698 0.16607 -0.18808 0.055019 0.026715 -0.3164 -0.046583 -0.051591 0.023475 -0.11007 0.085642 0.28394 0.040497 0.071986 0.14157 -0.021199 0.44718 0.20088 -0.12964 -0.067183 0.47614 0.13394 -0.17287 -0.37324 -0.17285 0.02683 -0.1316 0.09116 -0.46487 0.1274 -0.090159 -0.10552 0.068006 -0.13381 0.17056 0.089509 -0.23133 -0.27572 0.061534 -0.051646 0.28377 0.25286 -0.24139 -0.19905 0.12049 -0.1011 0.27392 0.27843 0.26449 -0.18292 -0.048961 0.19198 0.17192 0.33659 -0.20184 -0.34305 -0.24553 -0.15399 0.3945 0.22839 -0.25753 -0.25675 -0.37332 -0.23884 -0.048816 0.78323 0.18851 -0.26477 0.096566 0.062658 -0.30668 -0.43334 0.10006 0.21136 0.039459 -0.11077 0.24421 0.60942 -0.46646 0.086385 -0.39702 -0.23363 0.021307 -0.10778 -0.2281 0.50803 0.11567 0.16165 -0.066737 -0.29556 0.022612 -0.28135 0.0635 0.14019 0.13871 -0.36049 -0.035\n\n('.', array([ 0.012   ,  0.2075  , -0.1257  , -0.5933  ,  0.1252  ,  0.1598  ,\n        0.1375  , -0.3315  , -0.137   ,  1.789   , -0.471   ,  0.704   ,\n        0.2668  , -0.08997 , -0.1816  ,  0.0672  ,  0.05334 ,  1.56    ,\n       -0.2542  ,  0.03842 , -0.01409 ,  0.05676 ,  0.02344 ,  0.02405 ,\n        0.3171  ,  0.1903  , -0.375   ,  0.0356  ,  0.1181  ,  0.01203 ,\n       -0.03757 , -0.5044  , -0.04926 ,  0.09235 ,  0.1103  , -0.07306 ,\n        0.3398  ,  0.2825  ,  0.1342  ,  0.0701  , -0.0221  , -0.281   ,\n        0.496   , -0.4868  , -0.09094 , -0.1538  , -0.3801  , -0.01423 ,\n       -0.194   , -0.11066 , -0.01409 , -0.1791  ,  0.2451  , -0.1688  ,\n       -0.1536  , -0.1381  ,  0.02151 ,  0.137   ,  0.006805, -0.1492  ,\n       -0.3816  ,  0.1273  ,  0.4402  ,  0.3267  , -0.4612  ,  0.06866 ,\n        0.3474  ,  0.1882  , -0.3184  ,  0.4446  , -0.2095  , -0.2698  ,\n        0.4895  ,  0.1539  ,  0.05295 , -0.04984 ,  0.11206 ,  0.1488  ,\n       -0.37    ,  0.3079  , -0.3386  ,  0.04514 , -0.1898  ,  0.2664  ,\n       -0.264   , -0.4756  ,  0.6836  , -0.3066  ,  0.2461  ,  0.3162  ,\n       -0.0711  ,  0.03041 ,  0.08813 ,  0.045   ,  0.2013  , -0.2162  ,\n       -0.3638  , -0.2595  , -0.424   , -0.1431  , -0.10205 ,  0.215   ,\n       -0.2192  , -0.1793  ,  0.2155  ,  0.1381  ,  0.245   , -0.2559  ,\n        0.0548  ,  0.213   ,  0.2563  , -0.2568  ,  0.1796  , -0.4763  ,\n       -0.2517  , -0.00915 , -0.05435 , -0.2101  ,  0.126   , -0.408   ,\n       -0.02116 ,  0.2058  ,  0.1892  , -0.005188, -0.514   ,  0.2886  ,\n       -0.07776 , -0.2769  ,  0.4656  , -0.1422  , -0.1788  , -0.4358  ,\n       -0.3247  ,  0.1504  , -0.05838 ,  0.4966  ,  0.2047  ,  0.01987 ,\n        0.1333  ,  0.1282  , -1.018   ,  0.29    ,  0.29    ,  0.03    ,\n       -0.1076  ,  0.2866  , -0.2439  ,  0.229   , -0.2625  , -0.0693  ,\n       -0.1788  ,  0.2194  ,  0.1515  ,  0.0457  , -0.0505  ,  0.0715  ,\n       -0.1027  , -0.0807  ,  0.303   ,  0.0313  ,  0.266   , -0.006096,\n        0.10315 , -0.4     , -0.04395 , -0.05762 ,  0.08704 , -0.09814 ,\n        0.2284  , -0.00521 ,  0.0381  ,  0.01591 , -0.2062  ,  0.02185 ,\n        0.004044, -0.04306 , -0.002295, -0.261   , -0.258   , -0.2815  ,\n       -0.2312  , -0.01041 , -0.301   , -0.4043  ,  0.014656, -0.10443 ,\n        0.3037  , -0.2096  ,  0.312   ,  0.0683  ,  0.1008  ,  0.01042 ,\n        0.54    ,  0.2986  ,  0.1266  ,  0.01376 ,  0.2174  , -0.3953  ,\n        0.06665 ,  0.5034  ,  0.1492  , -0.11554 ,  0.01004 ,  0.0957  ,\n        0.166   , -0.1881  ,  0.05502 ,  0.02672 , -0.3164  , -0.04657 ,\n       -0.0516  ,  0.02347 , -0.11005 ,  0.08563 ,  0.284   ,  0.0405  ,\n        0.07196 ,  0.1416  , -0.0212  ,  0.4473  ,  0.2009  , -0.1296  ,\n       -0.0672  ,  0.476   ,  0.1339  , -0.1729  , -0.3733  , -0.1729  ,\n        0.02682 , -0.1316  ,  0.0912  , -0.4648  ,  0.1274  , -0.09015 ,\n       -0.1055  ,  0.068   , -0.1338  ,  0.1705  ,  0.08954 , -0.2313  ,\n       -0.2756  ,  0.06152 , -0.05164 ,  0.2837  ,  0.253   , -0.2413  ,\n       -0.1991  ,  0.1205  , -0.1011  ,  0.274   ,  0.2783  ,  0.2644  ,\n       -0.1829  , -0.04895 ,  0.192   ,  0.1719  ,  0.3367  , -0.2018  ,\n       -0.343   , -0.2455  , -0.1539  ,  0.3945  ,  0.2284  , -0.2576  ,\n       -0.2568  , -0.3733  , -0.2389  , -0.04883 ,  0.783   ,  0.1885  ,\n       -0.2646  ,  0.09656 ,  0.0627  , -0.3066  , -0.4333  ,  0.10004 ,\n        0.2113  ,  0.03946 , -0.1108  ,  0.2443  ,  0.6094  , -0.4666  ,\n        0.08636 , -0.397   , -0.2336  ,  0.0213  , -0.1078  , -0.2281  ,\n        0.508   ,  0.11566 ,  0.1616  , -0.0667  , -0.2957  ,  0.02261 ,\n       -0.2812  ,  0.0635  ,  0.1401  ,  0.1387  , -0.3606  , -0.035   ],\n      dtype=float16))\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nwith open(EMBEDDING_FILE, 'r', encoding='utf8') as fp:\n    for o in fp:\n        print(get_coefs(*o.split()))\n        break","execution_count":20,"outputs":[{"output_type":"stream","text":"(',', array([-0.08276 ,  0.672   , -0.1499  , -0.065   ,  0.0565  ,  0.4023  ,\n        0.002775, -0.331   , -0.307   ,  2.082   ,  0.03183 ,  0.01364 ,\n        0.3027  ,  0.00713 , -0.582   , -0.2773  , -0.06226 ,  1.1455  ,\n       -0.2423  ,  0.1235  , -0.12244 ,  0.3315  , -0.00616 , -0.3054  ,\n       -0.1306  , -0.0546  ,  0.03708 , -0.07056 ,  0.5894  , -0.304   ,\n        0.2898  , -0.1465  , -0.2705  ,  0.3716  ,  0.3203  , -0.2913  ,\n        0.00525 , -0.1321  , -0.05273 ,  0.08734 , -0.2666  , -0.169   ,\n        0.01516 , -0.00838 , -0.1487  ,  0.2341  , -0.2072  , -0.0914  ,\n        0.4006  , -0.1722  ,  0.1814  ,  0.376   , -0.2869  ,  0.3728  ,\n       -0.1619  ,  0.18    ,  0.3032  , -0.1322  ,  0.1835  ,  0.09576 ,\n        0.0949  ,  0.008286,  0.1176  ,  0.3406  ,  0.03677 , -0.2908  ,\n        0.0583  , -0.02782 ,  0.08295 ,  0.1862  , -0.0315  ,  0.2798  ,\n       -0.0744  , -0.1376  , -0.2186  ,  0.1814  ,  0.04086 , -0.113   ,\n        0.2411  ,  0.3657  , -0.2751  , -0.05685 ,  0.3486  ,  0.01189 ,\n        0.1451  , -0.714   ,  0.4849  ,  0.1481  ,  0.623   ,  0.2059  ,\n        0.584   , -0.1344  ,  0.402   ,  0.1831  ,  0.2803  , -0.4236  ,\n       -0.2563  ,  0.1771  , -0.541   ,  0.166   , -0.03607 ,  0.08496 ,\n       -0.65    ,  0.07556 , -0.2883  ,  0.4062  , -0.2803  ,  0.09406 ,\n        0.324   ,  0.2844  , -0.2634  ,  0.11554 ,  0.0719  , -0.4722  ,\n       -0.1837  , -0.3472  ,  0.2996  , -0.665   ,  0.002516, -0.4233  ,\n        0.2751  ,  0.36    ,  0.1631  ,  0.2396  , -0.05923 ,  0.3262  ,\n        0.2056  ,  0.03867 , -0.0458  ,  0.0898  ,  0.4314  , -0.1595  ,\n        0.0853  , -0.2656  , -0.15    ,  0.0843  , -0.1671  , -0.43    ,\n        0.06082 ,  0.1312  , -0.2411  ,  0.6655  ,  0.4453  , -0.1802  ,\n       -0.1392  ,  0.5625  ,  0.2146  , -0.4644  , -0.012215,  0.02998 ,\n       -0.0511  , -0.2013  ,  0.808   ,  0.4739  , -0.05765 ,  0.4622  ,\n        0.1609  , -0.2096  , -0.05453 ,  0.1558  , -0.1371  ,  0.1298  ,\n       -0.01193 , -0.003378, -0.136   , -0.0807  ,  0.2007  ,  0.05405 ,\n        0.0468  ,  0.05954 ,  0.04626 ,  0.1775  , -0.311   ,  0.2812  ,\n       -0.2435  ,  0.08527 , -0.2101  , -0.1947  ,  0.00273 , -0.4634  ,\n        0.148   , -0.3152  , -0.0659  ,  0.0361  ,  0.429   , -0.3376  ,\n        0.1643  ,  0.3257  , -0.05038 , -0.0543  ,  0.2407  ,  0.4192  ,\n        0.1301  , -0.1716  , -0.3782  , -0.2308  , -0.01947 , -0.293   ,\n       -0.3083  ,  0.303   , -0.2266  ,  0.0816  , -0.1852  , -0.2141  ,\n        0.4062  , -0.2898  ,  0.07416 , -0.178   ,  0.286   , -0.0396  ,\n       -0.2339  , -0.3606  , -0.0675  , -0.09106 ,  0.2344  , -0.00413 ,\n        0.003231,  0.007214,  0.0087  ,  0.2162  ,  0.0499  ,  0.3557  ,\n        0.1375  ,  0.07336 ,  0.1416  ,  0.2412  , -0.01332 ,  0.1561  ,\n        0.0834  ,  0.08813 , -0.01936 ,  0.438   ,  0.084   ,  0.4531  ,\n       -0.505   , -0.10864 , -0.2527  , -0.1825  ,  0.2045  ,  0.1332  ,\n        0.1294  ,  0.0506  , -0.1561  , -0.3955  ,  0.1254  ,  0.2488  ,\n       -0.1927  , -0.3184  , -0.1272  ,  0.434   ,  0.3118  , -0.004093,\n       -0.2094  , -0.07996 ,  0.1161  , -0.05078 ,  0.01527 , -0.2803  ,\n       -0.1249  ,  0.2358  ,  0.2339  , -0.1403  ,  0.02846 ,  0.5693  ,\n       -0.1649  , -0.03644 ,  0.01005 , -0.171   , -0.0426  ,  0.04495 ,\n       -0.4392  , -0.2615  ,  0.3008  , -0.06076 , -0.4531  , -0.1908  ,\n       -0.2029  ,  0.2769  , -0.06088 ,  0.11945 ,  0.622   , -0.1935  ,\n        0.4785  , -0.301   ,  0.0594  ,  0.0749  ,  0.06107 , -0.4663  ,\n        0.4006  , -0.191   , -0.1433  ,  0.01826 , -0.1864  ,  0.207   ,\n       -0.356   ,  0.05338 , -0.0508  , -0.1918  , -0.3784  , -0.0659  ],\n      dtype=float16))\nCPU times: user 4.58 ms, sys: 991 µs, total: 5.57 ms\nWall time: 5.83 ms\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nwith open(EMBEDDING_FILE, 'r') as fp:\n    embedding_index = dict(get_coefs(*o.split(\" \")) for o in fp)","execution_count":21,"outputs":[{"output_type":"stream","text":"CPU times: user 4min 34s, sys: 5.5 s, total: 4min 39s\nWall time: 4min 40s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":" Since all_embeds.std() is coming to 'inf' or Infinity, hard code the values. \n \n For mean, there is no issue  emb_mean,emb_std = -0.005838499,0.48782197"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint('Size of the embedding index: ', len(embedding_index))\n\n# stack the embedding values to calculate the mean and std for a normal distribution\nall_embeds = np.stack(list(embedding_index.values()))\nembed_mean = np.mean(all_embeds) # or all_embeds.mean(), all_embeds.std() will do\nembed_std = 0.48782197\nembed_size = all_embeds.shape[1]\nprint(embed_mean, embed_std)\nprint('embedding matrix shape', all_embeds.shape)\n\ndel all_embeds","execution_count":22,"outputs":[{"output_type":"stream","text":"Size of the embedding index:  2196017\n-0.00584 0.48782197\n","name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"name 'prin' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'prin' is not defined"]}]},{"metadata":{},"cell_type":"markdown","source":"Lets look at the tokens from our training data and use the learned glove embeddings to extract the embeddings for those tokens only.\nBefore extracting, the embeddings are normalized. This is because, in the glove embeddings, there are some tokens in lower and upper case, which has different embeddings, though there are quite similar, while others don't have this. So it is better to normalize them before using the learned embeddings for the training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# get the tokens from our training data. Remember the Keras tokenizer has already beeen fit on the train text.\n\nword_index = tokenizer.word_index\n# vocab length should be 1 more to account for tokens which has embeddings missing\nprint('Number of words in the vocabulary is :', len(word_index) +1)\n\nvocab_length = len(word_index)+1\n\n# select minimum of tokens tokenized words from train and max_features we want to learn\nnb_words = min(len(word_index), max_features)\n\n''' Use the below when not going to normalize the distribution'''\nembedding_matrix = np.zeros((len(word_index)+1, embed_size))\n\n# normalize the distribution fo embeddings and store them in a matrix, the tokens will be added as keys in the next step\nembedding_matrix = np.random.normal(embed_mean, embed_std, (nb_words+1, embed_size))\n\ncount_found = nb_words\nmissing_tokens =[]\n\n\nfor word, index in word_index.items():\n    if index >= max_features:\n        continue\n    embedding_vector = embedding_index.get(word) # get the embedding vector for the word\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n    else:\n        count_found -=1\n        missing_tokens.append(word)\n\nprint('Found embedding for {} words from the embeddings, from a total of {}'.format(count_found, nb_words))\n       \nprint('Number of missing tokens are: ', len(missing_tokens))        \nprint('Size of the embedding matrix is: ', embedding_matrix.shape)","execution_count":23,"outputs":[{"output_type":"stream","text":"Number of words in the vocabulary is : 209442\nFound embedding for 45959 words from the embeddings, from a total of 50000\nNumber of missing tokens are:  4041\nSize of the embedding matrix is:  (50001, 300)\nCPU times: user 847 ms, sys: 6.67 ms, total: 854 ms\nWall time: 870 ms\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Now lets define a Neural network model to use the above as inputs.\n\nThe embedding layer can be seeded with the GloVe word embedding weights. Finally, we do not want to update the `learned word weights` in this model, therefore we will set the `trainable` attribute for the model to be `False`"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n#model.add(Embedding(input_dim = max_features, output_dim = embed_size, input_length = max_len, weights=[embedding_matrix], trainable=False))\nmodel.add(Embedding(input_dim = max_features+1, output_dim = embed_size, input_length = max_len, weights=[embedding_matrix], trainable=False))\nmodel.add(Bidirectional(GRU(64, return_sequences=True)))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(rate=0.1))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss ='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","execution_count":25,"outputs":[{"output_type":"stream","text":"Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_3 (Embedding)      (None, 100, 300)          15000300  \n_________________________________________________________________\nbidirectional_2 (Bidirection (None, 100, 128)          140160    \n_________________________________________________________________\nglobal_max_pooling1d_2 (Glob (None, 128)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 16)                2064      \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 16)                0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 15,142,541\nTrainable params: 142,241\nNon-trainable params: 15,000,300\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel.fit(train_X, train_y, batch_size=512, epochs=1, validation_data=[valid_X, valid_y])","execution_count":26,"outputs":[{"output_type":"stream","text":"Train on 1175509 samples, validate on 130613 samples\nEpoch 1/1\n1175509/1175509 [==============================] - 1040s 884us/step - loss: 0.1394 - accuracy: 0.9484 - val_loss: 0.1207 - val_accuracy: 0.9525\nCPU times: user 25min 58s, sys: 3min 59s, total: 29min 57s\nWall time: 17min 21s\n","name":"stdout"},{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"<keras.callbacks.callbacks.History at 0x7f438a625d50>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**Prediction using Glove embedding trained NN model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_glove_val_y = model.predict([valid_X], batch_size=1024, verbose=1)\n\n# check the F1_score at different thresholds\nfor thresh in np.arange(0.1, 0.5, 0.01):\n    score = metrics.f1_score(valid_y, (pred_glove_val_y > thresh).astype(int))\n    print('F1_score at threshold {} is {}'.format(thresh, round(score, 3)))","execution_count":27,"outputs":[{"output_type":"stream","text":"130613/130613 [==============================] - 5s 40us/step\nF1_score at threshold 0.1 is 0.528\nF1_score at threshold 0.11 is 0.536\nF1_score at threshold 0.12 is 0.544\nF1_score at threshold 0.13 is 0.551\nF1_score at threshold 0.13999999999999999 is 0.558\nF1_score at threshold 0.14999999999999997 is 0.562\nF1_score at threshold 0.15999999999999998 is 0.567\nF1_score at threshold 0.16999999999999998 is 0.572\nF1_score at threshold 0.17999999999999997 is 0.576\nF1_score at threshold 0.18999999999999995 is 0.58\nF1_score at threshold 0.19999999999999996 is 0.583\nF1_score at threshold 0.20999999999999996 is 0.587\nF1_score at threshold 0.21999999999999995 is 0.589\nF1_score at threshold 0.22999999999999995 is 0.592\nF1_score at threshold 0.23999999999999994 is 0.595\nF1_score at threshold 0.24999999999999992 is 0.596\nF1_score at threshold 0.2599999999999999 is 0.597\nF1_score at threshold 0.2699999999999999 is 0.601\nF1_score at threshold 0.2799999999999999 is 0.602\nF1_score at threshold 0.2899999999999999 is 0.603\nF1_score at threshold 0.29999999999999993 is 0.604\nF1_score at threshold 0.30999999999999994 is 0.606\nF1_score at threshold 0.3199999999999999 is 0.606\nF1_score at threshold 0.32999999999999985 is 0.606\nF1_score at threshold 0.33999999999999986 is 0.605\nF1_score at threshold 0.34999999999999987 is 0.605\nF1_score at threshold 0.3599999999999999 is 0.604\nF1_score at threshold 0.3699999999999999 is 0.602\nF1_score at threshold 0.3799999999999999 is 0.602\nF1_score at threshold 0.3899999999999999 is 0.601\nF1_score at threshold 0.3999999999999998 is 0.599\nF1_score at threshold 0.4099999999999998 is 0.596\nF1_score at threshold 0.4199999999999998 is 0.595\nF1_score at threshold 0.4299999999999998 is 0.591\nF1_score at threshold 0.43999999999999984 is 0.59\nF1_score at threshold 0.44999999999999984 is 0.589\nF1_score at threshold 0.45999999999999985 is 0.585\nF1_score at threshold 0.46999999999999986 is 0.582\nF1_score at threshold 0.47999999999999976 is 0.578\nF1_score at threshold 0.48999999999999977 is 0.576\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_glove_test_y = model.predict([test_X], batch_size=1024, verbose=1)","execution_count":28,"outputs":[{"output_type":"stream","text":"375806/375806 [==============================] - 14s 38us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"del word_index, embedding_index, all_embeds, embedding_matrix, embedding_index, model\ngc.collect()\ntime.sleep(10)","execution_count":31,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'embedding_index' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-3b47e54dd48a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'embedding_index' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}