{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT on COLA dataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "62ee0f2a147e4314ad8cc6d9110ef014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e1b860c536c848099830d065aed8248a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_abf2b2c9d7ad4e788d111136434dc22e",
              "IPY_MODEL_9cd829031b8142a5bcab80931b78868b"
            ]
          }
        },
        "e1b860c536c848099830d065aed8248a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "abf2b2c9d7ad4e788d111136434dc22e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_33543d1875014cec99c7967fbc3558c2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dd855dd22fff4120b175943c5f67f684"
          }
        },
        "9cd829031b8142a5bcab80931b78868b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_26646a3afa8a4829ac2bf0fccbace10b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 2.35MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_40d6c8062a4b4fd699cbe03ab2e97602"
          }
        },
        "33543d1875014cec99c7967fbc3558c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dd855dd22fff4120b175943c5f67f684": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "26646a3afa8a4829ac2bf0fccbace10b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "40d6c8062a4b4fd699cbe03ab2e97602": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bb9269f8d50c41de9c21364c3c43131c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4e2437e2f0464ab287896e597ad215bf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9a1ea7f817a840a6b27287871289d1cd",
              "IPY_MODEL_90fc851ba25749438b5fd2cc1b0a5e99"
            ]
          }
        },
        "4e2437e2f0464ab287896e597ad215bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a1ea7f817a840a6b27287871289d1cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7af8a0fb53bc4a169584a8425504b57f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6c31cc16c00b4a17ab02fcbec8fc6120"
          }
        },
        "90fc851ba25749438b5fd2cc1b0a5e99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e43596cb8531420fb67e5244bc7593c5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:13&lt;00:00, 31.4B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bf211248730448b182ce79b073f27151"
          }
        },
        "7af8a0fb53bc4a169584a8425504b57f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6c31cc16c00b4a17ab02fcbec8fc6120": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e43596cb8531420fb67e5244bc7593c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bf211248730448b182ce79b073f27151": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e4b1bb7633b74fddb74a277e0b4dd902": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7870444763d7467bbca83d52c4927808",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0391e606cfae4dd49cb255916aa3cc54",
              "IPY_MODEL_6b400d18094c40629556464be04365eb"
            ]
          }
        },
        "7870444763d7467bbca83d52c4927808": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0391e606cfae4dd49cb255916aa3cc54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_028354343a4f42ea89ea02eb9068c3b7",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_127576feaa0e4f67bbe2e5c647e90567"
          }
        },
        "6b400d18094c40629556464be04365eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4ed7a9242daf415980673c2e5f03856d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:10&lt;00:00, 43.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_011b61555a26454bb231f55b36016b47"
          }
        },
        "028354343a4f42ea89ea02eb9068c3b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "127576feaa0e4f67bbe2e5c647e90567": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4ed7a9242daf415980673c2e5f03856d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "011b61555a26454bb231f55b36016b47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc6x4l38hCUH",
        "colab_type": "code",
        "outputId": "caaa5739-e2bb-4075-f3cb-f4d5ba5d9dcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# get the GPU device\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "if device_name == '/device:GPU:0':\n",
        "  print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "  print('No GPU found')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjGtB8rShMR3",
        "colab_type": "code",
        "outputId": "24d53bcb-5381-47e4-8714-b86b251fb13b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "  # tell PyTorch to use the GPU\n",
        "  device = torch.device('cuda')\n",
        "\n",
        "  print(\"There are %d GPUs available\" %torch.cuda.device_count())\n",
        "  print('Use GPU:', torch.cuda.get_device_name(0))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPUs available\n",
            "Use GPU: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp8JoPnGi41g",
        "colab_type": "code",
        "outputId": "1faf355c-83c9-4787-dfd6-b0f8b3e3bd6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        }
      },
      "source": [
        "! pip install Transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/97/7db72a0beef1825f82188a4b923e62a146271ac2ced7928baa4d47ef2467/transformers-2.9.1-py3-none-any.whl (641kB)\n",
            "\r\u001b[K     |▌                               | 10kB 19.7MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |████                            | 81kB 2.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 501kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 512kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 522kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 532kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 542kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 552kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 563kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 573kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 583kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 593kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 604kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 614kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 624kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 634kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from Transformers) (0.7)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 12.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from Transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from Transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from Transformers) (1.18.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 20.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from Transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from Transformers) (2.23.0)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 24.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->Transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->Transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->Transformers) (0.14.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->Transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->Transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->Transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->Transformers) (2.9)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=c21e6ea30374d62357dc2b966ce5a39b3ba60ee2bc1bc49239d917f08da5565a\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, Transformers\n",
            "Successfully installed Transformers-2.9.1 sacremoses-0.0.43 sentencepiece-0.1.90 tokenizers-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx4danQujOuR",
        "colab_type": "text"
      },
      "source": [
        "Loading the `Cola Dataset`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJpTrFiDjFjK",
        "colab_type": "code",
        "outputId": "92765b1b-d178-4a12-a814-27af9f3b11f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=94eff99bd7dcdb646fec93ba439a266a4d0ca08ddfc60829ff8791021683f2d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRO6bHrtjYvL",
        "colab_type": "code",
        "outputId": "7588ea58-eb91-4264-8a9a-bef3ba40a5e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "import wget\n",
        "\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "  wget.download(url, './cola_public_1.1.zip')\n",
        "  #!wget url\n",
        "\n",
        "for dirname, _, filenames in os.walk('./'):\n",
        "  for filename in filenames:\n",
        "    if 'cola' in filename:\n",
        "      print(os.path.join(dirname, filename))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./cola_public_1.1.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwOZjLvTjyff",
        "colab_type": "code",
        "outputId": "c39909b2-cbb6-47e1-ea3d-2e9d5f0a599f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "# unzip the data\n",
        "if not os.path.exists('./cola_public_1.1/'):\n",
        "  !unzip ./cola_public_1.1.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ./cola_public_1.1.zip\n",
            "   creating: cola_public/\n",
            "  inflating: cola_public/README      \n",
            "   creating: cola_public/tokenized/\n",
            "  inflating: cola_public/tokenized/in_domain_dev.tsv  \n",
            "  inflating: cola_public/tokenized/in_domain_train.tsv  \n",
            "  inflating: cola_public/tokenized/out_of_domain_dev.tsv  \n",
            "   creating: cola_public/raw/\n",
            "  inflating: cola_public/raw/in_domain_dev.tsv  \n",
            "  inflating: cola_public/raw/in_domain_train.tsv  \n",
            "  inflating: cola_public/raw/out_of_domain_dev.tsv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXIkyjWGly3w",
        "colab_type": "code",
        "outputId": "fd9bc3bb-3119-4f5f-ca4e-65079991983a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('./cola_public/tokenized/in_domain_train.tsv', sep='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "print(df.shape)\n",
        "df.sample(10)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8551, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1556</th>\n",
              "      <td>r-67</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>tom ordered bacon , and dick lettuce , and i t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8549</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i had the strangest feeling that i knew you .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7569</th>\n",
              "      <td>sks13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>bill 's mother saw him .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>770</th>\n",
              "      <td>bc01</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>we love they .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7840</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>the therapist 's analysis of lucy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>921</th>\n",
              "      <td>bc01</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>while holly did n't discuss a report about eve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2501</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>pauline hammered the metal flat .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8250</th>\n",
              "      <td>ad03</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>gilgamesh might have been not reading the cune...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7510</th>\n",
              "      <td>sks13</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>john hurt john with john 's umbrella when john...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4801</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>john asks which book by his father he read .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "1556            r-67  ...  tom ordered bacon , and dick lettuce , and i t...\n",
              "8549            ad03  ...      i had the strangest feeling that i knew you .\n",
              "7569           sks13  ...                           bill 's mother saw him .\n",
              "770             bc01  ...                                     we love they .\n",
              "7840            ad03  ...                  the therapist 's analysis of lucy\n",
              "921             bc01  ...  while holly did n't discuss a report about eve...\n",
              "2501            l-93  ...                  pauline hammered the metal flat .\n",
              "8250            ad03  ...  gilgamesh might have been not reading the cune...\n",
              "7510           sks13  ...  john hurt john with john 's umbrella when john...\n",
              "4801            ks08  ...       john asks which book by his father he read .\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z6tFFo2m2S1",
        "colab_type": "text"
      },
      "source": [
        "We care about `sentence` and `label`\n",
        "`label` = 0 , means not grammatically not acceptable\n",
        "`label` = 1, means grammatrically acceptable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv82lD_wm-pR",
        "colab_type": "code",
        "outputId": "5f78484e-9ff7-4904-940b-938e8348540a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7207</th>\n",
              "      <td>it is put a picture of bill on your desk befor...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2840</th>\n",
              "      <td>carrie touched the stick against the cat .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4507</th>\n",
              "      <td>mary sang a song , but lee did never .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>mickey teamed with the women up .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>john write books .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "7207  it is put a picture of bill on your desk befor...      0\n",
              "2840         carrie touched the stick against the cat .      0\n",
              "4507             mary sang a song , but lee did never .      0\n",
              "440                   mickey teamed with the women up .      0\n",
              "346                                  john write books .      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0HmVe_AnW_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the list of sentences and their lables\n",
        "\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq4M0ZrmnxeN",
        "colab_type": "text"
      },
      "source": [
        "## Using BERT tokenizer\n",
        "we need to use Bert tokenizer before feeding to BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80DKSXbinr8h",
        "colab_type": "code",
        "outputId": "c7690d83-94d9-4d02-c44f-5a95e406adea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "62ee0f2a147e4314ad8cc6d9110ef014",
            "e1b860c536c848099830d065aed8248a",
            "abf2b2c9d7ad4e788d111136434dc22e",
            "9cd829031b8142a5bcab80931b78868b",
            "33543d1875014cec99c7967fbc3558c2",
            "dd855dd22fff4120b175943c5f67f684",
            "26646a3afa8a4829ac2bf0fccbace10b",
            "40d6c8062a4b4fd699cbe03ab2e97602"
          ]
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "print('Loading Bert tokenizer.....')\n",
        "# use the uncased pretrained version of the BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Bert tokenizer.....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62ee0f2a147e4314ad8cc6d9110ef014",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUnCvhRPpYIT",
        "colab_type": "code",
        "outputId": "da8e05e4-f41b-462d-c236-0a48f734d2e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Apply to one sentence and see\n",
        "print('original sentence', sentences[0])\n",
        "print('tokenized', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# sentence mapped to token ids\n",
        "print('Token ids: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original sentence our friends wo n't buy this analysis , let alone the next one we propose .\n",
            "tokenized ['our', 'friends', 'wo', 'n', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "Token ids:  [2256, 2814, 24185, 1050, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuUbfwQ6qKi9",
        "colab_type": "text"
      },
      "source": [
        "The above can be done using `tokenizer.encode(sentences[0])`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVZwnFY4plYt",
        "colab_type": "code",
        "outputId": "982b6a00-5e46-4026-c7b0-4cd8f7ce22d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "print(sentences[0])\n",
        "tokenizer.encode(sentences[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "our friends wo n't buy this analysis , let alone the next one we propose .\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101,\n",
              " 2256,\n",
              " 2814,\n",
              " 24185,\n",
              " 1050,\n",
              " 1005,\n",
              " 1056,\n",
              " 4965,\n",
              " 2023,\n",
              " 4106,\n",
              " 1010,\n",
              " 2292,\n",
              " 2894,\n",
              " 1996,\n",
              " 2279,\n",
              " 2028,\n",
              " 2057,\n",
              " 16599,\n",
              " 1012,\n",
              " 102]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwH8Dzr1q9Pg",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTefE8zgqUn6",
        "colab_type": "code",
        "outputId": "411a7294-4663-4fbc-8db0-ab1dd296e7c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# maximum sentence length for padding and truncating\n",
        "\n",
        "max_len = 0\n",
        "for sent in sentences:\n",
        "  # tokenize the text and add the '[CLS]' and '[SEP] tokens\n",
        "  input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "  # update the max_len\n",
        "  max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length', max_len)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length 47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OggpO8IWr5ak",
        "colab_type": "text"
      },
      "source": [
        "In case there are some bigger sentences in text, maximum lenght will be set as `64` instead of `47`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSxSLrkysI9_",
        "colab_type": "text"
      },
      "source": [
        "Now we're ready to perform the real tokenization.\n",
        "\n",
        "The `tokenizer.encode_plus` function combines multiple steps for us:\n",
        "\n",
        "1. Split the sentence into tokens.\n",
        "2. Add the special `[CLS]` and `[SEP]` tokens.\n",
        "3. Map the tokens to their IDs.\n",
        "4. Pad or truncate all sentences to the same length.\n",
        "5. Create the attention masks which explicitly differentiate real tokens from `[PAD]` tokens.\n",
        "  * Mask: 1, means real token\n",
        "  * Mask: 0, mean padded token\n",
        "\n",
        "6. It returns a dictionary of input_ids, token_type_ids and the attention_mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA9yOLKRrwgk",
        "colab_type": "code",
        "outputId": "094a2933-dfb7-4e5f-de94-79667ed391d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  our friends wo n't buy this analysis , let alone the next one we propose .\n",
            "Token IDs: tensor([  101,  2256,  2814, 24185,  1050,  1005,  1056,  4965,  2023,  4106,\n",
            "         1010,  2292,  2894,  1996,  2279,  2028,  2057, 16599,  1012,   102,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKwvJBimtlUX",
        "colab_type": "code",
        "outputId": "01bc932f-3892-4ed1-ccd5-5db351b468a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "tokenizer.encode_plus(sentences[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 2256, 2814, 24185, 1050, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS4feTrCtqlq",
        "colab_type": "code",
        "outputId": "7cb59cf1-0ff7-4b19-8f4b-ca5ecb31f2c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "input_ids"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 101, 2256, 2814,  ...,    0,    0,    0],\n",
              "        [ 101, 2028, 2062,  ...,    0,    0,    0],\n",
              "        [ 101, 2028, 2062,  ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [ 101, 2009, 2003,  ...,    0,    0,    0],\n",
              "        [ 101, 1045, 2018,  ...,    0,    0,    0],\n",
              "        [ 101, 2054, 2035,  ...,    0,    0,    0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0SvPdNkRRCS",
        "colab_type": "text"
      },
      "source": [
        "## Training and Validation splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW9B-s09PgpS",
        "colab_type": "code",
        "outputId": "9c2b474c-3464-4fec-a747-9c0cb90e2684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a Tensor Dataset\n",
        "dataset  = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# create a 90-10 split for train-validation\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# divide the dataset by randomly selecting samples\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5} validation samples'.format(val_size))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7,695 training samples\n",
            "  856 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dXoFcIYZ9o8",
        "colab_type": "text"
      },
      "source": [
        "create an iterator for a dataset using the torch DataLoader class. This helps to save on memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0DjnlAuZZ9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# batch size is requires. For fine tuning BERT, author recommends a batch size of 16 or 32\n",
        "batch_size=32\n",
        "\n",
        "# Data loader for training dataset\n",
        "train_dataloader  = DataLoader(\n",
        "                      train_dataset,\n",
        "                      batch_size = batch_size,\n",
        "                      sampler = RandomSampler(train_dataset)\n",
        "                      )\n",
        "\n",
        "# Data loader for validation\n",
        "# the order does not matter and hence we will just read sequentially\n",
        "validation_dataloader = DataLoader(\n",
        "                          val_dataset,\n",
        "                          batch_size = batch_size,\n",
        "                          sampler = SequentialSampler(val_dataset) # pulls batches sequentially\n",
        "                            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1up-U0GVbU7J",
        "colab_type": "text"
      },
      "source": [
        "## Train Classification model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-e6gYaDbaOZ",
        "colab_type": "text"
      },
      "source": [
        "For this task, \n",
        "1. we first want to modify the `pre-trained BERT model` to give outputs for classification\n",
        "2. and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task.\n",
        "\n",
        "Thankfully, the **huggingface** pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. **Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task**\n",
        "\n",
        "Here is the current list of classes provided for fine-tuning:\n",
        "\n",
        "* BertModel\n",
        "* BertForPreTraining\n",
        "* BertForMaskedLM\n",
        "* BertForNextSentencePrediction\n",
        "* **BertForSequenceClassification** - The one we'll use.\n",
        "* BertForTokenClassification\n",
        "* BertForQuestionAnswering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lKj2HDNbQov",
        "colab_type": "code",
        "outputId": "2cb6a228-72ba-424a-85d4-0a551dc9177c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bb9269f8d50c41de9c21364c3c43131c",
            "4e2437e2f0464ab287896e597ad215bf",
            "9a1ea7f817a840a6b27287871289d1cd",
            "90fc851ba25749438b5fd2cc1b0a5e99",
            "7af8a0fb53bc4a169584a8425504b57f",
            "6c31cc16c00b4a17ab02fcbec8fc6120",
            "e43596cb8531420fb67e5244bc7593c5",
            "bf211248730448b182ce79b073f27151",
            "e4b1bb7633b74fddb74a277e0b4dd902",
            "7870444763d7467bbca83d52c4927808",
            "0391e606cfae4dd49cb255916aa3cc54",
            "6b400d18094c40629556464be04365eb",
            "028354343a4f42ea89ea02eb9068c3b7",
            "127576feaa0e4f67bbe2e5c647e90567",
            "4ed7a9242daf415980673c2e5f03856d",
            "011b61555a26454bb231f55b36016b47"
          ]
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# load the pretrained BERT model with a single linear classification at the top\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "              'bert-base-uncased', # use the 12 layer BERT model with uncased vocab\n",
        "              num_labels = 2, # for binary_classification\n",
        "              output_attentions = False, # whether the model returns attention weights\n",
        "              output_hidden_states = False, # whether the model outputs all hidden states\n",
        "        )\n",
        "\n",
        "# Tell pytorch to run this model on GPU\n",
        "model.cuda()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb9269f8d50c41de9c21364c3c43131c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4b1bb7633b74fddb74a277e0b4dd902",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oHpqH7TeJBf",
        "colab_type": "text"
      },
      "source": [
        "Explore all the model parameters by name\n",
        "Names and dimensions of the weights for:\n",
        "1. The embedding layer\n",
        "2. the first of the 12 transformers\n",
        "3. Output layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myscgiDfdv7r",
        "colab_type": "code",
        "outputId": "d932accd-5af1-43f1-b423-0b9091d5b6fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        }
      },
      "source": [
        "# get the model parameters as a list of tuples\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} parameters'.format(len(params)))\n",
        "\n",
        "print('\\n--------Embedding layer-------------\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "  #print('{} ------------------------ {}'.format(p[0], str(tuple(p[1].size()))))\n",
        "  # Using {:<55}  {:>12} is text aligment\n",
        "  # {:<55}: allocate 55 spaces to the left for the text, so this aligns from the left\n",
        "  # {:>12}: allocate 12 spaces starting from right, so this aligns from the right\n",
        "  print('{:<55}  {:>12}'.format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n----------First Transformer----------------\\n')\n",
        "for p in params[5:21]:\n",
        "  #print('{} ------------------------- {}'.format(p[0], str(tuple(p[1].size()))))\n",
        "  print('{:<55}  {:>12}'.format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n-----------------Output Layer--------------------\\n')\n",
        "for p in params[-4:]:\n",
        "  #print('{} -------------------------{}'.format(p[0], str(tuple(p[1].size()))))\n",
        "  print('{:<55}  {:>12}'.format(p[0], str(tuple(p[1].size()))))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 parameters\n",
            "\n",
            "--------Embedding layer-------------\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                   (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                 (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                 (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                               (768,)\n",
            "bert.embeddings.LayerNorm.bias                                 (768,)\n",
            "\n",
            "----------First Transformer----------------\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight           (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                 (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight             (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                   (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight           (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                 (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight         (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias               (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight         (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias           (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight            (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                  (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                  (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                         (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                   (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                     (768,)\n",
            "\n",
            "-----------------Output Layer--------------------\n",
            "\n",
            "bert.pooler.dense.weight                                   (768, 768)\n",
            "bert.pooler.dense.bias                                         (768,)\n",
            "classifier.weight                                            (2, 768)\n",
            "classifier.bias                                                  (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmjdLmq6oDTJ",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer and Learning Rate scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cqOgcfDoNrv",
        "colab_type": "text"
      },
      "source": [
        "Model is loaded. Now we need to grab the training `hyperparameters` from within the stored model\n",
        "\n",
        "For the purpose of tuning, it is recommended to choose from the following values based on from Appendix A.3 of the [BERT paper]((https://arxiv.org/pdf/1810.04805.pdf):\n",
        "\n",
        ">- **Batch Size**: 16, 32\n",
        "- **Learning Rate(Adam)**: 5e-5, 3e-5, 2e-5\n",
        "- **Number of epochs**: 2, 4\n",
        "\n",
        "The epsilon parameter `eps = 1e-8` is \"a very small number to prevent any division by zero in the implementation\" (from [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiPEq1Eqe8Ox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AdamW is a class from the hugging face library\n",
        "# Adam optimizer with weight decay fix\n",
        "# the parameters passed to the optmizer are the model parameters\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # initial learning rate\n",
        "                  eps = 1e-8 # default\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU_2UytBtdEh",
        "colab_type": "text"
      },
      "source": [
        "Define the `scheduler` which handles the 'learning rate decay`.\n",
        "When training a NN, you want to take large steps initially for the  initial learning decay rate and take progessively smaller steps as we horne in on the optimal value for the weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAjMa7UHpxex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# number of training epochs\n",
        "epochs =4\n",
        "\n",
        "# Total number of training steps = [number of batches]  X [number of epochs]\n",
        "# this is not the same as the number of training samples\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# create the learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "                                          optimizer,\n",
        "                                          num_warmup_steps = 0, # default value\n",
        "                                          num_training_steps = total_steps\n",
        "                                          )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O13bcsDguoNO",
        "colab_type": "text"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5iCz0ZtvhdD",
        "colab_type": "text"
      },
      "source": [
        "Below is our training loop. There's a lot going on, but fundamentally for each pass in our loop we have a trianing phase and a validation phase. \n",
        "\n",
        "\n",
        "**Training:**\n",
        "- Unpack our data inputs and labels\n",
        "- Load data onto the GPU for acceleration\n",
        "- Clear out the gradients calculated in the previous pass. \n",
        "    - In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out.\n",
        "- Forward pass (feed input data through the network)\n",
        "- Backward pass (backpropagation)\n",
        "- Tell the network to update parameters with optimizer.step()\n",
        "- Track variables for monitoring progress\n",
        "\n",
        "**Evalution:**\n",
        "- Unpack our data inputs and labels\n",
        "- Load data onto the GPU for acceleration\n",
        "- Forward pass (feed input data through the network)\n",
        "- Compute loss on our validation data and track variables for monitoring progress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO8ph_vOvqLk",
        "colab_type": "text"
      },
      "source": [
        "define a helper function for accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQdklmTsuODd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# find accuracy of prediction vs labels\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "  labels_flat = labels.flatten()\n",
        "  return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7_7vvgNyvmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper function for elaped time\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "  '''\n",
        "  Takes time in seconds and returns\n",
        "  a string in hh:mm:ss\n",
        "  '''\n",
        "  elapsed_rounded = int(round(elapsed))\n",
        "\n",
        "  # format in hh:mm:ss\n",
        "  return str(datetime.timedelta(seconds = elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CkXwCnazzXR",
        "colab_type": "text"
      },
      "source": [
        "## Kick of training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWkX-S_XYSR5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "outputId": "361029b0-46a2-43e8-cea4-ca8aa83d7f2b"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "epochs=4\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings\n",
        "training_stats= []\n",
        "\n",
        "# measure total time take for the whole run\n",
        "total_t0 = time.time()\n",
        "\n",
        "# for each epoch\n",
        "for epoch_i in range(0, epochs):\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    # perform one full pass on the training set\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # measure how long the training epoch takes\n",
        "    t0 = time.time()\n",
        "\n",
        "    # reset the training loss for this epoch\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # put the model in 'train' mode\n",
        "    model.train()\n",
        "\n",
        "    # for each batch of the training data\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        \n",
        "        # progress update every 40 batches\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "          # calculate the elaped time\n",
        "          elapsed = format_time(time.time() - t0)\n",
        "\n",
        "          # report progress\n",
        "          print('Batch: {} of {}.. Elaped time: {:}'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass\n",
        "\n",
        "        model.zero_grad()\n",
        "        \n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        loss, logits = model(\n",
        "                            b_input_ids,\n",
        "                            attention_mask = b_input_mask,\n",
        "                            labels = b_labels,\n",
        "                            token_type_ids = None\n",
        "                          )\n",
        "        \n",
        "        # accumulate the training loss over all batches\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # perform backward pass to calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # update learning rate        \n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss across all batches\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # measure how long this epoch took\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # put the model in evalaution mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # evaluate validation set for each epoch\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_attention_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "\n",
        "            (loss, logits) = model(\n",
        "                                  b_input_ids,\n",
        "                                  attention_mask = b_attention_mask,\n",
        "                                  labels = b_labels,\n",
        "                                  token_type_ids = None\n",
        "                                )\n",
        "            \n",
        "        # accumulate the validation loss\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # move the logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids =  b_labels.to('cpu').numpy()\n",
        "\n",
        "        # calaculate the  accuracy for this batch and accumulate over all batches\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    # report the final accuracy for this validation run\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print('Average Accuracy for this validation run is {:.2f}'.format(avg_val_accuracy))\n",
        "\n",
        "    # calculate the average loss across all batches\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "    # measure time taken\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    print('Average validation loss: {:.2f}'.format(avg_val_loss))\n",
        "    print('Time taken for validation {}'.format(validation_time))\n",
        "\n",
        "    training_stats.append(\n",
        "                {\n",
        "                'epoch': epoch_i + 1,\n",
        "                'Training Loss': avg_train_loss,\n",
        "                'Valid. Loss': avg_val_loss,\n",
        "                'Valid. Accur.': avg_val_accuracy,\n",
        "                'Training Time': training_time,\n",
        "                'Validation Time': validation_time   \n",
        "               }\n",
        "                )\n",
        "    \n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch: 40 of 241.. Elaped time: 0:00:24\n",
            "Batch: 80 of 241.. Elaped time: 0:00:48\n",
            "Batch: 120 of 241.. Elaped time: 0:01:13\n",
            "Batch: 160 of 241.. Elaped time: 0:01:37\n",
            "Batch: 200 of 241.. Elaped time: 0:02:02\n",
            "Batch: 240 of 241.. Elaped time: 0:02:26\n",
            "\n",
            "  Average training loss: 0.51\n",
            "  Training epoch took: 0:02:26\n",
            "\n",
            "Running Validation...\n",
            "Average Accuracy for this validation run is 0.83\n",
            "Average validation loss: 0.40\n",
            "Time taken for validation 0:00:05\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "Batch: 40 of 241.. Elaped time: 0:00:24\n",
            "Batch: 80 of 241.. Elaped time: 0:00:49\n",
            "Batch: 120 of 241.. Elaped time: 0:01:13\n",
            "Batch: 160 of 241.. Elaped time: 0:01:38\n",
            "Batch: 200 of 241.. Elaped time: 0:02:02\n",
            "Batch: 240 of 241.. Elaped time: 0:02:27\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0:02:27\n",
            "\n",
            "Running Validation...\n",
            "Average Accuracy for this validation run is 0.86\n",
            "Average validation loss: 0.36\n",
            "Time taken for validation 0:00:05\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:05:05 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KifeD2WuoZRh",
        "colab_type": "text"
      },
      "source": [
        "## Summary of the training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4PaWboXmAtk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "6561e43e-e12a-47ea-d28e-0a96ff89b475"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# create a dataframe from the training stats\n",
        "df_stats =  pd.DataFrame(data = training_stats)\n",
        "\n",
        "df_stats"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0:02:26</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0:02:27</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   epoch  Training Loss  ...  Training Time  Validation Time\n",
              "0      1           0.51  ...        0:02:26          0:00:05\n",
              "1      2           0.32  ...        0:02:27          0:00:05\n",
              "\n",
              "[2 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyEH56V3mXhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}