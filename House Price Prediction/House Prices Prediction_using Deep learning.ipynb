{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\n/kaggle/input/house-prices-advanced-regression-techniques/test.csv\n/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\n/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\n/kaggle/input/xgbregressor-model/finalized_XGBRegressor_model.pkl\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ndf_test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\ndf_train.head()","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n\n  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n\n  YrSold  SaleType  SaleCondition  SalePrice  \n0   2008        WD         Normal     208500  \n1   2007        WD         Normal     181500  \n2   2008        WD         Normal     223500  \n3   2006        WD        Abnorml     140000  \n4   2008        WD         Normal     250000  \n\n[5 rows x 81 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>Alley</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>...</th>\n      <th>PoolArea</th>\n      <th>PoolQC</th>\n      <th>Fence</th>\n      <th>MiscFeature</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>208500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>181500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>223500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>140000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>250000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 81 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Size of training set is : {}\\n\\n'.format(df_train.shape))\ndisplay(df_train.info())\nprint('Size of test set is : {}\\n\\n'.format(df_test.shape))\ndisplay(df_test.info())","execution_count":3,"outputs":[{"output_type":"stream","text":"Size of training set is : (1460, 81)\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1460 entries, 0 to 1459\nData columns (total 81 columns):\nId               1460 non-null int64\nMSSubClass       1460 non-null int64\nMSZoning         1460 non-null object\nLotFrontage      1201 non-null float64\nLotArea          1460 non-null int64\nStreet           1460 non-null object\nAlley            91 non-null object\nLotShape         1460 non-null object\nLandContour      1460 non-null object\nUtilities        1460 non-null object\nLotConfig        1460 non-null object\nLandSlope        1460 non-null object\nNeighborhood     1460 non-null object\nCondition1       1460 non-null object\nCondition2       1460 non-null object\nBldgType         1460 non-null object\nHouseStyle       1460 non-null object\nOverallQual      1460 non-null int64\nOverallCond      1460 non-null int64\nYearBuilt        1460 non-null int64\nYearRemodAdd     1460 non-null int64\nRoofStyle        1460 non-null object\nRoofMatl         1460 non-null object\nExterior1st      1460 non-null object\nExterior2nd      1460 non-null object\nMasVnrType       1452 non-null object\nMasVnrArea       1452 non-null float64\nExterQual        1460 non-null object\nExterCond        1460 non-null object\nFoundation       1460 non-null object\nBsmtQual         1423 non-null object\nBsmtCond         1423 non-null object\nBsmtExposure     1422 non-null object\nBsmtFinType1     1423 non-null object\nBsmtFinSF1       1460 non-null int64\nBsmtFinType2     1422 non-null object\nBsmtFinSF2       1460 non-null int64\nBsmtUnfSF        1460 non-null int64\nTotalBsmtSF      1460 non-null int64\nHeating          1460 non-null object\nHeatingQC        1460 non-null object\nCentralAir       1460 non-null object\nElectrical       1459 non-null object\n1stFlrSF         1460 non-null int64\n2ndFlrSF         1460 non-null int64\nLowQualFinSF     1460 non-null int64\nGrLivArea        1460 non-null int64\nBsmtFullBath     1460 non-null int64\nBsmtHalfBath     1460 non-null int64\nFullBath         1460 non-null int64\nHalfBath         1460 non-null int64\nBedroomAbvGr     1460 non-null int64\nKitchenAbvGr     1460 non-null int64\nKitchenQual      1460 non-null object\nTotRmsAbvGrd     1460 non-null int64\nFunctional       1460 non-null object\nFireplaces       1460 non-null int64\nFireplaceQu      770 non-null object\nGarageType       1379 non-null object\nGarageYrBlt      1379 non-null float64\nGarageFinish     1379 non-null object\nGarageCars       1460 non-null int64\nGarageArea       1460 non-null int64\nGarageQual       1379 non-null object\nGarageCond       1379 non-null object\nPavedDrive       1460 non-null object\nWoodDeckSF       1460 non-null int64\nOpenPorchSF      1460 non-null int64\nEnclosedPorch    1460 non-null int64\n3SsnPorch        1460 non-null int64\nScreenPorch      1460 non-null int64\nPoolArea         1460 non-null int64\nPoolQC           7 non-null object\nFence            281 non-null object\nMiscFeature      54 non-null object\nMiscVal          1460 non-null int64\nMoSold           1460 non-null int64\nYrSold           1460 non-null int64\nSaleType         1460 non-null object\nSaleCondition    1460 non-null object\nSalePrice        1460 non-null int64\ndtypes: float64(3), int64(35), object(43)\nmemory usage: 924.0+ KB\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}},{"output_type":"stream","text":"Size of test set is : (1459, 80)\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1459 entries, 0 to 1458\nData columns (total 80 columns):\nId               1459 non-null int64\nMSSubClass       1459 non-null int64\nMSZoning         1455 non-null object\nLotFrontage      1232 non-null float64\nLotArea          1459 non-null int64\nStreet           1459 non-null object\nAlley            107 non-null object\nLotShape         1459 non-null object\nLandContour      1459 non-null object\nUtilities        1457 non-null object\nLotConfig        1459 non-null object\nLandSlope        1459 non-null object\nNeighborhood     1459 non-null object\nCondition1       1459 non-null object\nCondition2       1459 non-null object\nBldgType         1459 non-null object\nHouseStyle       1459 non-null object\nOverallQual      1459 non-null int64\nOverallCond      1459 non-null int64\nYearBuilt        1459 non-null int64\nYearRemodAdd     1459 non-null int64\nRoofStyle        1459 non-null object\nRoofMatl         1459 non-null object\nExterior1st      1458 non-null object\nExterior2nd      1458 non-null object\nMasVnrType       1443 non-null object\nMasVnrArea       1444 non-null float64\nExterQual        1459 non-null object\nExterCond        1459 non-null object\nFoundation       1459 non-null object\nBsmtQual         1415 non-null object\nBsmtCond         1414 non-null object\nBsmtExposure     1415 non-null object\nBsmtFinType1     1417 non-null object\nBsmtFinSF1       1458 non-null float64\nBsmtFinType2     1417 non-null object\nBsmtFinSF2       1458 non-null float64\nBsmtUnfSF        1458 non-null float64\nTotalBsmtSF      1458 non-null float64\nHeating          1459 non-null object\nHeatingQC        1459 non-null object\nCentralAir       1459 non-null object\nElectrical       1459 non-null object\n1stFlrSF         1459 non-null int64\n2ndFlrSF         1459 non-null int64\nLowQualFinSF     1459 non-null int64\nGrLivArea        1459 non-null int64\nBsmtFullBath     1457 non-null float64\nBsmtHalfBath     1457 non-null float64\nFullBath         1459 non-null int64\nHalfBath         1459 non-null int64\nBedroomAbvGr     1459 non-null int64\nKitchenAbvGr     1459 non-null int64\nKitchenQual      1458 non-null object\nTotRmsAbvGrd     1459 non-null int64\nFunctional       1457 non-null object\nFireplaces       1459 non-null int64\nFireplaceQu      729 non-null object\nGarageType       1383 non-null object\nGarageYrBlt      1381 non-null float64\nGarageFinish     1381 non-null object\nGarageCars       1458 non-null float64\nGarageArea       1458 non-null float64\nGarageQual       1381 non-null object\nGarageCond       1381 non-null object\nPavedDrive       1459 non-null object\nWoodDeckSF       1459 non-null int64\nOpenPorchSF      1459 non-null int64\nEnclosedPorch    1459 non-null int64\n3SsnPorch        1459 non-null int64\nScreenPorch      1459 non-null int64\nPoolArea         1459 non-null int64\nPoolQC           3 non-null object\nFence            290 non-null object\nMiscFeature      51 non-null object\nMiscVal          1459 non-null int64\nMoSold           1459 non-null int64\nYrSold           1459 non-null int64\nSaleType         1458 non-null object\nSaleCondition    1459 non-null object\ndtypes: float64(11), int64(26), object(43)\nmemory usage: 912.0+ KB\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":" ## 1. Handling missing values in data"},{"metadata":{},"cell_type":"markdown","source":"### Check for columns with missing values and the count"},{"metadata":{"trusted":true},"cell_type":"code","source":"null = df_train.isnull().sum()\n#display(null)\nnull_df = pd.DataFrame({'features': null.index, 'count': null.values})\ndisplay(null_df[null_df['count'] > 0])\n\nprint('Max entries per column in the dataset is: {}'.format(df_train.shape[0]))\nprint('Columns with more than 50% of the data missing in the training data is \\n: {}'.format(null_df[null_df['count'] > df_train.shape[0]*0.5]))\n#null_df[null_df['count'] > df_train.shape[0]*0.5]","execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"        features  count\n3    LotFrontage    259\n6          Alley   1369\n25    MasVnrType      8\n26    MasVnrArea      8\n30      BsmtQual     37\n31      BsmtCond     37\n32  BsmtExposure     38\n33  BsmtFinType1     37\n35  BsmtFinType2     38\n42    Electrical      1\n57   FireplaceQu    690\n58    GarageType     81\n59   GarageYrBlt     81\n60  GarageFinish     81\n63    GarageQual     81\n64    GarageCond     81\n72        PoolQC   1453\n73         Fence   1179\n74   MiscFeature   1406","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>features</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>LotFrontage</td>\n      <td>259</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Alley</td>\n      <td>1369</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>MasVnrType</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>MasVnrArea</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>BsmtQual</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>BsmtCond</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>BsmtExposure</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>BsmtFinType1</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>BsmtFinType2</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>Electrical</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>FireplaceQu</td>\n      <td>690</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>GarageType</td>\n      <td>81</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>GarageYrBlt</td>\n      <td>81</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>GarageFinish</td>\n      <td>81</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>GarageQual</td>\n      <td>81</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>GarageCond</td>\n      <td>81</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>PoolQC</td>\n      <td>1453</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>Fence</td>\n      <td>1179</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>MiscFeature</td>\n      <td>1406</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"stream","text":"Max entries per column in the dataset is: 1460\nColumns with more than 50% of the data missing in the training data is \n:        features  count\n6         Alley   1369\n72       PoolQC   1453\n73        Fence   1179\n74  MiscFeature   1406\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_null(df):\n    null = df.isnull().sum()\n    #display(null)\n    null_df = pd.DataFrame({'features': null.index, 'count': null.values})\n    return null_df[null_df['count'] > 0]","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_null_50(df):\n    null = df.isnull().sum()\n    #display(null)\n    null_df = pd.DataFrame({'features': null.index, 'count': null.values})\n    return null_df[null_df['count'] > df.shape[0]*0.5]","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Columns with more than 50% of the data missing in the training data are')\ncheck_null_50(df_train)","execution_count":7,"outputs":[{"output_type":"stream","text":"Columns with more than 50% of the data missing in the training data are\n","name":"stdout"},{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"       features  count\n6         Alley   1369\n72       PoolQC   1453\n73        Fence   1179\n74  MiscFeature   1406","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>features</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>Alley</td>\n      <td>1369</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>PoolQC</td>\n      <td>1453</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>Fence</td>\n      <td>1179</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>MiscFeature</td>\n      <td>1406</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Columns with more than 50% of the data missing in the test data are')\ncheck_null_50(df_test)","execution_count":8,"outputs":[{"output_type":"stream","text":"Columns with more than 50% of the data missing in the test data are\n","name":"stdout"},{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"       features  count\n6         Alley   1352\n57  FireplaceQu    730\n72       PoolQC   1456\n73        Fence   1169\n74  MiscFeature   1408","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>features</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>Alley</td>\n      <td>1352</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>FireplaceQu</td>\n      <td>730</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>PoolQC</td>\n      <td>1456</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>Fence</td>\n      <td>1169</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>MiscFeature</td>\n      <td>1408</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.heatmap(df_train.isnull(), yticklabels=False, cbar=False)","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f0a98caaf28>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# features to remove, which have more than 50% missing data\nremove_features = null_df[null_df['count'] > df_train.shape[0]*0.5].features.tolist()\nremove_features","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"['Alley', 'PoolQC', 'Fence', 'MiscFeature']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove the columns from the train and test sets\ndf_train.drop(remove_features, axis=1, inplace=True)\ndf_test.drop(remove_features, axis=1, inplace=True)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove the 'Id'column in both train and test\ndf_train.drop('Id', axis=1, inplace=True)\ndf_test.drop('Id', axis=1, inplace=True)\ndf_train.head()","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape LandContour  \\\n0          60       RL         65.0     8450   Pave      Reg         Lvl   \n1          20       RL         80.0     9600   Pave      Reg         Lvl   \n2          60       RL         68.0    11250   Pave      IR1         Lvl   \n3          70       RL         60.0     9550   Pave      IR1         Lvl   \n4          60       RL         84.0    14260   Pave      IR1         Lvl   \n\n  Utilities LotConfig LandSlope  ... EnclosedPorch 3SsnPorch ScreenPorch  \\\n0    AllPub    Inside       Gtl  ...             0         0           0   \n1    AllPub       FR2       Gtl  ...             0         0           0   \n2    AllPub    Inside       Gtl  ...             0         0           0   \n3    AllPub    Corner       Gtl  ...           272         0           0   \n4    AllPub       FR2       Gtl  ...             0         0           0   \n\n  PoolArea MiscVal  MoSold  YrSold  SaleType  SaleCondition SalePrice  \n0        0       0       2    2008        WD         Normal    208500  \n1        0       0       5    2007        WD         Normal    181500  \n2        0       0       9    2008        WD         Normal    223500  \n3        0       0       2    2006        WD        Abnorml    140000  \n4        0       0      12    2008        WD         Normal    250000  \n\n[5 rows x 76 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>LotConfig</th>\n      <th>LandSlope</th>\n      <th>...</th>\n      <th>EnclosedPorch</th>\n      <th>3SsnPorch</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>Pave</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>208500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>Pave</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>FR2</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>181500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>Pave</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>223500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>Pave</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Corner</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>272</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>140000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>60</td>\n      <td>RL</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>Pave</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>FR2</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>250000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 76 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(df_train.shape)\ndisplay(df_test.shape)","execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"(1460, 76)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(1459, 75)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_train.dtypes\n# display(check_null(df_train))\n# display(check_null(df_test))","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_train[df_train.isnull().any(axis=1)]\n# select the object data types\ndf_train_obj = df_train.select_dtypes(include=['object']).copy()\n\n# check for missing in object type\n#check_null(df_train_obj)\ndf_train_obj.empty","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"False"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Fill missing values in train and test sets\n### 1. Replace all numeric features with the mean\n### 2. Replace all categorical features with the mode"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_missing(df):\n    df_obj = df.select_dtypes(include=['object']).copy()\n    if df_obj.empty==False:\n        for col in check_null(df_obj)['features'].tolist():\n            df[col] = df[col].fillna(df[col].mode()[0])\n    \n    df_numeric = df.select_dtypes(include=['int64', 'float64']).copy()\n    if df_numeric.empty==False:\n        for col in check_null(df_numeric)['features'].tolist():\n            df[col] = df[col].fillna(df[col].mean())","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fill_missing(df_train)\nfill_missing(df_test)","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for any missing values in both numeric and categorical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"check_null(df_train)\ncheck_null(df_test)","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"Empty DataFrame\nColumns: [features, count]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>features</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for col in check_null(df_train_obj)['features'].tolist():\n#     df_train[col] = df_train[col].fillna(df_train[col].mode()[0])","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_train['FireplaceQu'].mode()[0]\n# check_null(df_train)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train_numeric = df_train.select_dtypes(include=['int64', 'float64']).copy()\n# check_null(df_train_numeric)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for col in check_null(df_train_numeric)['features'].tolist():\n#     df_train[col] = df_train[col].fillna(df_train[col].mean())\n\n# # check for any missing values\n# check_null(df_train)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train.info()\n# print('\\n\\nFinal training data shape is {}'.format(df_train.shape))","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train_obj['MSZoning'].value_counts()\n#df_train_obj['MSZoning']","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pd.get_dummies(df_train_obj['MSZoning'], drop_first=True)","execution_count":25,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Since there can be sub-categories in test that are not in train, lets combine them together and encode them"},{"metadata":{},"cell_type":"markdown","source":"### Combine train and test sets before encoding the categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ref: https://www.datacamp.com/community/tutorials/joining-dataframes-pandas\n\ncombined_df = pd.concat([df_train, df_test], axis=0, sort=False, keys=['train', 'test']) # row wise\ncombined_df","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"            MSSubClass MSZoning  LotFrontage  LotArea Street LotShape  \\\ntrain 0             60       RL         65.0     8450   Pave      Reg   \n      1             20       RL         80.0     9600   Pave      Reg   \n      2             60       RL         68.0    11250   Pave      IR1   \n      3             70       RL         60.0     9550   Pave      IR1   \n      4             60       RL         84.0    14260   Pave      IR1   \n...                ...      ...          ...      ...    ...      ...   \ntest  1454         160       RM         21.0     1936   Pave      Reg   \n      1455         160       RM         21.0     1894   Pave      Reg   \n      1456          20       RL        160.0    20000   Pave      Reg   \n      1457          85       RL         62.0    10441   Pave      Reg   \n      1458          60       RL         74.0     9627   Pave      Reg   \n\n           LandContour Utilities LotConfig LandSlope  ... EnclosedPorch  \\\ntrain 0            Lvl    AllPub    Inside       Gtl  ...             0   \n      1            Lvl    AllPub       FR2       Gtl  ...             0   \n      2            Lvl    AllPub    Inside       Gtl  ...             0   \n      3            Lvl    AllPub    Corner       Gtl  ...           272   \n      4            Lvl    AllPub       FR2       Gtl  ...             0   \n...                ...       ...       ...       ...  ...           ...   \ntest  1454         Lvl    AllPub    Inside       Gtl  ...             0   \n      1455         Lvl    AllPub    Inside       Gtl  ...             0   \n      1456         Lvl    AllPub    Inside       Gtl  ...             0   \n      1457         Lvl    AllPub    Inside       Gtl  ...             0   \n      1458         Lvl    AllPub    Inside       Mod  ...             0   \n\n           3SsnPorch ScreenPorch PoolArea MiscVal  MoSold  YrSold  SaleType  \\\ntrain 0            0           0        0       0       2    2008        WD   \n      1            0           0        0       0       5    2007        WD   \n      2            0           0        0       0       9    2008        WD   \n      3            0           0        0       0       2    2006        WD   \n      4            0           0        0       0      12    2008        WD   \n...              ...         ...      ...     ...     ...     ...       ...   \ntest  1454         0           0        0       0       6    2006        WD   \n      1455         0           0        0       0       4    2006        WD   \n      1456         0           0        0       0       9    2006        WD   \n      1457         0           0        0     700       7    2006        WD   \n      1458         0           0        0       0      11    2006        WD   \n\n            SaleCondition SalePrice  \ntrain 0            Normal  208500.0  \n      1            Normal  181500.0  \n      2            Normal  223500.0  \n      3           Abnorml  140000.0  \n      4            Normal  250000.0  \n...                   ...       ...  \ntest  1454         Normal       NaN  \n      1455        Abnorml       NaN  \n      1456        Abnorml       NaN  \n      1457         Normal       NaN  \n      1458         Normal       NaN  \n\n[2919 rows x 76 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>LotConfig</th>\n      <th>LandSlope</th>\n      <th>...</th>\n      <th>EnclosedPorch</th>\n      <th>3SsnPorch</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">train</th>\n      <th>0</th>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>Pave</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>208500.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>Pave</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>FR2</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>181500.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>Pave</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>223500.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>Pave</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Corner</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>272</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>140000.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>60</td>\n      <td>RL</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>Pave</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>FR2</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>250000.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">test</th>\n      <th>1454</th>\n      <td>160</td>\n      <td>RM</td>\n      <td>21.0</td>\n      <td>1936</td>\n      <td>Pave</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1455</th>\n      <td>160</td>\n      <td>RM</td>\n      <td>21.0</td>\n      <td>1894</td>\n      <td>Pave</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1456</th>\n      <td>20</td>\n      <td>RL</td>\n      <td>160.0</td>\n      <td>20000</td>\n      <td>Pave</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1457</th>\n      <td>85</td>\n      <td>RL</td>\n      <td>62.0</td>\n      <td>10441</td>\n      <td>Pave</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>700</td>\n      <td>7</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1458</th>\n      <td>60</td>\n      <td>RL</td>\n      <td>74.0</td>\n      <td>9627</td>\n      <td>Pave</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Mod</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>2919 rows Ã— 76 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to get the train and test sets back, use the 'keys'\ncombined_df.loc['train']\ncombined_df.loc['test']","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"      MSSubClass MSZoning  LotFrontage  LotArea Street LotShape LandContour  \\\n0             20       RH         80.0    11622   Pave      Reg         Lvl   \n1             20       RL         81.0    14267   Pave      IR1         Lvl   \n2             60       RL         74.0    13830   Pave      IR1         Lvl   \n3             60       RL         78.0     9978   Pave      IR1         Lvl   \n4            120       RL         43.0     5005   Pave      IR1         HLS   \n...          ...      ...          ...      ...    ...      ...         ...   \n1454         160       RM         21.0     1936   Pave      Reg         Lvl   \n1455         160       RM         21.0     1894   Pave      Reg         Lvl   \n1456          20       RL        160.0    20000   Pave      Reg         Lvl   \n1457          85       RL         62.0    10441   Pave      Reg         Lvl   \n1458          60       RL         74.0     9627   Pave      Reg         Lvl   \n\n     Utilities LotConfig LandSlope  ... EnclosedPorch 3SsnPorch ScreenPorch  \\\n0       AllPub    Inside       Gtl  ...             0         0         120   \n1       AllPub    Corner       Gtl  ...             0         0           0   \n2       AllPub    Inside       Gtl  ...             0         0           0   \n3       AllPub    Inside       Gtl  ...             0         0           0   \n4       AllPub    Inside       Gtl  ...             0         0         144   \n...        ...       ...       ...  ...           ...       ...         ...   \n1454    AllPub    Inside       Gtl  ...             0         0           0   \n1455    AllPub    Inside       Gtl  ...             0         0           0   \n1456    AllPub    Inside       Gtl  ...             0         0           0   \n1457    AllPub    Inside       Gtl  ...             0         0           0   \n1458    AllPub    Inside       Mod  ...             0         0           0   \n\n     PoolArea MiscVal  MoSold  YrSold  SaleType  SaleCondition SalePrice  \n0           0       0       6    2010        WD         Normal       NaN  \n1           0   12500       6    2010        WD         Normal       NaN  \n2           0       0       3    2010        WD         Normal       NaN  \n3           0       0       6    2010        WD         Normal       NaN  \n4           0       0       1    2010        WD         Normal       NaN  \n...       ...     ...     ...     ...       ...            ...       ...  \n1454        0       0       6    2006        WD         Normal       NaN  \n1455        0       0       4    2006        WD        Abnorml       NaN  \n1456        0       0       9    2006        WD        Abnorml       NaN  \n1457        0     700       7    2006        WD         Normal       NaN  \n1458        0       0      11    2006        WD         Normal       NaN  \n\n[1459 rows x 76 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>LotConfig</th>\n      <th>LandSlope</th>\n      <th>...</th>\n      <th>EnclosedPorch</th>\n      <th>3SsnPorch</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20</td>\n      <td>RH</td>\n      <td>80.0</td>\n      <td>11622</td>\n      <td>Pave</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>120</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20</td>\n      <td>RL</td>\n      <td>81.0</td>\n      <td>14267</td>\n      <td>Pave</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Corner</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>12500</td>\n      <td>6</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>60</td>\n      <td>RL</td>\n      <td>74.0</td>\n      <td>13830</td>\n      <td>Pave</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>60</td>\n      <td>RL</td>\n      <td>78.0</td>\n      <td>9978</td>\n      <td>Pave</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>120</td>\n      <td>RL</td>\n      <td>43.0</td>\n      <td>5005</td>\n      <td>Pave</td>\n      <td>IR1</td>\n      <td>HLS</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>144</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1454</th>\n      <td>160</td>\n      <td>RM</td>\n      <td>21.0</td>\n      <td>1936</td>\n      <td>Pave</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1455</th>\n      <td>160</td>\n      <td>RM</td>\n      <td>21.0</td>\n      <td>1894</td>\n      <td>Pave</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1456</th>\n      <td>20</td>\n      <td>RL</td>\n      <td>160.0</td>\n      <td>20000</td>\n      <td>Pave</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1457</th>\n      <td>85</td>\n      <td>RL</td>\n      <td>62.0</td>\n      <td>10441</td>\n      <td>Pave</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>700</td>\n      <td>7</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1458</th>\n      <td>60</td>\n      <td>RL</td>\n      <td>74.0</td>\n      <td>9627</td>\n      <td>Pave</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Mod</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>1459 rows Ã— 76 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### One hot encoding of the categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"#for col in df_train_obj.columns.tolist():\n# Ref: https://pbpython.com/categorical-encoding.html\n\n# df_train_obj_ohe = pd.get_dummies(df_train_obj, columns = df_train_obj.columns.tolist())\n\n# #df_train_obj_ohe\n# # drop the columns for which we have encoded from df_train\n# cat_cols_drop = df_train_obj.columns.tolist()\n# df_train.drop(cat_cols_drop, axis=1, inplace=True)\n\n# df_train_final = pd.concat([df_train, df_train_obj_ohe], axis=1)\n# df_train_final","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cat_ohe_multcols(df):\n    df_obj = df.select_dtypes(include=['object']).copy()\n    columns = df_obj.columns.tolist()\n    df_obj_ohe = pd.get_dummies(df, columns= columns, drop_first=True)\n    return df_obj_ohe","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df = cat_ohe_multcols(combined_df)\n#temp = pd.concat([combined_df,onehot_df], axis=1)\n\n# remove the duplicated columns\n#final_df = temp.loc[:, ~temp.columns.duplicated()]","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df['SalePrice']","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"train  0       208500.0\n       1       181500.0\n       2       223500.0\n       3       140000.0\n       4       250000.0\n                 ...   \ntest   1454         NaN\n       1455         NaN\n       1456         NaN\n       1457         NaN\n       1458         NaN\nName: SalePrice, Length: 2919, dtype: float64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df.shape","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"(2919, 237)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for duplicate columns\nnp.unique(final_df.columns.duplicated())","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"array([False])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"            MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  \\\ntrain 0             60         65.0     8450            7            5   \n      1             20         80.0     9600            6            8   \n      2             60         68.0    11250            7            5   \n      3             70         60.0     9550            7            5   \n      4             60         84.0    14260            8            5   \n...                ...          ...      ...          ...          ...   \ntest  1454         160         21.0     1936            4            7   \n      1455         160         21.0     1894            4            5   \n      1456          20        160.0    20000            5            7   \n      1457          85         62.0    10441            5            5   \n      1458          60         74.0     9627            7            5   \n\n            YearBuilt  YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  \\\ntrain 0          2003          2003       196.0       706.0         0.0  ...   \n      1          1976          1976         0.0       978.0         0.0  ...   \n      2          2001          2002       162.0       486.0         0.0  ...   \n      3          1915          1970         0.0       216.0         0.0  ...   \n      4          2000          2000       350.0       655.0         0.0  ...   \n...               ...           ...         ...         ...         ...  ...   \ntest  1454       1970          1970         0.0         0.0         0.0  ...   \n      1455       1970          1970         0.0       252.0         0.0  ...   \n      1456       1960          1996         0.0      1224.0         0.0  ...   \n      1457       1992          1992         0.0       337.0         0.0  ...   \n      1458       1993          1994        94.0       758.0         0.0  ...   \n\n            SaleType_ConLI  SaleType_ConLw  SaleType_New  SaleType_Oth  \\\ntrain 0                  0               0             0             0   \n      1                  0               0             0             0   \n      2                  0               0             0             0   \n      3                  0               0             0             0   \n      4                  0               0             0             0   \n...                    ...             ...           ...           ...   \ntest  1454               0               0             0             0   \n      1455               0               0             0             0   \n      1456               0               0             0             0   \n      1457               0               0             0             0   \n      1458               0               0             0             0   \n\n            SaleType_WD  SaleCondition_AdjLand  SaleCondition_Alloca  \\\ntrain 0               1                      0                     0   \n      1               1                      0                     0   \n      2               1                      0                     0   \n      3               1                      0                     0   \n      4               1                      0                     0   \n...                 ...                    ...                   ...   \ntest  1454            1                      0                     0   \n      1455            1                      0                     0   \n      1456            1                      0                     0   \n      1457            1                      0                     0   \n      1458            1                      0                     0   \n\n            SaleCondition_Family  SaleCondition_Normal  SaleCondition_Partial  \ntrain 0                        0                     1                      0  \n      1                        0                     1                      0  \n      2                        0                     1                      0  \n      3                        0                     0                      0  \n      4                        0                     1                      0  \n...                          ...                   ...                    ...  \ntest  1454                     0                     1                      0  \n      1455                     0                     0                      0  \n      1456                     0                     0                      0  \n      1457                     0                     1                      0  \n      1458                     0                     1                      0  \n\n[2919 rows x 237 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>MSSubClass</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>OverallQual</th>\n      <th>OverallCond</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n      <th>MasVnrArea</th>\n      <th>BsmtFinSF1</th>\n      <th>BsmtFinSF2</th>\n      <th>...</th>\n      <th>SaleType_ConLI</th>\n      <th>SaleType_ConLw</th>\n      <th>SaleType_New</th>\n      <th>SaleType_Oth</th>\n      <th>SaleType_WD</th>\n      <th>SaleCondition_AdjLand</th>\n      <th>SaleCondition_Alloca</th>\n      <th>SaleCondition_Family</th>\n      <th>SaleCondition_Normal</th>\n      <th>SaleCondition_Partial</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">train</th>\n      <th>0</th>\n      <td>60</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>7</td>\n      <td>5</td>\n      <td>2003</td>\n      <td>2003</td>\n      <td>196.0</td>\n      <td>706.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>6</td>\n      <td>8</td>\n      <td>1976</td>\n      <td>1976</td>\n      <td>0.0</td>\n      <td>978.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>60</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>7</td>\n      <td>5</td>\n      <td>2001</td>\n      <td>2002</td>\n      <td>162.0</td>\n      <td>486.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>70</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>7</td>\n      <td>5</td>\n      <td>1915</td>\n      <td>1970</td>\n      <td>0.0</td>\n      <td>216.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>60</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>8</td>\n      <td>5</td>\n      <td>2000</td>\n      <td>2000</td>\n      <td>350.0</td>\n      <td>655.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">test</th>\n      <th>1454</th>\n      <td>160</td>\n      <td>21.0</td>\n      <td>1936</td>\n      <td>4</td>\n      <td>7</td>\n      <td>1970</td>\n      <td>1970</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1455</th>\n      <td>160</td>\n      <td>21.0</td>\n      <td>1894</td>\n      <td>4</td>\n      <td>5</td>\n      <td>1970</td>\n      <td>1970</td>\n      <td>0.0</td>\n      <td>252.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1456</th>\n      <td>20</td>\n      <td>160.0</td>\n      <td>20000</td>\n      <td>5</td>\n      <td>7</td>\n      <td>1960</td>\n      <td>1996</td>\n      <td>0.0</td>\n      <td>1224.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1457</th>\n      <td>85</td>\n      <td>62.0</td>\n      <td>10441</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1992</td>\n      <td>1992</td>\n      <td>0.0</td>\n      <td>337.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1458</th>\n      <td>60</td>\n      <td>74.0</td>\n      <td>9627</td>\n      <td>7</td>\n      <td>5</td>\n      <td>1993</td>\n      <td>1994</td>\n      <td>94.0</td>\n      <td>758.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2919 rows Ã— 237 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Now that the categorical variables have been 'ohe' get the train and test sets back"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Train = final_df.loc['train']\ndf_Test = final_df.loc['test']","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Train.head()","execution_count":36,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n0          60         65.0     8450            7            5       2003   \n1          20         80.0     9600            6            8       1976   \n2          60         68.0    11250            7            5       2001   \n3          70         60.0     9550            7            5       1915   \n4          60         84.0    14260            8            5       2000   \n\n   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  SaleType_ConLI  \\\n0          2003       196.0       706.0         0.0  ...               0   \n1          1976         0.0       978.0         0.0  ...               0   \n2          2002       162.0       486.0         0.0  ...               0   \n3          1970         0.0       216.0         0.0  ...               0   \n4          2000       350.0       655.0         0.0  ...               0   \n\n   SaleType_ConLw  SaleType_New  SaleType_Oth  SaleType_WD  \\\n0               0             0             0            1   \n1               0             0             0            1   \n2               0             0             0            1   \n3               0             0             0            1   \n4               0             0             0            1   \n\n   SaleCondition_AdjLand  SaleCondition_Alloca  SaleCondition_Family  \\\n0                      0                     0                     0   \n1                      0                     0                     0   \n2                      0                     0                     0   \n3                      0                     0                     0   \n4                      0                     0                     0   \n\n   SaleCondition_Normal  SaleCondition_Partial  \n0                     1                      0  \n1                     1                      0  \n2                     1                      0  \n3                     0                      0  \n4                     1                      0  \n\n[5 rows x 237 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSSubClass</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>OverallQual</th>\n      <th>OverallCond</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n      <th>MasVnrArea</th>\n      <th>BsmtFinSF1</th>\n      <th>BsmtFinSF2</th>\n      <th>...</th>\n      <th>SaleType_ConLI</th>\n      <th>SaleType_ConLw</th>\n      <th>SaleType_New</th>\n      <th>SaleType_Oth</th>\n      <th>SaleType_WD</th>\n      <th>SaleCondition_AdjLand</th>\n      <th>SaleCondition_Alloca</th>\n      <th>SaleCondition_Family</th>\n      <th>SaleCondition_Normal</th>\n      <th>SaleCondition_Partial</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>60</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>7</td>\n      <td>5</td>\n      <td>2003</td>\n      <td>2003</td>\n      <td>196.0</td>\n      <td>706.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>6</td>\n      <td>8</td>\n      <td>1976</td>\n      <td>1976</td>\n      <td>0.0</td>\n      <td>978.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>60</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>7</td>\n      <td>5</td>\n      <td>2001</td>\n      <td>2002</td>\n      <td>162.0</td>\n      <td>486.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>70</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>7</td>\n      <td>5</td>\n      <td>1915</td>\n      <td>1970</td>\n      <td>0.0</td>\n      <td>216.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>60</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>8</td>\n      <td>5</td>\n      <td>2000</td>\n      <td>2000</td>\n      <td>350.0</td>\n      <td>655.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 237 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if there any columns of obj type\ndisplay(df_Train.info())\ndisplay(df_Test.info())","execution_count":37,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1460 entries, 0 to 1459\nColumns: 237 entries, MSSubClass to SaleCondition_Partial\ndtypes: float64(12), int64(25), uint8(200)\nmemory usage: 718.6 KB\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}},{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1459 entries, 0 to 1458\nColumns: 237 entries, MSSubClass to SaleCondition_Partial\ndtypes: float64(12), int64(25), uint8(200)\nmemory usage: 718.1 KB\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#final_df.select_dtypes(include=['int64', 'float64'])","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop 'SalesPrice' from df_Test, which we need to predict\ndf_Test.drop(['SalePrice'], axis=1, inplace=True)","execution_count":39,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py:4117: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  errors=errors,\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train\nX_train = df_Train.drop(['SalePrice'], axis=1)\ny_train = df_Train['SalePrice']","execution_count":40,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction using ML algorithms"},{"metadata":{},"cell_type":"markdown","source":"### XGBoost Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost\nxgbReg = xgboost.XGBRegressor()\nxgbReg.fit(X_train, y_train)","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints=None,\n             learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n             min_child_weight=1, missing=nan, monotone_constraints=None,\n             n_estimators=100, n_jobs=0, num_parallel_tree=1,\n             objective='reg:squarederror', random_state=0, reg_alpha=0,\n             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n             validate_parameters=False, verbosity=None)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prediction\ny_pred = xgbReg.predict(df_Test)","execution_count":42,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameter optimization - XGBoost Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"# XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1,\n#              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n#              importance_type='gain', interaction_constraints=None,\n#              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n#              min_child_weight=1, missing=nan, monotone_constraints=None,\n#              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n#              objective='reg:squarederror', random_state=0, reg_alpha=0,\n#              reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n#              validate_parameters=False, verbosity=None)\n\nbase_score = [0.25, 0.5, 0.75, 1.0]\nbooster =['gbtree', 'gblinear']\ngamma = [0.01, 0.001, 0.1, 0]\nlearning_rate = [0.05, 0.1, 0.15, 0.25, 0.3]\nmin_child_weight=[1, 2, 4, 6]\nn_estimators= [100, 300, 500, 900, 1200, 1500]\nn_jobs=-1\n\n# define a grid of hyperparameters to tune\nhyperparameter_grid = {\n    'base_score': base_score,\n    'booster': booster,\n    'gamma': gamma,\n    'learning_rate': learning_rate,\n    'min_child_weight': min_child_weight,\n    'n_estimators': n_estimators\n    }","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nrandom_cv = RandomizedSearchCV(estimator=xgbReg, \n                   param_distributions= hyperparameter_grid,\n                   scoring= 'neg_mean_absolute_error',\n                   cv = 5,\n                   n_jobs=-1,\n                   n_iter=50,\n                   return_train_score=True,\n                   random_state=0\n                  )","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#random_cv.fit(X_train, y_train)","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# finding the best fit\n#random_cv.best_estimator_","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using the best fit\nxgbReg = xgboost.XGBRegressor(base_score=0.75, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0.001, gpu_id=-1,\n             importance_type='gain', interaction_constraints=None,\n             learning_rate=0.05, max_delta_step=0, max_depth=6,\n             min_child_weight=4, missing=None, monotone_constraints=None,\n             n_estimators=1500, n_jobs=0, num_parallel_tree=1,\n             objective='reg:squarederror', random_state=0, reg_alpha=0,\n             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n             validate_parameters=False, verbosity=None)","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbReg.fit(X_train, y_train)","execution_count":48,"outputs":[{"output_type":"execute_result","execution_count":48,"data":{"text/plain":"XGBRegressor(base_score=0.75, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0.001, gpu_id=-1,\n             importance_type='gain', interaction_constraints=None,\n             learning_rate=0.05, max_delta_step=0, max_depth=6,\n             min_child_weight=4, missing=nan, monotone_constraints=None,\n             n_estimators=1500, n_jobs=0, num_parallel_tree=1,\n             objective='reg:squarederror', random_state=0, reg_alpha=0,\n             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n             validate_parameters=False, verbosity=None)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the mode file\nimport pickle\nfilename='finalized_XGBRegressor_model.pkl'\npickle.dump(xgbReg, open(filename, 'wb'))","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to load the saved model from the pickle file\n#Pickled_LR_Model = pickle.load(file)\npickle.load(open('finalized_XGBRegressor_model.pkl', 'rb' ))\n# or another way\nwith open('finalized_XGBRegressor_model.pkl', 'rb') as file:\n    pickled_xgbReg_model = pickle.load(file)\npickled_xgbReg_model","execution_count":50,"outputs":[{"output_type":"execute_result","execution_count":50,"data":{"text/plain":"XGBRegressor(base_score=0.75, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0.001, gpu_id=-1,\n             importance_type='gain', interaction_constraints=None,\n             learning_rate=0.05, max_delta_step=0, max_depth=6,\n             min_child_weight=4, missing=nan, monotone_constraints=None,\n             n_estimators=1500, n_jobs=0, num_parallel_tree=1,\n             objective='reg:squarederror', random_state=0, reg_alpha=0,\n             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n             validate_parameters=False, verbosity=None)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prediction\ny_pred = xgbReg.predict(df_Test)","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a sample submission\nsubmission_temp = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv')\npred_df = pd.DataFrame(y_pred)\nsubmission = pd.concat([submission_temp['Id'], pred_df], axis=1)\nsubmission.columns=['Id', 'SalePrice']\nsubmission.to_csv('submission.csv', index=False)","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find the accuracy of the training model by splitting the train into train and validation set\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\ndf = df_Train.copy()\n\ntrain, test = train_test_split(df, test_size=0.3, random_state=42)","execution_count":53,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train.loc[:, train.columns != 'Saleprice']","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost\nxgbReg = xgboost.XGBRegressor()\nxgbReg.fit(train.loc[:, train.columns != 'SalePrice'], train['SalePrice'])\nprediction = xgbReg.predict(test.loc[:, test.columns != 'SalePrice'])\n\nprint('R2 score is {}'.format(r2_score(prediction, test['SalePrice'])))\nprint('MSE is {}'.format(mean_squared_error(prediction, test['SalePrice'])))","execution_count":55,"outputs":[{"output_type":"stream","text":"R2 score is 0.8665477754289191\nMSE is 772027396.5291915\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# implement cross validation\nfrom sklearn.model_selection import cross_val_score\nX = df.loc[:, df.columns != 'SalePrice']\ny = df['SalePrice']\ncross_val_score(xgbReg, X, y, cv=5, scoring='r2')","execution_count":56,"outputs":[{"output_type":"execute_result","execution_count":56,"data":{"text/plain":"array([0.88232858, 0.82540454, 0.85436581, 0.89119106, 0.87059589])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import sklearn\n# sklearn.metrics.SCORERS","execution_count":57,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nX = train.loc[:, train.columns != 'SalePrice']\ny = train['SalePrice']\nlr.fit(X, y)\nprediction = lr.predict(test.loc[:, test.columns != 'SalePrice'])\nr2_score(prediction, test['SalePrice'])\n#mean_squared_error(prediction, test['SalePrice'])","execution_count":58,"outputs":[{"output_type":"execute_result","execution_count":58,"data":{"text/plain":"0.7364249824197948"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cross validation\nX = df.loc[:, df.columns != 'SalePrice']\ny = df['SalePrice']\ncross_val_score(lr, X, y, cv = 5, scoring='r2')","execution_count":59,"outputs":[{"output_type":"execute_result","execution_count":59,"data":{"text/plain":"array([0.11751452, 0.8186585 , 0.8099909 , 0.8874237 , 0.67036452])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Using Polynomial Features - testing accuracy in train/test splits on df_Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using polynomial features\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2)\nX_ = poly.fit_transform(X)\nX_test = test.loc[:, test.columns != 'SalePrice']\nX_test_ = poly.fit_transform(X_test)\n\n# Linear regression on transformed features\nlr.fit(X_, y)\nprediction = lr.predict(X_test_)\nr2_score(prediction, test['SalePrice'])","execution_count":60,"outputs":[{"output_type":"execute_result","execution_count":60,"data":{"text/plain":"1.0"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### we are not using the below model as it did not score well"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the model on the test set provided\n#prediction_test_data = lr.predict(df_Test)\n\n# transformation on test data\ndf_Test_ = poly.fit_transform(df_Test)\nprediction_test = lr.predict(df_Test_)\nprediction_test[:5]","execution_count":61,"outputs":[{"output_type":"execute_result","execution_count":61,"data":{"text/plain":"array([  -30681.0197329 , -1978894.47798475,   178758.7162709 ,\n         143880.39780274,   270321.14143257])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # create a sample submission\n# submission_temp = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv')\n# pred_df = pd.DataFrame(prediction_test)\n# submission = pd.concat([submission_temp['Id'], pred_df], axis=1)\n# submission.columns=['Id', 'SalePrice']\n# submission.to_csv('submission.csv', index=False)","execution_count":62,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Implementing a Neural Network implementation"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Test.values","execution_count":63,"outputs":[{"output_type":"execute_result","execution_count":63,"data":{"text/plain":"array([[2.0000e+01, 8.0000e+01, 1.1622e+04, ..., 0.0000e+00, 1.0000e+00,\n        0.0000e+00],\n       [2.0000e+01, 8.1000e+01, 1.4267e+04, ..., 0.0000e+00, 1.0000e+00,\n        0.0000e+00],\n       [6.0000e+01, 7.4000e+01, 1.3830e+04, ..., 0.0000e+00, 1.0000e+00,\n        0.0000e+00],\n       ...,\n       [2.0000e+01, 1.6000e+02, 2.0000e+04, ..., 0.0000e+00, 0.0000e+00,\n        0.0000e+00],\n       [8.5000e+01, 6.2000e+01, 1.0441e+04, ..., 0.0000e+00, 1.0000e+00,\n        0.0000e+00],\n       [6.0000e+01, 7.4000e+01, 9.6270e+03, ..., 0.0000e+00, 1.0000e+00,\n        0.0000e+00]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for keras loss function\n\nfrom keras import backend as kb\ndef keras_custom_RMSE(y_true, y_pred):\n    return kb.sqrt(kb.mean(kb.square(y_true - y_pred)))","execution_count":74,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\n# for different kinds of kernel initializers\n# refer: https://datascience.stackexchange.com/questions/37378/what-are-kernel-initializers-and-what-is-their-significance\n\nmodel = Sequential()\nmodel.add(Dense(units=100, kernel_initializer = 'he_uniform', activation='relu', input_dim=236))\n#model.add(Dense(units=100, activation='relu', input_dim=236))\nmodel.add(Dropout(rate=0.2))\nmodel.add(Dense(units=100, kernel_initializer = 'he_uniform', activation='relu'))\n#model.add(Dense(units=50, activation='relu'))\nmodel.add(Dropout(rate=0.2))\nmodel.add(Dense(units=50, kernel_initializer = 'he_uniform', activation='relu'))\n#model.add(Dense(units=25, activation='relu'))\nmodel.add(Dense(units=1))\n\n# compile the model\n#model.compile(loss='mean_squared_error', optimizer='Adamax')\n\n# to use 'root mean square eror', we need to define a custom function in keras, defined below\n#optimizer= keras.optimizers.RMSprop(0.001)\nmodel.compile(loss= keras_custom_RMSE, optimizer='Adam')\n#model.compile(loss= keras_custom_RMSE, optimizer=optimizer)\n\n\n# fit the model\n# ref: https://www.geeksforgeeks.org/keras-fit-and-keras-fit_generator/\n\nmodel.fit(X_train.values, y_train.values,\\\n          validation_split=0.2,\\\n          batch_size=16,\\\n          epochs = 1000)","execution_count":75,"outputs":[{"output_type":"stream","text":"Train on 1168 samples, validate on 292 samples\nEpoch 1/1000\n1168/1168 [==============================] - 0s 373us/step - loss: 123608.2493 - val_loss: 70098.8139\nEpoch 2/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 66207.2825 - val_loss: 59160.9602\nEpoch 3/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 56693.4690 - val_loss: 52366.8563\nEpoch 4/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 48501.5318 - val_loss: 50242.3059\nEpoch 5/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 44249.9953 - val_loss: 47593.6648\nEpoch 6/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 41300.5943 - val_loss: 46224.1128\nEpoch 7/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 42949.4904 - val_loss: 48909.9849\nEpoch 8/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 42681.7856 - val_loss: 46169.7763\nEpoch 9/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 40084.5559 - val_loss: 46097.9513\nEpoch 10/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 41091.2728 - val_loss: 46634.0837\nEpoch 11/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 42316.3995 - val_loss: 45857.2763\nEpoch 12/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 40403.7700 - val_loss: 45775.1318\nEpoch 13/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 39712.1581 - val_loss: 45309.5942\nEpoch 14/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 39710.5537 - val_loss: 46612.1431\nEpoch 15/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 40023.0357 - val_loss: 46762.3852\nEpoch 16/1000\n1168/1168 [==============================] - 0s 158us/step - loss: 39095.8867 - val_loss: 46564.4201\nEpoch 17/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 39599.9990 - val_loss: 46101.2078\nEpoch 18/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 39535.1525 - val_loss: 46296.2112\nEpoch 19/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 39248.4222 - val_loss: 46215.8561\nEpoch 20/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 38388.3727 - val_loss: 45279.9525\nEpoch 21/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 38651.1093 - val_loss: 44773.2160\nEpoch 22/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 39315.4106 - val_loss: 45219.6297\nEpoch 23/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 37880.1664 - val_loss: 45023.1748\nEpoch 24/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 38867.0604 - val_loss: 45173.7886\nEpoch 25/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 39098.6412 - val_loss: 45113.7034\nEpoch 26/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 37749.5249 - val_loss: 45412.4312\nEpoch 27/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 37272.6402 - val_loss: 44740.7961\nEpoch 28/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 37098.3773 - val_loss: 45514.6508\nEpoch 29/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 36500.8655 - val_loss: 45849.8230\nEpoch 30/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 38542.4509 - val_loss: 44820.8469\nEpoch 31/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 37276.5599 - val_loss: 44481.0291\nEpoch 32/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 36624.5770 - val_loss: 46697.4300\nEpoch 33/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 36340.7032 - val_loss: 46664.4937\nEpoch 34/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 37228.8850 - val_loss: 45435.1642\nEpoch 35/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 36898.5861 - val_loss: 44821.2519\nEpoch 36/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 36103.7684 - val_loss: 45297.3327\nEpoch 37/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 35873.7312 - val_loss: 46708.4430\nEpoch 38/1000\n1168/1168 [==============================] - 0s 158us/step - loss: 36842.1564 - val_loss: 45998.1394\nEpoch 39/1000\n1168/1168 [==============================] - 0s 156us/step - loss: 35747.3580 - val_loss: 45907.5146\nEpoch 40/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 35721.4574 - val_loss: 44331.3222\nEpoch 41/1000\n1168/1168 [==============================] - 0s 169us/step - loss: 36018.5798 - val_loss: 44805.3331\nEpoch 42/1000\n1168/1168 [==============================] - 0s 179us/step - loss: 36908.8655 - val_loss: 45835.8378\nEpoch 43/1000\n1168/1168 [==============================] - 0s 191us/step - loss: 35600.9747 - val_loss: 46453.0347\nEpoch 44/1000\n1168/1168 [==============================] - 0s 175us/step - loss: 35937.0368 - val_loss: 44563.2832\nEpoch 45/1000\n1168/1168 [==============================] - 0s 176us/step - loss: 35653.1609 - val_loss: 43460.0795\nEpoch 46/1000\n1168/1168 [==============================] - 0s 173us/step - loss: 36009.7143 - val_loss: 44052.3936\nEpoch 47/1000\n1168/1168 [==============================] - 0s 179us/step - loss: 35951.5184 - val_loss: 44495.5209\nEpoch 48/1000\n1168/1168 [==============================] - 0s 176us/step - loss: 36044.7571 - val_loss: 44769.5361\nEpoch 49/1000\n1168/1168 [==============================] - 0s 172us/step - loss: 35382.5732 - val_loss: 44206.1121\nEpoch 50/1000\n1168/1168 [==============================] - 0s 176us/step - loss: 35014.9429 - val_loss: 45061.7304\nEpoch 51/1000\n1168/1168 [==============================] - 0s 183us/step - loss: 36597.4535 - val_loss: 44189.6307\nEpoch 52/1000\n1168/1168 [==============================] - 0s 177us/step - loss: 34908.5522 - val_loss: 44557.3632\nEpoch 53/1000\n1168/1168 [==============================] - 0s 177us/step - loss: 35437.4679 - val_loss: 44751.8897\nEpoch 54/1000\n1168/1168 [==============================] - 0s 173us/step - loss: 35834.8974 - val_loss: 44178.0826\nEpoch 55/1000\n1168/1168 [==============================] - 0s 176us/step - loss: 36244.4819 - val_loss: 43751.8392\nEpoch 56/1000\n1168/1168 [==============================] - 0s 175us/step - loss: 35802.1021 - val_loss: 43430.7664\nEpoch 57/1000\n1168/1168 [==============================] - 0s 174us/step - loss: 35599.9381 - val_loss: 45412.9541\nEpoch 58/1000\n1168/1168 [==============================] - 0s 176us/step - loss: 36158.2393 - val_loss: 45732.3329\nEpoch 59/1000\n1168/1168 [==============================] - 0s 176us/step - loss: 35401.4916 - val_loss: 44271.3909\nEpoch 60/1000\n1168/1168 [==============================] - 0s 177us/step - loss: 33743.0195 - val_loss: 44268.4557\nEpoch 61/1000\n1168/1168 [==============================] - 0s 177us/step - loss: 36149.2805 - val_loss: 43102.3325\nEpoch 62/1000\n1168/1168 [==============================] - 0s 175us/step - loss: 36243.6686 - val_loss: 43422.5429\nEpoch 63/1000\n1168/1168 [==============================] - 0s 173us/step - loss: 34632.7324 - val_loss: 44819.2057\nEpoch 64/1000\n1168/1168 [==============================] - 0s 175us/step - loss: 34930.6355 - val_loss: 43400.8405\nEpoch 65/1000\n1168/1168 [==============================] - 0s 177us/step - loss: 32763.3190 - val_loss: 42561.5308\nEpoch 66/1000\n1168/1168 [==============================] - 0s 178us/step - loss: 34231.0817 - val_loss: 43418.9496\nEpoch 67/1000\n1168/1168 [==============================] - 0s 177us/step - loss: 35021.7219 - val_loss: 42049.6083\nEpoch 68/1000\n1168/1168 [==============================] - 0s 179us/step - loss: 34815.4928 - val_loss: 42180.6043\nEpoch 69/1000\n1168/1168 [==============================] - 0s 176us/step - loss: 36191.8544 - val_loss: 44081.0376\nEpoch 70/1000\n1168/1168 [==============================] - 0s 176us/step - loss: 36837.3889 - val_loss: 43453.2493\nEpoch 71/1000\n1168/1168 [==============================] - 0s 176us/step - loss: 35166.6377 - val_loss: 42077.7650\n","name":"stdout"},{"output_type":"stream","text":"Epoch 72/1000\n1168/1168 [==============================] - 0s 178us/step - loss: 34902.6396 - val_loss: 43345.5960\nEpoch 73/1000\n1168/1168 [==============================] - 0s 173us/step - loss: 35262.2791 - val_loss: 47189.2735\nEpoch 74/1000\n1168/1168 [==============================] - 0s 177us/step - loss: 35260.4881 - val_loss: 44632.2064\nEpoch 75/1000\n1168/1168 [==============================] - 0s 173us/step - loss: 34253.9790 - val_loss: 44582.6915\nEpoch 76/1000\n1168/1168 [==============================] - 0s 175us/step - loss: 34210.3607 - val_loss: 43249.8744\nEpoch 77/1000\n1168/1168 [==============================] - 0s 177us/step - loss: 33217.6512 - val_loss: 42388.9248\nEpoch 78/1000\n1168/1168 [==============================] - 0s 174us/step - loss: 34028.8537 - val_loss: 42701.8616\nEpoch 79/1000\n1168/1168 [==============================] - 0s 173us/step - loss: 35844.3916 - val_loss: 41774.0550\nEpoch 80/1000\n1168/1168 [==============================] - 0s 176us/step - loss: 35758.0086 - val_loss: 46716.8267\nEpoch 81/1000\n1168/1168 [==============================] - 0s 177us/step - loss: 34456.4218 - val_loss: 42926.4453\nEpoch 82/1000\n1168/1168 [==============================] - 0s 174us/step - loss: 34417.0452 - val_loss: 42712.9473\nEpoch 83/1000\n1168/1168 [==============================] - 0s 176us/step - loss: 33683.6533 - val_loss: 42420.6727\nEpoch 84/1000\n1168/1168 [==============================] - 0s 176us/step - loss: 33943.9256 - val_loss: 43676.6224\nEpoch 85/1000\n1168/1168 [==============================] - 0s 176us/step - loss: 34756.1144 - val_loss: 42233.8990\nEpoch 86/1000\n1168/1168 [==============================] - 0s 174us/step - loss: 32918.3754 - val_loss: 41875.4309\nEpoch 87/1000\n1168/1168 [==============================] - 0s 175us/step - loss: 33267.7922 - val_loss: 41653.9774\nEpoch 88/1000\n1168/1168 [==============================] - 0s 177us/step - loss: 35540.1654 - val_loss: 41209.8705\nEpoch 89/1000\n1168/1168 [==============================] - 0s 176us/step - loss: 34046.4463 - val_loss: 42125.6330\nEpoch 90/1000\n1168/1168 [==============================] - 0s 175us/step - loss: 33788.6844 - val_loss: 42490.9044\nEpoch 91/1000\n1168/1168 [==============================] - 0s 175us/step - loss: 33689.9209 - val_loss: 42192.8364\nEpoch 92/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 34640.3896 - val_loss: 43960.9481\nEpoch 93/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 31938.3652 - val_loss: 40390.5604\nEpoch 94/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 34202.4434 - val_loss: 41712.2113\nEpoch 95/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 33366.5596 - val_loss: 40176.9585\nEpoch 96/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 31433.1212 - val_loss: 46152.5779\nEpoch 97/1000\n1168/1168 [==============================] - 0s 177us/step - loss: 35635.0565 - val_loss: 41981.6956\nEpoch 98/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 33179.5733 - val_loss: 40309.7239\nEpoch 99/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 32273.1741 - val_loss: 41575.8436\nEpoch 100/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 32364.7684 - val_loss: 40659.5286\nEpoch 101/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 32298.2614 - val_loss: 40287.6251\nEpoch 102/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 32945.2838 - val_loss: 39923.7917\nEpoch 103/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 33277.1177 - val_loss: 39995.4472\nEpoch 104/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 32088.0012 - val_loss: 40433.9412\nEpoch 105/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 31669.1622 - val_loss: 39769.0302\nEpoch 106/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 32956.3404 - val_loss: 40956.0054\nEpoch 107/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 34799.2070 - val_loss: 42869.1888\nEpoch 108/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 32738.9742 - val_loss: 42031.8262\nEpoch 109/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 31784.1763 - val_loss: 41250.3351\nEpoch 110/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 32062.8063 - val_loss: 39779.0746\nEpoch 111/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 31988.6771 - val_loss: 38530.8664\nEpoch 112/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 31553.7331 - val_loss: 39748.9562\nEpoch 113/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 30858.7511 - val_loss: 43603.2639\nEpoch 114/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 32940.7255 - val_loss: 38892.9590\nEpoch 115/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 33105.0100 - val_loss: 46466.6987\nEpoch 116/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 33397.1485 - val_loss: 39936.7368\nEpoch 117/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 32331.1949 - val_loss: 39759.0178\nEpoch 118/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 30791.6448 - val_loss: 38503.4277\nEpoch 119/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 32821.4416 - val_loss: 38963.6244\nEpoch 120/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 31672.3202 - val_loss: 39163.8903\nEpoch 121/1000\n1168/1168 [==============================] - 0s 176us/step - loss: 31111.7642 - val_loss: 38678.5571\nEpoch 122/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 31052.5657 - val_loss: 38750.1256\nEpoch 123/1000\n1168/1168 [==============================] - 0s 182us/step - loss: 31466.7784 - val_loss: 45050.9243\nEpoch 124/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 32516.1784 - val_loss: 38374.4988\nEpoch 125/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 30578.7009 - val_loss: 39510.1629\nEpoch 126/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 29876.0891 - val_loss: 41003.3620\nEpoch 127/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 31525.7370 - val_loss: 38472.4031\nEpoch 128/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 31836.3468 - val_loss: 38825.1659\nEpoch 129/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 30397.4052 - val_loss: 38240.0823\nEpoch 130/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 29731.4409 - val_loss: 39272.1782\nEpoch 131/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 30082.9552 - val_loss: 38191.5383\nEpoch 132/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 31552.5048 - val_loss: 43495.3786\nEpoch 133/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 31162.2754 - val_loss: 37908.1412\nEpoch 134/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 30246.8542 - val_loss: 37241.3451\nEpoch 135/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 30151.9003 - val_loss: 39248.9200\nEpoch 136/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 30070.5769 - val_loss: 37072.6940\nEpoch 137/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 29936.2194 - val_loss: 38514.0319\nEpoch 138/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 30375.2908 - val_loss: 38298.4100\nEpoch 139/1000\n1168/1168 [==============================] - 0s 168us/step - loss: 29895.8310 - val_loss: 38091.0112\nEpoch 140/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 30559.7175 - val_loss: 40710.3674\nEpoch 141/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 31454.6109 - val_loss: 39173.9304\nEpoch 142/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 28645.1129 - val_loss: 38091.1732\n","name":"stdout"},{"output_type":"stream","text":"Epoch 143/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 30552.1679 - val_loss: 36840.1085\nEpoch 144/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 29887.1790 - val_loss: 36791.2096\nEpoch 145/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 30887.2496 - val_loss: 37559.3274\nEpoch 146/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 28188.1501 - val_loss: 36293.4187\nEpoch 147/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 30885.6817 - val_loss: 36704.9881\nEpoch 148/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 29686.3874 - val_loss: 39081.0461\nEpoch 149/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 29841.7169 - val_loss: 37854.3033\nEpoch 150/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 29775.0784 - val_loss: 35889.2525\nEpoch 151/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 29706.2219 - val_loss: 37396.1107\nEpoch 152/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 28845.2049 - val_loss: 36667.2958\nEpoch 153/1000\n1168/1168 [==============================] - 0s 174us/step - loss: 30328.5230 - val_loss: 36221.8124\nEpoch 154/1000\n1168/1168 [==============================] - 0s 172us/step - loss: 29343.5259 - val_loss: 36246.9407\nEpoch 155/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 29120.0413 - val_loss: 37289.9348\nEpoch 156/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 29692.7633 - val_loss: 36272.7558\nEpoch 157/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 29159.3978 - val_loss: 35597.2898\nEpoch 158/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 30910.9526 - val_loss: 38657.5201\nEpoch 159/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 30447.4515 - val_loss: 36664.3219\nEpoch 160/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 28283.4039 - val_loss: 35807.3531\nEpoch 161/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 28321.2906 - val_loss: 36312.3728\nEpoch 162/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 28567.9424 - val_loss: 36829.9567\nEpoch 163/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 30226.5388 - val_loss: 37419.6662\nEpoch 164/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 29345.6485 - val_loss: 37376.4213\nEpoch 165/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 28697.2416 - val_loss: 35597.6727\nEpoch 166/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 28280.3741 - val_loss: 38019.2408\nEpoch 167/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 28030.1384 - val_loss: 35359.6729\nEpoch 168/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 28780.4197 - val_loss: 34480.1443\nEpoch 169/1000\n1168/1168 [==============================] - 0s 168us/step - loss: 28978.5138 - val_loss: 35532.9485\nEpoch 170/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 28982.9224 - val_loss: 35476.1071\nEpoch 171/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 28569.1194 - val_loss: 35532.4340\nEpoch 172/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 28597.4672 - val_loss: 36741.2722\nEpoch 173/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 30520.1224 - val_loss: 35289.2211\nEpoch 174/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 29111.5478 - val_loss: 37990.8419\nEpoch 175/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 29569.9462 - val_loss: 37928.4346\nEpoch 176/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 28495.3803 - val_loss: 35618.0067\nEpoch 177/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 27463.0664 - val_loss: 37228.6736\nEpoch 178/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 28534.6352 - val_loss: 36814.7673\nEpoch 179/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 26727.3628 - val_loss: 34748.4888\nEpoch 180/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 30105.5023 - val_loss: 37379.7621\nEpoch 181/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 28900.7688 - val_loss: 35451.0171\nEpoch 182/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 28689.4799 - val_loss: 35151.0726\nEpoch 183/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 28738.6083 - val_loss: 34904.0027\nEpoch 184/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 28123.4939 - val_loss: 35108.6002\nEpoch 185/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 27888.8220 - val_loss: 34863.5532\nEpoch 186/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 30738.0868 - val_loss: 37069.4930\nEpoch 187/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 28282.5004 - val_loss: 34242.9383\nEpoch 188/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 27211.6499 - val_loss: 34070.2057\nEpoch 189/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 28033.8160 - val_loss: 36161.7673\nEpoch 190/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 29041.1056 - val_loss: 35321.5235\nEpoch 191/1000\n1168/1168 [==============================] - 0s 169us/step - loss: 28403.4610 - val_loss: 35023.6872\nEpoch 192/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 28174.7639 - val_loss: 36017.2312\nEpoch 193/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 30717.1906 - val_loss: 36902.9442\nEpoch 194/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 28917.0305 - val_loss: 34258.0112\nEpoch 195/1000\n1168/1168 [==============================] - 0s 216us/step - loss: 27964.7554 - val_loss: 35245.1970\nEpoch 196/1000\n1168/1168 [==============================] - 0s 183us/step - loss: 27490.5502 - val_loss: 39900.0523\nEpoch 197/1000\n1168/1168 [==============================] - 0s 183us/step - loss: 28429.2775 - val_loss: 36468.5976\nEpoch 198/1000\n1168/1168 [==============================] - 0s 174us/step - loss: 28652.6456 - val_loss: 37096.1748\nEpoch 199/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 29232.7126 - val_loss: 34998.2947\nEpoch 200/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 27070.6026 - val_loss: 34089.2331\nEpoch 201/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 27462.6733 - val_loss: 35000.3977\nEpoch 202/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 27656.1298 - val_loss: 34656.8969\nEpoch 203/1000\n1168/1168 [==============================] - 0s 172us/step - loss: 28060.5425 - val_loss: 33551.5168\nEpoch 204/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 27932.1021 - val_loss: 34863.9643\nEpoch 205/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 29157.8347 - val_loss: 34201.9876\nEpoch 206/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 27837.3109 - val_loss: 38014.8932\nEpoch 207/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 26651.6095 - val_loss: 33876.6337\nEpoch 208/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 28003.9280 - val_loss: 34781.3329\nEpoch 209/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 27576.1526 - val_loss: 34040.9487\nEpoch 210/1000\n1168/1168 [==============================] - 0s 172us/step - loss: 27434.6306 - val_loss: 34938.9007\nEpoch 211/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 28879.3847 - val_loss: 33751.5829\nEpoch 212/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 28415.5440 - val_loss: 36745.5366\nEpoch 213/1000\n","name":"stdout"},{"output_type":"stream","text":"1168/1168 [==============================] - 0s 164us/step - loss: 27546.1957 - val_loss: 33926.2212\nEpoch 214/1000\n1168/1168 [==============================] - 0s 177us/step - loss: 28849.0906 - val_loss: 32711.9738\nEpoch 215/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 28104.1652 - val_loss: 33759.7333\nEpoch 216/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 28291.9711 - val_loss: 34494.7025\nEpoch 217/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 27979.8055 - val_loss: 33655.1730\nEpoch 218/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 27408.6071 - val_loss: 39130.9443\nEpoch 219/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 26208.0754 - val_loss: 33665.6227\nEpoch 220/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 30218.2339 - val_loss: 33947.7007\nEpoch 221/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 26931.7018 - val_loss: 34365.1469\nEpoch 222/1000\n1168/1168 [==============================] - 0s 169us/step - loss: 27776.9222 - val_loss: 33896.4864\nEpoch 223/1000\n1168/1168 [==============================] - 0s 179us/step - loss: 27729.0584 - val_loss: 32880.6900\nEpoch 224/1000\n1168/1168 [==============================] - 0s 199us/step - loss: 28580.1981 - val_loss: 34776.6819\nEpoch 225/1000\n1168/1168 [==============================] - 0s 181us/step - loss: 28742.4711 - val_loss: 35149.1957\nEpoch 226/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 26380.7889 - val_loss: 33377.6703\nEpoch 227/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 29268.2482 - val_loss: 35562.7450\nEpoch 228/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 28327.3804 - val_loss: 32347.6997\nEpoch 229/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 27345.1543 - val_loss: 32888.4740\nEpoch 230/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 28884.0639 - val_loss: 33709.8650\nEpoch 231/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 27851.6085 - val_loss: 34223.3231\nEpoch 232/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 27221.7016 - val_loss: 33411.8393\nEpoch 233/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 27055.4303 - val_loss: 32723.9969\nEpoch 234/1000\n1168/1168 [==============================] - 0s 175us/step - loss: 26919.3645 - val_loss: 33609.1142\nEpoch 235/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 27248.2876 - val_loss: 34141.5570\nEpoch 236/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 27797.5784 - val_loss: 35115.6253\nEpoch 237/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 27472.0238 - val_loss: 33926.9702\nEpoch 238/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 28796.3520 - val_loss: 41074.0440\nEpoch 239/1000\n1168/1168 [==============================] - 0s 169us/step - loss: 29151.3951 - val_loss: 34312.1432\nEpoch 240/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 28863.6885 - val_loss: 35897.3842\nEpoch 241/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 27573.1790 - val_loss: 34075.5246\nEpoch 242/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 27283.5557 - val_loss: 37007.5172\nEpoch 243/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 27631.1929 - val_loss: 32910.9653\nEpoch 244/1000\n1168/1168 [==============================] - 0s 174us/step - loss: 26377.1429 - val_loss: 33707.3153\nEpoch 245/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 26064.0766 - val_loss: 32306.0010\nEpoch 246/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 28341.9417 - val_loss: 33155.8893\nEpoch 247/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 28564.8385 - val_loss: 32199.1150\nEpoch 248/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 27493.5448 - val_loss: 35841.5179\nEpoch 249/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 27028.6216 - val_loss: 33603.3641\nEpoch 250/1000\n1168/1168 [==============================] - 0s 174us/step - loss: 26814.7203 - val_loss: 33574.4572\nEpoch 251/1000\n1168/1168 [==============================] - 0s 169us/step - loss: 26913.9084 - val_loss: 38004.1740\nEpoch 252/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 27983.9384 - val_loss: 32594.6765\nEpoch 253/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 26402.9754 - val_loss: 32824.5110\nEpoch 254/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 26206.2637 - val_loss: 38307.9984\nEpoch 255/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 27331.6883 - val_loss: 32732.3045\nEpoch 256/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 25930.1422 - val_loss: 33527.4575\nEpoch 257/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 26978.6010 - val_loss: 32739.6739\nEpoch 258/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 26560.4504 - val_loss: 32352.8291\nEpoch 259/1000\n1168/1168 [==============================] - 0s 158us/step - loss: 26022.5065 - val_loss: 34467.9380\nEpoch 260/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 25611.7681 - val_loss: 32537.5500\nEpoch 261/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 27097.4928 - val_loss: 34206.9262\nEpoch 262/1000\n1168/1168 [==============================] - 0s 158us/step - loss: 26483.6755 - val_loss: 35041.4942\nEpoch 263/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 27066.3759 - val_loss: 33857.8264\nEpoch 264/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 27717.8124 - val_loss: 33702.4182\nEpoch 265/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 26227.3746 - val_loss: 32682.9823\nEpoch 266/1000\n1168/1168 [==============================] - 0s 185us/step - loss: 25217.7620 - val_loss: 31787.0172\nEpoch 267/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 27072.5221 - val_loss: 34045.3406\nEpoch 268/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 27237.2454 - val_loss: 32730.9972\nEpoch 269/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 26617.9351 - val_loss: 32048.1129\nEpoch 270/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 26751.7147 - val_loss: 34853.1643\nEpoch 271/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 27849.3309 - val_loss: 31202.6671\nEpoch 272/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 26683.3428 - val_loss: 32383.6507\nEpoch 273/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 27654.6773 - val_loss: 32714.9737\nEpoch 274/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 26803.4483 - val_loss: 33262.6172\nEpoch 275/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 27406.9927 - val_loss: 31888.8602\nEpoch 276/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 25601.5028 - val_loss: 31358.4195\nEpoch 277/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 25571.2089 - val_loss: 33665.7659\nEpoch 278/1000\n1168/1168 [==============================] - 0s 158us/step - loss: 26290.9357 - val_loss: 32641.4670\nEpoch 279/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 27303.4927 - val_loss: 35168.3868\nEpoch 280/1000\n1168/1168 [==============================] - 0s 158us/step - loss: 27424.1916 - val_loss: 31571.3522\nEpoch 281/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 27631.0510 - val_loss: 39764.1341\nEpoch 282/1000\n1168/1168 [==============================] - 0s 158us/step - loss: 29143.5123 - val_loss: 31897.5967\nEpoch 283/1000\n1168/1168 [==============================] - 0s 157us/step - loss: 26508.1174 - val_loss: 32247.3790\n","name":"stdout"},{"output_type":"stream","text":"Epoch 284/1000\n1168/1168 [==============================] - 0s 158us/step - loss: 26793.9033 - val_loss: 34890.6872\nEpoch 285/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 28750.3856 - val_loss: 32118.1480\nEpoch 286/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 26548.1834 - val_loss: 32745.6834\nEpoch 287/1000\n1168/1168 [==============================] - 0s 158us/step - loss: 26467.1413 - val_loss: 33730.8057\nEpoch 288/1000\n1168/1168 [==============================] - 0s 158us/step - loss: 28817.4122 - val_loss: 35332.8043\nEpoch 289/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 25880.7991 - val_loss: 32119.4129\nEpoch 290/1000\n1168/1168 [==============================] - 0s 158us/step - loss: 27002.3381 - val_loss: 31528.9823\nEpoch 291/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 26496.7043 - val_loss: 32356.7593\nEpoch 292/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 25518.1320 - val_loss: 34303.0484\nEpoch 293/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 26053.2664 - val_loss: 32417.7886\nEpoch 294/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 26469.9900 - val_loss: 33952.5259\nEpoch 295/1000\n1168/1168 [==============================] - 0s 158us/step - loss: 26027.8720 - val_loss: 32565.2437\nEpoch 296/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 26068.4535 - val_loss: 33308.2836\nEpoch 297/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 27504.6129 - val_loss: 31414.5727\nEpoch 298/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 25649.0094 - val_loss: 34757.1983\nEpoch 299/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 25768.9967 - val_loss: 31745.4412\nEpoch 300/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 27560.6556 - val_loss: 33212.5583\nEpoch 301/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 25914.9840 - val_loss: 34851.2825\nEpoch 302/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 24842.8687 - val_loss: 32634.5378\nEpoch 303/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 25121.2214 - val_loss: 31362.9225\nEpoch 304/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 25749.1393 - val_loss: 32541.0815\nEpoch 305/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 26682.6288 - val_loss: 37498.8775\nEpoch 306/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 25666.3300 - val_loss: 34416.0916\nEpoch 307/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 25752.3824 - val_loss: 32101.8534\nEpoch 308/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 25851.1967 - val_loss: 33727.8987\nEpoch 309/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24831.7664 - val_loss: 32355.2264\nEpoch 310/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 29328.5544 - val_loss: 41082.4280\nEpoch 311/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 26221.3083 - val_loss: 32257.2416\nEpoch 312/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 26835.3743 - val_loss: 31729.2308\nEpoch 313/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 25135.4536 - val_loss: 32301.2055\nEpoch 314/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 26983.7165 - val_loss: 34290.3392\nEpoch 315/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 26876.6300 - val_loss: 31198.6570\nEpoch 316/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 25056.9325 - val_loss: 33044.5832\nEpoch 317/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 25014.8586 - val_loss: 30487.1728\nEpoch 318/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 25494.8882 - val_loss: 37258.6660\nEpoch 319/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 26045.8162 - val_loss: 30575.9989\nEpoch 320/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 26881.9739 - val_loss: 30125.6692\nEpoch 321/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 24962.6522 - val_loss: 30271.0121\nEpoch 322/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 25472.4514 - val_loss: 32373.2133\nEpoch 323/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 25797.6709 - val_loss: 31414.3167\nEpoch 324/1000\n1168/1168 [==============================] - 0s 179us/step - loss: 26125.9049 - val_loss: 31298.1824\nEpoch 325/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 26556.3631 - val_loss: 33576.8650\nEpoch 326/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 25237.4450 - val_loss: 31057.3639\nEpoch 327/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 26324.7638 - val_loss: 37510.8610\nEpoch 328/1000\n1168/1168 [==============================] - 0s 174us/step - loss: 25481.8325 - val_loss: 30006.0091\nEpoch 329/1000\n1168/1168 [==============================] - 0s 168us/step - loss: 24247.8503 - val_loss: 30466.2510\nEpoch 330/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 24645.1570 - val_loss: 33237.7686\nEpoch 331/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 26693.8322 - val_loss: 31195.0113\nEpoch 332/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 27286.9082 - val_loss: 30469.5342\nEpoch 333/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 24594.0713 - val_loss: 35110.8422\nEpoch 334/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 24950.3575 - val_loss: 31241.9764\nEpoch 335/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 25926.1091 - val_loss: 33129.0205\nEpoch 336/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 26085.8342 - val_loss: 31477.4601\nEpoch 337/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 26031.5638 - val_loss: 30343.2488\nEpoch 338/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 26200.5790 - val_loss: 30250.1437\nEpoch 339/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 26250.4103 - val_loss: 31981.7246\nEpoch 340/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 25636.6785 - val_loss: 31313.8782\nEpoch 341/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 25734.2501 - val_loss: 31738.0438\nEpoch 342/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 25708.0787 - val_loss: 30314.9961\nEpoch 343/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 24562.7692 - val_loss: 30350.9070\nEpoch 344/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 25428.1937 - val_loss: 32990.1156\nEpoch 345/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 26153.3386 - val_loss: 30721.9923\nEpoch 346/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 26525.5331 - val_loss: 30587.9280\nEpoch 347/1000\n1168/1168 [==============================] - 0s 169us/step - loss: 26754.2076 - val_loss: 30694.9642\nEpoch 348/1000\n1168/1168 [==============================] - 0s 169us/step - loss: 25821.2030 - val_loss: 33324.7004\nEpoch 349/1000\n1168/1168 [==============================] - 0s 170us/step - loss: 25824.0551 - val_loss: 31149.2929\nEpoch 350/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 25076.7447 - val_loss: 30055.6162\nEpoch 351/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 24200.7225 - val_loss: 40035.6844\nEpoch 352/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 26428.4398 - val_loss: 31743.8900\nEpoch 353/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 25088.9726 - val_loss: 29840.3719\nEpoch 354/1000\n","name":"stdout"},{"output_type":"stream","text":"1168/1168 [==============================] - 0s 166us/step - loss: 25565.6948 - val_loss: 30021.9041\nEpoch 355/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 25656.6243 - val_loss: 33425.6330\nEpoch 356/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 24231.4012 - val_loss: 30340.2071\nEpoch 357/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 24681.7513 - val_loss: 31224.4815\nEpoch 358/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 24658.9385 - val_loss: 31917.7159\nEpoch 359/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24992.9173 - val_loss: 30757.1635\nEpoch 360/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 25047.8374 - val_loss: 31336.9531\nEpoch 361/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 26029.4021 - val_loss: 31594.2446\nEpoch 362/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24573.4024 - val_loss: 30980.9393\nEpoch 363/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 24147.8670 - val_loss: 30736.9203\nEpoch 364/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 24678.6624 - val_loss: 35571.6129\nEpoch 365/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 23927.3705 - val_loss: 30235.3843\nEpoch 366/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 25611.3041 - val_loss: 29657.9770\nEpoch 367/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 25539.1288 - val_loss: 29842.2415\nEpoch 368/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 24928.6112 - val_loss: 30980.6476\nEpoch 369/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 24575.4661 - val_loss: 31324.9538\nEpoch 370/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 25445.7425 - val_loss: 29748.2785\nEpoch 371/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24775.7551 - val_loss: 29413.5749\nEpoch 372/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 26538.9588 - val_loss: 31971.3438\nEpoch 373/1000\n1168/1168 [==============================] - 0s 168us/step - loss: 24746.8144 - val_loss: 30741.5815\nEpoch 374/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 27206.1903 - val_loss: 30190.6264\nEpoch 375/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 23774.4909 - val_loss: 29560.6637\nEpoch 376/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 23712.9769 - val_loss: 32370.4544\nEpoch 377/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 23801.4241 - val_loss: 30144.5277\nEpoch 378/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 26141.7592 - val_loss: 28676.9234\nEpoch 379/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 26636.4672 - val_loss: 33632.8608\nEpoch 380/1000\n1168/1168 [==============================] - 0s 168us/step - loss: 24839.8546 - val_loss: 34705.8105\nEpoch 381/1000\n1168/1168 [==============================] - 0s 175us/step - loss: 26421.9671 - val_loss: 29259.9068\nEpoch 382/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 25333.8320 - val_loss: 31599.9096\nEpoch 383/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24653.9034 - val_loss: 30278.7960\nEpoch 384/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 25117.1254 - val_loss: 31161.5263\nEpoch 385/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 25288.8926 - val_loss: 30509.7598\nEpoch 386/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 25564.5887 - val_loss: 29607.6392\nEpoch 387/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24299.4892 - val_loss: 29355.0398\nEpoch 388/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 24198.4157 - val_loss: 30175.9685\nEpoch 389/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 23455.2025 - val_loss: 31003.7773\nEpoch 390/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 24285.3656 - val_loss: 30478.9143\nEpoch 391/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 25193.9315 - val_loss: 31451.9811\nEpoch 392/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 25088.1096 - val_loss: 31603.1292\nEpoch 393/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 25874.7005 - val_loss: 29399.0849\nEpoch 394/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 24735.5026 - val_loss: 31154.8817\nEpoch 395/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 23660.1925 - val_loss: 29706.2922\nEpoch 396/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 24758.4830 - val_loss: 28772.1496\nEpoch 397/1000\n1168/1168 [==============================] - 0s 169us/step - loss: 24624.7039 - val_loss: 28418.8606\nEpoch 398/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24536.5979 - val_loss: 29454.6314\nEpoch 399/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 25101.7946 - val_loss: 29891.5358\nEpoch 400/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 25344.5630 - val_loss: 34565.2098\nEpoch 401/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 24999.8523 - val_loss: 34909.8901\nEpoch 402/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 23073.3988 - val_loss: 30713.9038\nEpoch 403/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 24412.1043 - val_loss: 29621.8083\nEpoch 404/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 24313.6913 - val_loss: 29411.4116\nEpoch 405/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 24223.8733 - val_loss: 28341.7481\nEpoch 406/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24933.4085 - val_loss: 28832.5623\nEpoch 407/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 25601.5083 - val_loss: 30258.0160\nEpoch 408/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 23683.1963 - val_loss: 28467.2476\nEpoch 409/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 23840.6402 - val_loss: 29267.6541\nEpoch 410/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 24008.7028 - val_loss: 30484.7126\nEpoch 411/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 24277.1163 - val_loss: 29412.5561\nEpoch 412/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 23944.0414 - val_loss: 28909.8390\nEpoch 413/1000\n1168/1168 [==============================] - 0s 168us/step - loss: 24226.3019 - val_loss: 30680.6371\nEpoch 414/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 25043.5919 - val_loss: 30500.5181\nEpoch 415/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24515.7341 - val_loss: 29177.2950\nEpoch 416/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24748.8047 - val_loss: 28866.3332\nEpoch 417/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 24100.5914 - val_loss: 30053.2961\nEpoch 418/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 26029.6781 - val_loss: 29118.6618\nEpoch 419/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24362.9343 - val_loss: 30303.7957\nEpoch 420/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 24600.0185 - val_loss: 29485.4996\nEpoch 421/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 25499.7521 - val_loss: 29144.4636\nEpoch 422/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24750.5304 - val_loss: 28222.9400\nEpoch 423/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 25543.9531 - val_loss: 28045.7344\nEpoch 424/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 25130.7162 - val_loss: 29262.4353\n","name":"stdout"},{"output_type":"stream","text":"Epoch 425/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24085.9058 - val_loss: 29304.7776\nEpoch 426/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 25180.8224 - val_loss: 32424.1285\nEpoch 427/1000\n1168/1168 [==============================] - 0s 169us/step - loss: 25410.6638 - val_loss: 29242.7705\nEpoch 428/1000\n1168/1168 [==============================] - 0s 172us/step - loss: 23740.0224 - val_loss: 29435.1274\nEpoch 429/1000\n1168/1168 [==============================] - 0s 180us/step - loss: 23625.0413 - val_loss: 29085.2892\nEpoch 430/1000\n1168/1168 [==============================] - 0s 217us/step - loss: 25731.5775 - val_loss: 28357.1992\nEpoch 431/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 23568.7230 - val_loss: 30104.5373\nEpoch 432/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 23256.3567 - val_loss: 29946.5566\nEpoch 433/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 23435.6678 - val_loss: 29754.2164\nEpoch 434/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 24874.0579 - val_loss: 29966.8419\nEpoch 435/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 25690.2137 - val_loss: 28884.3232\nEpoch 436/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 24604.3669 - val_loss: 29057.8348\nEpoch 437/1000\n1168/1168 [==============================] - 0s 178us/step - loss: 25208.5244 - val_loss: 27975.8231\nEpoch 438/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 25381.7267 - val_loss: 29590.3055\nEpoch 439/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 25829.0775 - val_loss: 28516.7269\nEpoch 440/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24015.8366 - val_loss: 28991.3582\nEpoch 441/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 23449.4497 - val_loss: 28705.9571\nEpoch 442/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 23789.1822 - val_loss: 29833.3578\nEpoch 443/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24961.6158 - val_loss: 27971.2625\nEpoch 444/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24472.7174 - val_loss: 34978.0309\nEpoch 445/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 24210.0362 - val_loss: 28391.3696\nEpoch 446/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 25619.2700 - val_loss: 35916.5627\nEpoch 447/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 23827.8162 - val_loss: 27777.3490\nEpoch 448/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 24339.1278 - val_loss: 28236.3842\nEpoch 449/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24505.7469 - val_loss: 29228.8699\nEpoch 450/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 23231.2988 - val_loss: 32397.0285\nEpoch 451/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 25344.5088 - val_loss: 29844.6587\nEpoch 452/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24225.7436 - val_loss: 28018.7389\nEpoch 453/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 24419.6514 - val_loss: 29086.1663\nEpoch 454/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 24929.8794 - val_loss: 29023.2162\nEpoch 455/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 23987.1348 - val_loss: 29194.8486\nEpoch 456/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 24401.0559 - val_loss: 30578.5270\nEpoch 457/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 24266.5830 - val_loss: 27640.7768\nEpoch 458/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 23151.7931 - val_loss: 28714.5609\nEpoch 459/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 23116.7099 - val_loss: 32348.3842\nEpoch 460/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 24581.6071 - val_loss: 28605.7290\nEpoch 461/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 24625.4113 - val_loss: 29986.1052\nEpoch 462/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 23729.1271 - val_loss: 30796.7771\nEpoch 463/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 23135.0022 - val_loss: 30011.4534\nEpoch 464/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 23576.2491 - val_loss: 29681.5677\nEpoch 465/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 23808.8387 - val_loss: 27958.8350\nEpoch 466/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 24336.2820 - val_loss: 28827.6508\nEpoch 467/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 23358.0224 - val_loss: 28773.2808\nEpoch 468/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 23188.1276 - val_loss: 27276.4711\nEpoch 469/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 23872.9503 - val_loss: 30090.4889\nEpoch 470/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 23334.4779 - val_loss: 29622.8097\nEpoch 471/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 22487.3115 - val_loss: 28714.7561\nEpoch 472/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 24177.7663 - val_loss: 30629.2105\nEpoch 473/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 23777.5289 - val_loss: 29150.6041\nEpoch 474/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 23363.1034 - val_loss: 27359.1466\nEpoch 475/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24130.2156 - val_loss: 28240.5130\nEpoch 476/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 23874.3460 - val_loss: 27826.4386\nEpoch 477/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 22413.1523 - val_loss: 28881.3072\nEpoch 478/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 23394.8927 - val_loss: 29015.6307\nEpoch 479/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 23555.0865 - val_loss: 27724.4879\nEpoch 480/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 23818.7501 - val_loss: 32307.3750\nEpoch 481/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 24883.9349 - val_loss: 33061.5811\nEpoch 482/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 24154.6427 - val_loss: 31581.7496\nEpoch 483/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 23755.2331 - val_loss: 31197.2297\nEpoch 484/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 23742.9550 - val_loss: 29058.1956\nEpoch 485/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 23515.3338 - val_loss: 28662.7141\nEpoch 486/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 24130.2211 - val_loss: 29509.6402\nEpoch 487/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 23651.4782 - val_loss: 29253.9637\nEpoch 488/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 23738.3084 - val_loss: 29138.1301\nEpoch 489/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 24323.5480 - val_loss: 29449.6965\nEpoch 490/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 24322.9434 - val_loss: 28837.0058\nEpoch 491/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24383.0532 - val_loss: 33458.1069\nEpoch 492/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24366.7344 - val_loss: 31792.0231\nEpoch 493/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 24375.3775 - val_loss: 28909.5788\nEpoch 494/1000\n1168/1168 [==============================] - 0s 176us/step - loss: 23532.2371 - val_loss: 29005.1187\nEpoch 495/1000\n","name":"stdout"},{"output_type":"stream","text":"1168/1168 [==============================] - 0s 167us/step - loss: 22800.8863 - val_loss: 31271.8040\nEpoch 496/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 23330.4871 - val_loss: 29994.5468\nEpoch 497/1000\n1168/1168 [==============================] - 0s 168us/step - loss: 23267.2170 - val_loss: 30651.0756\nEpoch 498/1000\n1168/1168 [==============================] - 0s 169us/step - loss: 26447.3989 - val_loss: 29986.6469\nEpoch 499/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 23847.7500 - val_loss: 29089.9052\nEpoch 500/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 24352.1519 - val_loss: 30789.7694\nEpoch 501/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 23077.6229 - val_loss: 30187.1983\nEpoch 502/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 24994.8560 - val_loss: 28517.5304\nEpoch 503/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22464.6563 - val_loss: 27415.5811\nEpoch 504/1000\n1168/1168 [==============================] - 0s 199us/step - loss: 24019.5031 - val_loss: 29289.8873\nEpoch 505/1000\n1168/1168 [==============================] - 0s 192us/step - loss: 23711.1553 - val_loss: 27408.5914\nEpoch 506/1000\n1168/1168 [==============================] - 0s 181us/step - loss: 24344.4309 - val_loss: 29388.6366\nEpoch 507/1000\n1168/1168 [==============================] - 0s 171us/step - loss: 23914.7886 - val_loss: 27649.5556\nEpoch 508/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 23110.3220 - val_loss: 28851.9782\nEpoch 509/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 22623.0654 - val_loss: 29770.0274\nEpoch 510/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 24973.3415 - val_loss: 29099.5501\nEpoch 511/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24114.0051 - val_loss: 31012.3749\nEpoch 512/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 23566.5030 - val_loss: 29545.7743\nEpoch 513/1000\n1168/1168 [==============================] - 0s 179us/step - loss: 24611.8592 - val_loss: 29532.1985\nEpoch 514/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22579.5279 - val_loss: 28889.8921\nEpoch 515/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24423.5408 - val_loss: 28204.9924\nEpoch 516/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22345.3040 - val_loss: 28650.5460\nEpoch 517/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 25400.4803 - val_loss: 31298.3591\nEpoch 518/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 23168.9794 - val_loss: 29025.2115\nEpoch 519/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22823.8295 - val_loss: 29088.9728\nEpoch 520/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22108.3090 - val_loss: 28165.9606\nEpoch 521/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 23724.7533 - val_loss: 36128.9048\nEpoch 522/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 24732.9122 - val_loss: 32462.4897\nEpoch 523/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 24077.1807 - val_loss: 28339.6299\nEpoch 524/1000\n1168/1168 [==============================] - 0s 181us/step - loss: 24430.8519 - val_loss: 27713.3050\nEpoch 525/1000\n1168/1168 [==============================] - 0s 172us/step - loss: 23205.1674 - val_loss: 28823.5327\nEpoch 526/1000\n1168/1168 [==============================] - 0s 177us/step - loss: 23827.8568 - val_loss: 29026.8119\nEpoch 527/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 24384.6393 - val_loss: 28708.6495\nEpoch 528/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22549.2373 - val_loss: 27600.5478\nEpoch 529/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22955.3766 - val_loss: 28295.3935\nEpoch 530/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 23718.7510 - val_loss: 32137.4055\nEpoch 531/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 22484.2621 - val_loss: 28733.5995\nEpoch 532/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 23485.7248 - val_loss: 27681.8076\nEpoch 533/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22583.6118 - val_loss: 28532.3746\nEpoch 534/1000\n1168/1168 [==============================] - 0s 168us/step - loss: 23224.5576 - val_loss: 29278.6149\nEpoch 535/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 22334.8660 - val_loss: 29160.8582\nEpoch 536/1000\n1168/1168 [==============================] - 0s 158us/step - loss: 24181.9385 - val_loss: 29488.7434\nEpoch 537/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 23108.0637 - val_loss: 29221.2695\nEpoch 538/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22746.3604 - val_loss: 28090.8546\nEpoch 539/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 23703.9180 - val_loss: 27093.4969\nEpoch 540/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 23537.4628 - val_loss: 28211.6792\nEpoch 541/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 24117.9731 - val_loss: 29322.5040\nEpoch 542/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 25218.4447 - val_loss: 26982.3829\nEpoch 543/1000\n1168/1168 [==============================] - 0s 158us/step - loss: 22893.8655 - val_loss: 29227.9873\nEpoch 544/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 22966.7724 - val_loss: 29203.8230\nEpoch 545/1000\n1168/1168 [==============================] - 0s 169us/step - loss: 22932.6886 - val_loss: 27977.5423\nEpoch 546/1000\n1168/1168 [==============================] - 0s 158us/step - loss: 23247.4261 - val_loss: 28352.1836\nEpoch 547/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 22979.4520 - val_loss: 28343.1532\nEpoch 548/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 22595.0063 - val_loss: 27311.9952\nEpoch 549/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 23972.5008 - val_loss: 29332.8847\nEpoch 550/1000\n1168/1168 [==============================] - 0s 176us/step - loss: 23728.3435 - val_loss: 27779.9374\nEpoch 551/1000\n1168/1168 [==============================] - 0s 181us/step - loss: 23242.0915 - val_loss: 28745.3035\nEpoch 552/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 21858.7800 - val_loss: 29571.5343\nEpoch 553/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22897.6672 - val_loss: 29047.3327\nEpoch 554/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 22363.9848 - val_loss: 28892.9262\nEpoch 555/1000\n1168/1168 [==============================] - 0s 177us/step - loss: 23206.9046 - val_loss: 31554.6821\nEpoch 556/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22930.1392 - val_loss: 30508.1623\nEpoch 557/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 24041.2932 - val_loss: 29266.9940\nEpoch 558/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 22891.5215 - val_loss: 28657.1399\nEpoch 559/1000\n1168/1168 [==============================] - 0s 171us/step - loss: 22890.8410 - val_loss: 28901.5817\nEpoch 560/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 22159.2871 - val_loss: 28278.1868\nEpoch 561/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 22987.2946 - val_loss: 26517.2478\nEpoch 562/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 25037.6314 - val_loss: 28057.8699\nEpoch 563/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 23649.0990 - val_loss: 29901.1773\nEpoch 564/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 22851.9008 - val_loss: 27906.3380\nEpoch 565/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 22049.6622 - val_loss: 28790.1793\n","name":"stdout"},{"output_type":"stream","text":"Epoch 566/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 23944.8915 - val_loss: 27057.3910\nEpoch 567/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 23299.7947 - val_loss: 28086.3202\nEpoch 568/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 23085.0926 - val_loss: 27972.8147\nEpoch 569/1000\n1168/1168 [==============================] - 0s 158us/step - loss: 22749.0178 - val_loss: 28306.9807\nEpoch 570/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 23156.6533 - val_loss: 37445.3566\nEpoch 571/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 23969.6967 - val_loss: 27321.9942\nEpoch 572/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 23377.7483 - val_loss: 29314.2651\nEpoch 573/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24754.7118 - val_loss: 30468.1256\nEpoch 574/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22423.2007 - val_loss: 34923.8116\nEpoch 575/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 24764.0149 - val_loss: 32599.6065\nEpoch 576/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 23718.2767 - val_loss: 28018.5551\nEpoch 577/1000\n1168/1168 [==============================] - 0s 171us/step - loss: 22451.4632 - val_loss: 29752.0482\nEpoch 578/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22737.8514 - val_loss: 28814.1671\nEpoch 579/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 22412.9690 - val_loss: 28141.0928\nEpoch 580/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 22468.3448 - val_loss: 30328.8697\nEpoch 581/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 22004.1108 - val_loss: 33183.3626\nEpoch 582/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22636.7150 - val_loss: 28801.5138\nEpoch 583/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 23472.4122 - val_loss: 27812.5867\nEpoch 584/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 24744.6075 - val_loss: 29223.9940\nEpoch 585/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 23058.2423 - val_loss: 28101.8652\nEpoch 586/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22885.9987 - val_loss: 30854.6019\nEpoch 587/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 22718.5698 - val_loss: 28865.6300\nEpoch 588/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 21594.3829 - val_loss: 30782.0274\nEpoch 589/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22708.2439 - val_loss: 27868.6373\nEpoch 590/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22940.2993 - val_loss: 29347.9402\nEpoch 591/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22015.5589 - val_loss: 29975.9853\nEpoch 592/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 24055.9449 - val_loss: 30277.5510\nEpoch 593/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22535.8674 - val_loss: 37483.4242\nEpoch 594/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 23939.6697 - val_loss: 31275.4887\nEpoch 595/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 23448.0400 - val_loss: 29108.3396\nEpoch 596/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22822.5877 - val_loss: 32816.5708\nEpoch 597/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22606.5973 - val_loss: 28866.7141\nEpoch 598/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 22705.6415 - val_loss: 29496.6918\nEpoch 599/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 22366.6856 - val_loss: 27811.2915\nEpoch 600/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22602.5033 - val_loss: 28278.2252\nEpoch 601/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22389.3844 - val_loss: 29432.4865\nEpoch 602/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 22465.4441 - val_loss: 29789.2887\nEpoch 603/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 22803.8555 - val_loss: 28008.9901\nEpoch 604/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 23008.2558 - val_loss: 28184.9379\nEpoch 605/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22285.6275 - val_loss: 26804.6016\nEpoch 606/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 23688.0069 - val_loss: 29174.1526\nEpoch 607/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22478.8150 - val_loss: 27895.4085\nEpoch 608/1000\n1168/1168 [==============================] - 0s 177us/step - loss: 23579.2352 - val_loss: 28669.0870\nEpoch 609/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 23183.7063 - val_loss: 27947.0475\nEpoch 610/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 21323.1557 - val_loss: 27909.9144\nEpoch 611/1000\n1168/1168 [==============================] - 0s 171us/step - loss: 21571.4655 - val_loss: 28778.1365\nEpoch 612/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 24095.5487 - val_loss: 28500.7725\nEpoch 613/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 21556.3216 - val_loss: 27265.6523\nEpoch 614/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 22910.1169 - val_loss: 29299.9312\nEpoch 615/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 23388.7776 - val_loss: 29504.1087\nEpoch 616/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 23878.3830 - val_loss: 27229.9967\nEpoch 617/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22641.2769 - val_loss: 28208.8987\nEpoch 618/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22017.1989 - val_loss: 29115.2293\nEpoch 619/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22322.6139 - val_loss: 27993.5919\nEpoch 620/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22080.9279 - val_loss: 31834.4439\nEpoch 621/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22417.1608 - val_loss: 27061.1392\nEpoch 622/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 23286.9533 - val_loss: 28378.2186\nEpoch 623/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 23019.7756 - val_loss: 28283.0059\nEpoch 624/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 22295.8522 - val_loss: 32659.6529\nEpoch 625/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 21503.7279 - val_loss: 28452.9617\nEpoch 626/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22763.3432 - val_loss: 30343.5045\nEpoch 627/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21779.7165 - val_loss: 29711.5619\nEpoch 628/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22415.7893 - val_loss: 29307.2467\nEpoch 629/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21691.7053 - val_loss: 28940.3969\nEpoch 630/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22539.4941 - val_loss: 29377.8000\nEpoch 631/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 20744.1487 - val_loss: 28079.7023\nEpoch 632/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 23037.0463 - val_loss: 28528.1114\nEpoch 633/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22724.8820 - val_loss: 27481.5844\nEpoch 634/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 23085.8418 - val_loss: 27901.0857\nEpoch 635/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22220.1062 - val_loss: 28949.6394\nEpoch 636/1000\n","name":"stdout"},{"output_type":"stream","text":"1168/1168 [==============================] - 0s 166us/step - loss: 23492.2009 - val_loss: 27768.9455\nEpoch 637/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22215.4032 - val_loss: 28579.4879\nEpoch 638/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 21903.4788 - val_loss: 28760.9582\nEpoch 639/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22972.3471 - val_loss: 28094.7128\nEpoch 640/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21658.8607 - val_loss: 28286.5448\nEpoch 641/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 22209.9854 - val_loss: 29138.8063\nEpoch 642/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 22135.7852 - val_loss: 29616.4720\nEpoch 643/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 23145.5011 - val_loss: 28516.9458\nEpoch 644/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22906.8457 - val_loss: 27860.4480\nEpoch 645/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22914.2125 - val_loss: 27525.0684\nEpoch 646/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 21408.7151 - val_loss: 28853.1214\nEpoch 647/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22180.7492 - val_loss: 30252.0914\nEpoch 648/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22714.7683 - val_loss: 29213.1862\nEpoch 649/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 22560.9566 - val_loss: 29343.1111\nEpoch 650/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 21985.4187 - val_loss: 27419.8827\nEpoch 651/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 22520.6801 - val_loss: 29244.2554\nEpoch 652/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22566.3806 - val_loss: 28092.1038\nEpoch 653/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 21950.4325 - val_loss: 27971.5041\nEpoch 654/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 21707.0465 - val_loss: 27465.5230\nEpoch 655/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 23319.0136 - val_loss: 28240.8039\nEpoch 656/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22079.7850 - val_loss: 28468.5643\nEpoch 657/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 21945.6557 - val_loss: 30489.9202\nEpoch 658/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 22316.6179 - val_loss: 27375.2457\nEpoch 659/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 22129.9607 - val_loss: 27895.6948\nEpoch 660/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22787.7681 - val_loss: 28475.1304\nEpoch 661/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22386.7821 - val_loss: 28283.7079\nEpoch 662/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 24019.5981 - val_loss: 27821.2411\nEpoch 663/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 21892.0805 - val_loss: 28997.8380\nEpoch 664/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 22357.1235 - val_loss: 28217.7490\nEpoch 665/1000\n1168/1168 [==============================] - 0s 177us/step - loss: 20825.5654 - val_loss: 27801.1976\nEpoch 666/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22163.5929 - val_loss: 31086.9907\nEpoch 667/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 22849.8011 - val_loss: 35861.2177\nEpoch 668/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 23175.6573 - val_loss: 31348.3364\nEpoch 669/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 23245.2689 - val_loss: 28407.9699\nEpoch 670/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21871.0158 - val_loss: 30320.0902\nEpoch 671/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22466.9851 - val_loss: 28571.7157\nEpoch 672/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22431.3490 - val_loss: 28969.5386\nEpoch 673/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21654.9652 - val_loss: 29235.3249\nEpoch 674/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22980.0058 - val_loss: 28569.5736\nEpoch 675/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 23586.3395 - val_loss: 31802.9359\nEpoch 676/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 22498.3452 - val_loss: 28551.6419\nEpoch 677/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 21526.7903 - val_loss: 27938.2327\nEpoch 678/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 22358.2375 - val_loss: 29437.7926\nEpoch 679/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22211.1333 - val_loss: 33171.1331\nEpoch 680/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 22179.3637 - val_loss: 32136.1336\nEpoch 681/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 21658.6940 - val_loss: 30580.8258\nEpoch 682/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22185.2575 - val_loss: 27567.3351\nEpoch 683/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 21081.1571 - val_loss: 28809.3556\nEpoch 684/1000\n1168/1168 [==============================] - 0s 170us/step - loss: 22353.1941 - val_loss: 29289.2176\nEpoch 685/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 23007.9193 - val_loss: 28395.1272\nEpoch 686/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22928.7110 - val_loss: 30066.0792\nEpoch 687/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21948.8349 - val_loss: 29494.6572\nEpoch 688/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22007.6956 - val_loss: 28300.1783\nEpoch 689/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21994.0606 - val_loss: 27600.5544\nEpoch 690/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21327.7336 - val_loss: 27460.8891\nEpoch 691/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21106.9312 - val_loss: 31268.9374\nEpoch 692/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 23307.0976 - val_loss: 29648.7078\nEpoch 693/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22227.1893 - val_loss: 31360.5808\nEpoch 694/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22194.8068 - val_loss: 27858.2724\nEpoch 695/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22091.3419 - val_loss: 32102.8549\nEpoch 696/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22236.4865 - val_loss: 27037.0681\nEpoch 697/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22957.3932 - val_loss: 34262.1119\nEpoch 698/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21825.6924 - val_loss: 27263.7076\nEpoch 699/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22863.5650 - val_loss: 27078.3290\nEpoch 700/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21521.3987 - val_loss: 26679.9944\nEpoch 701/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 23133.7377 - val_loss: 27176.3400\nEpoch 702/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 24419.9363 - val_loss: 28247.8182\nEpoch 703/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 23024.2494 - val_loss: 33441.7310\nEpoch 704/1000\n1168/1168 [==============================] - 0s 168us/step - loss: 21732.8748 - val_loss: 28238.5203\nEpoch 705/1000\n1168/1168 [==============================] - 0s 168us/step - loss: 22222.8340 - val_loss: 29707.8949\nEpoch 706/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22290.4178 - val_loss: 26854.7828\n","name":"stdout"},{"output_type":"stream","text":"Epoch 707/1000\n1168/1168 [==============================] - 0s 172us/step - loss: 22347.2272 - val_loss: 29306.5927\nEpoch 708/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21851.0268 - val_loss: 28101.0200\nEpoch 709/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 21630.9247 - val_loss: 28884.4833\nEpoch 710/1000\n1168/1168 [==============================] - 0s 169us/step - loss: 23748.1586 - val_loss: 28412.2521\nEpoch 711/1000\n1168/1168 [==============================] - 0s 171us/step - loss: 21844.1120 - val_loss: 28511.1180\nEpoch 712/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 24477.0286 - val_loss: 28615.0383\nEpoch 713/1000\n1168/1168 [==============================] - 0s 171us/step - loss: 22541.4651 - val_loss: 29558.6249\nEpoch 714/1000\n1168/1168 [==============================] - 0s 169us/step - loss: 22804.1551 - val_loss: 27474.6565\nEpoch 715/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 21892.3072 - val_loss: 27341.2706\nEpoch 716/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 22911.5540 - val_loss: 27628.3787\nEpoch 717/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 23543.2667 - val_loss: 28214.6487\nEpoch 718/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 21844.4609 - val_loss: 28687.1247\nEpoch 719/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 22352.7568 - val_loss: 33148.7715\nEpoch 720/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 21531.4422 - val_loss: 28010.7057\nEpoch 721/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 20445.4779 - val_loss: 26859.5011\nEpoch 722/1000\n1168/1168 [==============================] - 0s 180us/step - loss: 22178.5584 - val_loss: 34461.3138\nEpoch 723/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 22941.6073 - val_loss: 27727.1968\nEpoch 724/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 21824.8742 - val_loss: 29059.4903\nEpoch 725/1000\n1168/1168 [==============================] - 0s 176us/step - loss: 22908.8745 - val_loss: 29140.1806\nEpoch 726/1000\n1168/1168 [==============================] - 0s 175us/step - loss: 22293.3604 - val_loss: 30383.4693\nEpoch 727/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22622.0379 - val_loss: 29229.7103\nEpoch 728/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22069.3055 - val_loss: 28475.0624\nEpoch 729/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22360.3912 - val_loss: 27239.2960\nEpoch 730/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 22615.0915 - val_loss: 31857.5316\nEpoch 731/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 21092.2422 - val_loss: 28754.2792\nEpoch 732/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22127.7344 - val_loss: 28140.9174\nEpoch 733/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21037.9887 - val_loss: 30625.0340\nEpoch 734/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 23449.9722 - val_loss: 30735.2243\nEpoch 735/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 22235.6200 - val_loss: 28136.6381\nEpoch 736/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22904.1344 - val_loss: 29710.4453\nEpoch 737/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22254.4105 - val_loss: 29205.4085\nEpoch 738/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21049.4316 - val_loss: 28891.2858\nEpoch 739/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 20644.8758 - val_loss: 29004.1462\nEpoch 740/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 20928.2451 - val_loss: 28404.0605\nEpoch 741/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21843.4714 - val_loss: 34349.6467\nEpoch 742/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22020.8313 - val_loss: 30060.6530\nEpoch 743/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21737.1735 - val_loss: 28241.2766\nEpoch 744/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 23010.5441 - val_loss: 30893.5236\nEpoch 745/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21216.6658 - val_loss: 31748.6976\nEpoch 746/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22929.1945 - val_loss: 35802.7219\nEpoch 747/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 21982.4134 - val_loss: 29893.6216\nEpoch 748/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 20851.7475 - val_loss: 35311.3473\nEpoch 749/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 21339.7375 - val_loss: 27458.3690\nEpoch 750/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21394.1765 - val_loss: 29827.6840\nEpoch 751/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 22255.4423 - val_loss: 32480.1780\nEpoch 752/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21857.4751 - val_loss: 29544.8398\nEpoch 753/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 23130.7681 - val_loss: 27716.4484\nEpoch 754/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22121.6086 - val_loss: 27659.7492\nEpoch 755/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 21448.8510 - val_loss: 27448.6884\nEpoch 756/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 23456.3332 - val_loss: 33811.8360\nEpoch 757/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 22139.1944 - val_loss: 28531.1007\nEpoch 758/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 22559.8836 - val_loss: 26884.5012\nEpoch 759/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 21585.1463 - val_loss: 31982.3947\nEpoch 760/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 23354.7346 - val_loss: 28252.8610\nEpoch 761/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 21821.2155 - val_loss: 28101.0172\nEpoch 762/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22981.2537 - val_loss: 29722.9486\nEpoch 763/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 21908.1019 - val_loss: 29003.4472\nEpoch 764/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21214.9396 - val_loss: 28599.5111\nEpoch 765/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 20884.5055 - val_loss: 28469.9819\nEpoch 766/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 22265.6632 - val_loss: 31226.1859\nEpoch 767/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 21757.4374 - val_loss: 28237.3725\nEpoch 768/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 21009.8766 - val_loss: 30329.4566\nEpoch 769/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 23088.0169 - val_loss: 28895.0304\nEpoch 770/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 20693.6803 - val_loss: 28943.3357\nEpoch 771/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 22320.7844 - val_loss: 31553.5345\nEpoch 772/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 22039.7244 - val_loss: 28983.8402\nEpoch 773/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 21125.1718 - val_loss: 33313.7576\nEpoch 774/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 21944.7596 - val_loss: 29646.5616\nEpoch 775/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 22476.2996 - val_loss: 29078.6229\nEpoch 776/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 21656.5420 - val_loss: 34870.6416\nEpoch 777/1000\n","name":"stdout"},{"output_type":"stream","text":"1168/1168 [==============================] - 0s 168us/step - loss: 21661.4170 - val_loss: 29184.5241\nEpoch 778/1000\n1168/1168 [==============================] - 0s 168us/step - loss: 21826.6666 - val_loss: 28992.5694\nEpoch 779/1000\n1168/1168 [==============================] - 0s 177us/step - loss: 21923.8781 - val_loss: 28011.2793\nEpoch 780/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 20722.5920 - val_loss: 31713.8045\nEpoch 781/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22007.9082 - val_loss: 30870.2618\nEpoch 782/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 22107.5070 - val_loss: 27770.3595\nEpoch 783/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21953.1205 - val_loss: 32104.9615\nEpoch 784/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21632.1359 - val_loss: 32769.9938\nEpoch 785/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 23532.8100 - val_loss: 27778.2808\nEpoch 786/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21780.9711 - val_loss: 28110.0022\nEpoch 787/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22099.3093 - val_loss: 32792.2457\nEpoch 788/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 21500.2531 - val_loss: 35977.8035\nEpoch 789/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 21847.7683 - val_loss: 31638.8668\nEpoch 790/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 21519.0699 - val_loss: 29548.8638\nEpoch 791/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22232.2252 - val_loss: 31335.4057\nEpoch 792/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22186.2401 - val_loss: 27775.0452\nEpoch 793/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22425.1123 - val_loss: 28417.9675\nEpoch 794/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 21936.4419 - val_loss: 30629.7699\nEpoch 795/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 22459.4513 - val_loss: 29348.4898\nEpoch 796/1000\n1168/1168 [==============================] - 0s 158us/step - loss: 21946.9255 - val_loss: 29735.3220\nEpoch 797/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 21700.6623 - val_loss: 27209.2563\nEpoch 798/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 24457.5931 - val_loss: 39659.1329\nEpoch 799/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 21610.1175 - val_loss: 27222.9451\nEpoch 800/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 20029.2925 - val_loss: 29239.1501\nEpoch 801/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 21492.6488 - val_loss: 29092.2766\nEpoch 802/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 21136.0733 - val_loss: 29940.7403\nEpoch 803/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 20514.2364 - val_loss: 28509.9967\nEpoch 804/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 21513.2777 - val_loss: 28564.6538\nEpoch 805/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 20519.0955 - val_loss: 28373.3470\nEpoch 806/1000\n1168/1168 [==============================] - 0s 158us/step - loss: 20813.8433 - val_loss: 29153.4327\nEpoch 807/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 21527.5369 - val_loss: 27557.6658\nEpoch 808/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 21860.3001 - val_loss: 29445.5307\nEpoch 809/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 21195.8190 - val_loss: 32538.4400\nEpoch 810/1000\n1168/1168 [==============================] - 0s 157us/step - loss: 22938.9331 - val_loss: 30010.4716\nEpoch 811/1000\n1168/1168 [==============================] - 0s 168us/step - loss: 21275.7678 - val_loss: 27059.6260\nEpoch 812/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 20960.6770 - val_loss: 29634.2510\nEpoch 813/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 20143.4657 - val_loss: 29708.7426\nEpoch 814/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 23633.4480 - val_loss: 32155.7952\nEpoch 815/1000\n1168/1168 [==============================] - 0s 200us/step - loss: 21694.4957 - val_loss: 32265.9300\nEpoch 816/1000\n1168/1168 [==============================] - 0s 177us/step - loss: 21612.2245 - val_loss: 26908.2976\nEpoch 817/1000\n1168/1168 [==============================] - 0s 173us/step - loss: 21540.4243 - val_loss: 34847.3023\nEpoch 818/1000\n1168/1168 [==============================] - 0s 168us/step - loss: 20714.1705 - val_loss: 32358.9843\nEpoch 819/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 21325.7609 - val_loss: 28511.1512\nEpoch 820/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 19701.0710 - val_loss: 27793.5959\nEpoch 821/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 21607.0769 - val_loss: 27921.9802\nEpoch 822/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 21286.3603 - val_loss: 29218.3599\nEpoch 823/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 20769.7242 - val_loss: 31233.3098\nEpoch 824/1000\n1168/1168 [==============================] - 0s 172us/step - loss: 20937.8976 - val_loss: 30744.9335\nEpoch 825/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 21026.0813 - val_loss: 29135.0180\nEpoch 826/1000\n1168/1168 [==============================] - 0s 159us/step - loss: 21271.5340 - val_loss: 29289.9876\nEpoch 827/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 21062.8761 - val_loss: 27573.8442\nEpoch 828/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 21385.1757 - val_loss: 28462.4765\nEpoch 829/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 21671.9871 - val_loss: 30115.4506\nEpoch 830/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 20156.0594 - val_loss: 32065.9046\nEpoch 831/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 21779.2266 - val_loss: 29628.2060\nEpoch 832/1000\n1168/1168 [==============================] - 0s 157us/step - loss: 22615.6462 - val_loss: 31494.5970\nEpoch 833/1000\n1168/1168 [==============================] - 0s 160us/step - loss: 21965.5017 - val_loss: 27706.3217\nEpoch 834/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 20673.9678 - val_loss: 37104.2823\nEpoch 835/1000\n1168/1168 [==============================] - 0s 170us/step - loss: 23124.4145 - val_loss: 32262.9098\nEpoch 836/1000\n1168/1168 [==============================] - 0s 181us/step - loss: 22230.5914 - val_loss: 31726.6468\nEpoch 837/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 20072.8726 - val_loss: 28804.5472\nEpoch 838/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21549.2686 - val_loss: 36721.7276\nEpoch 839/1000\n1168/1168 [==============================] - 0s 171us/step - loss: 23010.5418 - val_loss: 28271.3208\nEpoch 840/1000\n1168/1168 [==============================] - 0s 182us/step - loss: 20368.6476 - val_loss: 31012.6084\nEpoch 841/1000\n1168/1168 [==============================] - 0s 179us/step - loss: 21003.0349 - val_loss: 30015.0662\nEpoch 842/1000\n1168/1168 [==============================] - 0s 176us/step - loss: 20219.6168 - val_loss: 27330.4667\nEpoch 843/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 21713.7186 - val_loss: 30891.7735\nEpoch 844/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 20523.1459 - val_loss: 27926.8022\nEpoch 845/1000\n1168/1168 [==============================] - 0s 172us/step - loss: 20210.6114 - val_loss: 29742.1758\nEpoch 846/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 20859.3141 - val_loss: 26589.2922\nEpoch 847/1000\n1168/1168 [==============================] - 0s 170us/step - loss: 21623.7821 - val_loss: 36799.9794\n","name":"stdout"},{"output_type":"stream","text":"Epoch 848/1000\n1168/1168 [==============================] - 0s 171us/step - loss: 23014.2685 - val_loss: 30838.8066\nEpoch 849/1000\n1168/1168 [==============================] - 0s 173us/step - loss: 20869.9667 - val_loss: 31557.2426\nEpoch 850/1000\n1168/1168 [==============================] - 0s 175us/step - loss: 21446.3679 - val_loss: 29263.9494\nEpoch 851/1000\n1168/1168 [==============================] - 0s 171us/step - loss: 20888.6032 - val_loss: 28671.2999\nEpoch 852/1000\n1168/1168 [==============================] - 0s 171us/step - loss: 22879.8661 - val_loss: 27821.9155\nEpoch 853/1000\n1168/1168 [==============================] - 0s 170us/step - loss: 21509.3283 - val_loss: 30259.8077\nEpoch 854/1000\n1168/1168 [==============================] - 0s 171us/step - loss: 21581.9325 - val_loss: 30131.3059\nEpoch 855/1000\n1168/1168 [==============================] - 0s 179us/step - loss: 21274.8280 - val_loss: 29423.6324\nEpoch 856/1000\n1168/1168 [==============================] - 0s 171us/step - loss: 19982.4728 - val_loss: 32488.8984\nEpoch 857/1000\n1168/1168 [==============================] - 0s 169us/step - loss: 20663.5100 - val_loss: 32765.0967\nEpoch 858/1000\n1168/1168 [==============================] - 0s 172us/step - loss: 21708.2601 - val_loss: 32027.7838\nEpoch 859/1000\n1168/1168 [==============================] - 0s 174us/step - loss: 20891.5018 - val_loss: 28193.0743\nEpoch 860/1000\n1168/1168 [==============================] - 0s 176us/step - loss: 21194.7639 - val_loss: 27845.3229\nEpoch 861/1000\n1168/1168 [==============================] - 0s 174us/step - loss: 20863.8452 - val_loss: 29092.7786\nEpoch 862/1000\n1168/1168 [==============================] - 0s 169us/step - loss: 21726.7688 - val_loss: 29973.2556\nEpoch 863/1000\n1168/1168 [==============================] - 0s 173us/step - loss: 20566.1549 - val_loss: 28300.4713\nEpoch 864/1000\n1168/1168 [==============================] - 0s 181us/step - loss: 21779.7188 - val_loss: 30646.8157\nEpoch 865/1000\n1168/1168 [==============================] - 0s 189us/step - loss: 21588.7328 - val_loss: 32143.5523\nEpoch 866/1000\n1168/1168 [==============================] - 0s 175us/step - loss: 22882.9107 - val_loss: 30849.1571\nEpoch 867/1000\n1168/1168 [==============================] - 0s 178us/step - loss: 21612.9387 - val_loss: 32151.2337\nEpoch 868/1000\n1168/1168 [==============================] - 0s 178us/step - loss: 20739.3635 - val_loss: 30483.3477\nEpoch 869/1000\n1168/1168 [==============================] - 0s 170us/step - loss: 20208.9891 - val_loss: 35622.5149\nEpoch 870/1000\n1168/1168 [==============================] - 0s 181us/step - loss: 20814.5111 - val_loss: 30177.5675\nEpoch 871/1000\n1168/1168 [==============================] - 0s 172us/step - loss: 21037.0900 - val_loss: 27846.0735\nEpoch 872/1000\n1168/1168 [==============================] - 0s 171us/step - loss: 20278.8944 - val_loss: 29365.8486\nEpoch 873/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 21037.6855 - val_loss: 28503.2899\nEpoch 874/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 21263.7941 - val_loss: 29583.7373\nEpoch 875/1000\n1168/1168 [==============================] - 0s 168us/step - loss: 20826.8077 - val_loss: 28238.5509\nEpoch 876/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 21110.1614 - val_loss: 30794.8455\nEpoch 877/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 20588.5089 - val_loss: 30721.9424\nEpoch 878/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 20256.5403 - val_loss: 29761.6126\nEpoch 879/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 20965.1732 - val_loss: 28775.7629\nEpoch 880/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 21940.9487 - val_loss: 35710.5948\nEpoch 881/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 20948.7494 - val_loss: 32131.7673\nEpoch 882/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 20938.0748 - val_loss: 27611.9965\nEpoch 883/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22318.4923 - val_loss: 32279.1452\nEpoch 884/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 20721.6389 - val_loss: 29894.5773\nEpoch 885/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 20049.9149 - val_loss: 35775.4723\nEpoch 886/1000\n1168/1168 [==============================] - 0s 168us/step - loss: 21220.2946 - val_loss: 30986.0404\nEpoch 887/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 20530.6233 - val_loss: 31460.6631\nEpoch 888/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 21850.7852 - val_loss: 36822.2285\nEpoch 889/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 20951.0310 - val_loss: 30194.2287\nEpoch 890/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 22344.0933 - val_loss: 45177.6060\nEpoch 891/1000\n1168/1168 [==============================] - 0s 179us/step - loss: 23846.1237 - val_loss: 29445.8740\nEpoch 892/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 20265.0637 - val_loss: 31058.4419\nEpoch 893/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 21390.3959 - val_loss: 36394.6533\nEpoch 894/1000\n1168/1168 [==============================] - 0s 168us/step - loss: 20882.8443 - val_loss: 32171.3840\nEpoch 895/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 20639.7815 - val_loss: 37926.4394\nEpoch 896/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 22210.9439 - val_loss: 34036.7216\nEpoch 897/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 20308.4372 - val_loss: 35247.7110\nEpoch 898/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 20490.6945 - val_loss: 32396.1838\nEpoch 899/1000\n1168/1168 [==============================] - 0s 168us/step - loss: 21149.1638 - val_loss: 28109.5184\nEpoch 900/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 21743.8118 - val_loss: 29405.0904\nEpoch 901/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 21429.2204 - val_loss: 27914.1947\nEpoch 902/1000\n1168/1168 [==============================] - 0s 168us/step - loss: 22619.0201 - val_loss: 36445.2266\nEpoch 903/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 22631.4238 - val_loss: 28978.2592\nEpoch 904/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21332.7822 - val_loss: 37881.6703\nEpoch 905/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 20545.2860 - val_loss: 28775.4346\nEpoch 906/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 22290.8155 - val_loss: 30146.8341\nEpoch 907/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 21052.8223 - val_loss: 28640.7478\nEpoch 908/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 20902.4099 - val_loss: 30189.9049\nEpoch 909/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 20235.9382 - val_loss: 27474.1926\nEpoch 910/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 20246.1263 - val_loss: 36384.1781\nEpoch 911/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 20218.7520 - val_loss: 28355.0759\nEpoch 912/1000\n1168/1168 [==============================] - 0s 169us/step - loss: 21516.7014 - val_loss: 39661.4064\nEpoch 913/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 20156.9414 - val_loss: 32924.0089\nEpoch 914/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 20168.6302 - val_loss: 34026.8460\nEpoch 915/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 20060.5493 - val_loss: 31623.5371\nEpoch 916/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 19985.3644 - val_loss: 32087.4806\nEpoch 917/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 21217.3298 - val_loss: 36732.2665\nEpoch 918/1000\n","name":"stdout"},{"output_type":"stream","text":"1168/1168 [==============================] - 0s 163us/step - loss: 20573.6032 - val_loss: 31643.9013\nEpoch 919/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 21181.2696 - val_loss: 31733.1148\nEpoch 920/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 21069.6674 - val_loss: 32733.1035\nEpoch 921/1000\n1168/1168 [==============================] - 0s 168us/step - loss: 20023.2713 - val_loss: 29308.8703\nEpoch 922/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 21165.8572 - val_loss: 33488.6841\nEpoch 923/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 20172.2303 - val_loss: 35266.1714\nEpoch 924/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 19932.1202 - val_loss: 33336.1410\nEpoch 925/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 20291.2492 - val_loss: 29317.2964\nEpoch 926/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 20959.4527 - val_loss: 31291.9420\nEpoch 927/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 19844.0287 - val_loss: 31824.9006\nEpoch 928/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21249.4213 - val_loss: 36132.8894\nEpoch 929/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21221.6902 - val_loss: 31176.6394\nEpoch 930/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 20901.8545 - val_loss: 38209.7486\nEpoch 931/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 20397.1335 - val_loss: 30155.7398\nEpoch 932/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 20167.2340 - val_loss: 31853.7592\nEpoch 933/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 20650.4497 - val_loss: 31364.0390\nEpoch 934/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 20850.5716 - val_loss: 29733.6284\nEpoch 935/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 19895.2332 - val_loss: 37467.8957\nEpoch 936/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 20397.8440 - val_loss: 31781.8837\nEpoch 937/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 20452.7964 - val_loss: 40368.4840\nEpoch 938/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 20329.3955 - val_loss: 38033.8632\nEpoch 939/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 20880.9051 - val_loss: 37922.6871\nEpoch 940/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 20971.9241 - val_loss: 30149.6917\nEpoch 941/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 20619.1327 - val_loss: 35197.0753\nEpoch 942/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 20279.0266 - val_loss: 32832.9512\nEpoch 943/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21373.1835 - val_loss: 30858.4425\nEpoch 944/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 21715.2862 - val_loss: 32165.8770\nEpoch 945/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 20071.0382 - val_loss: 30985.0924\nEpoch 946/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 21581.2460 - val_loss: 33191.4829\nEpoch 947/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 22013.9834 - val_loss: 33641.7007\nEpoch 948/1000\n1168/1168 [==============================] - 0s 179us/step - loss: 19825.9315 - val_loss: 31926.6998\nEpoch 949/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 21033.6790 - val_loss: 32866.6421\nEpoch 950/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 20811.2977 - val_loss: 31364.9083\nEpoch 951/1000\n1168/1168 [==============================] - 0s 168us/step - loss: 20044.0285 - val_loss: 38835.0334\nEpoch 952/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 20547.3816 - val_loss: 33422.2709\nEpoch 953/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 20662.9531 - val_loss: 30609.9452\nEpoch 954/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 20552.0844 - val_loss: 34260.0628\nEpoch 955/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 20794.4142 - val_loss: 36896.2399\nEpoch 956/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 21353.4647 - val_loss: 33545.3239\nEpoch 957/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 20347.9192 - val_loss: 32628.2760\nEpoch 958/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 20627.8993 - val_loss: 36796.8060\nEpoch 959/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 21841.5468 - val_loss: 31357.2878\nEpoch 960/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 20762.5561 - val_loss: 38596.1775\nEpoch 961/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 20283.6799 - val_loss: 36191.6534\nEpoch 962/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 20799.1816 - val_loss: 32708.8073\nEpoch 963/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 19910.4300 - val_loss: 34480.7869\nEpoch 964/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 20359.9060 - val_loss: 41213.4212\nEpoch 965/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 20244.4489 - val_loss: 40694.4454\nEpoch 966/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 19684.0655 - val_loss: 34095.3401\nEpoch 967/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 19684.6591 - val_loss: 28307.7195\nEpoch 968/1000\n1168/1168 [==============================] - 0s 166us/step - loss: 20972.7832 - val_loss: 33989.0825\nEpoch 969/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 20023.5417 - val_loss: 33518.0464\nEpoch 970/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 20647.5669 - val_loss: 31446.1752\nEpoch 971/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 20403.5901 - val_loss: 36338.6718\nEpoch 972/1000\n1168/1168 [==============================] - 0s 168us/step - loss: 20889.1238 - val_loss: 41030.6346\nEpoch 973/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 20438.1074 - val_loss: 35374.4519\nEpoch 974/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 20090.1345 - val_loss: 29617.4283\nEpoch 975/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 20176.3527 - val_loss: 38352.2319\nEpoch 976/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 20436.3647 - val_loss: 30941.3564\nEpoch 977/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 19697.6786 - val_loss: 37024.1803\nEpoch 978/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 20667.1439 - val_loss: 38802.7363\nEpoch 979/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 22181.7553 - val_loss: 30944.3028\nEpoch 980/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 19675.1728 - val_loss: 33016.0098\nEpoch 981/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 19404.6085 - val_loss: 37732.8657\nEpoch 982/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 20522.0352 - val_loss: 33160.9426\nEpoch 983/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 19500.0232 - val_loss: 31299.3777\nEpoch 984/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 20382.3067 - val_loss: 32860.6599\nEpoch 985/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 20576.8137 - val_loss: 31558.4183\nEpoch 986/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 20071.9880 - val_loss: 35663.4815\nEpoch 987/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 20565.3933 - val_loss: 33978.4600\nEpoch 988/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 20737.5240 - val_loss: 30342.7141\n","name":"stdout"},{"output_type":"stream","text":"Epoch 989/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 19818.7580 - val_loss: 37761.9894\nEpoch 990/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 19615.8455 - val_loss: 35672.1401\nEpoch 991/1000\n1168/1168 [==============================] - 0s 161us/step - loss: 20214.8242 - val_loss: 33797.5461\nEpoch 992/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 19675.9373 - val_loss: 36960.0447\nEpoch 993/1000\n1168/1168 [==============================] - 0s 167us/step - loss: 19480.9640 - val_loss: 35384.9866\nEpoch 994/1000\n1168/1168 [==============================] - 0s 169us/step - loss: 19741.3087 - val_loss: 35464.9955\nEpoch 995/1000\n1168/1168 [==============================] - 0s 164us/step - loss: 20974.1735 - val_loss: 33360.6795\nEpoch 996/1000\n1168/1168 [==============================] - 0s 162us/step - loss: 21163.4391 - val_loss: 31317.2824\nEpoch 997/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 19467.1652 - val_loss: 35128.3790\nEpoch 998/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 19878.9719 - val_loss: 34385.1576\nEpoch 999/1000\n1168/1168 [==============================] - 0s 165us/step - loss: 19568.1898 - val_loss: 36218.4874\nEpoch 1000/1000\n1168/1168 [==============================] - 0s 163us/step - loss: 19731.6882 - val_loss: 40657.2418\n","name":"stdout"},{"output_type":"execute_result","execution_count":75,"data":{"text/plain":"<keras.callbacks.callbacks.History at 0x7f09ec68ef98>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving the keras model to a pickle file\nimport pickle\nfilename = 'keras_DL_model.pkl'\npickle.dump(model , open(filename, 'wb'))","execution_count":76,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction using NN\nprediction_nn = model.predict(df_Test)","execution_count":77,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission\n# create a sample submission\nsubmission_temp = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv')\npred_df = pd.DataFrame(prediction_nn)\nsubmission = pd.concat([submission_temp['Id'], pred_df], axis=1)\nsubmission.columns=['Id', 'SalePrice']\nsubmission.to_csv('submission.csv', index=False)","execution_count":78,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}