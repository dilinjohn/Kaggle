{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\n/kaggle/input/bert-multilingual/bert_multi_from_tfhub/assets/vocab.txt\n/kaggle/input/bert-multilingual/bert_multi_from_tfhub/saved_model.pb/saved_model.pb\n/kaggle/input/bert-multilingual/bert_multi_from_tfhub/variables/variables.index\n/kaggle/input/bert-multilingual/bert_multi_from_tfhub/variables/variables.data-00000-of-00001/variables.data-00000-of-00001\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport time\n\nfrom sklearn import metrics\n\nimport gc","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Configuring TPU's to use in the notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution stratergy\n\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\n    \nexcept ValueError:\n    tpu = None\n    \nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    \n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    \nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU\n    \n    strategy = tf.distribute.get_strategy()\n    \nprint('Replicas: ', strategy.num_replicas_in_sync)","execution_count":3,"outputs":[{"output_type":"stream","text":"Running on TPU  grpc://10.0.0.2:8470\nReplicas:  8\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv')\nvalidation = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\ntest = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/test.csv')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"                 id                                       comment_text  toxic  \\\n0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n\n   severe_toxic  obscene  threat  insult  identity_hate  \n0             0        0       0       0              0  \n1             0        0       0       0              0  \n2             0        0       0       0              0  \n3             0        0       0       0              0  \n4             0        0       0       0              0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation.head()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"   id                                       comment_text lang  toxic\n0   0  Este usuario ni siquiera llega al rango de    ...   es      0\n1   1  Il testo di questa voce pare esser scopiazzato...   it      0\n2   2  Vale. Sólo expongo mi pasado. Todo tiempo pasa...   es      1\n3   3  Bu maddenin alt başlığı olarak  uluslararası i...   tr      0\n4   4  Belçika nın şehirlerinin yanında ilçe ve belde...   tr      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>lang</th>\n      <th>toxic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Este usuario ni siquiera llega al rango de    ...</td>\n      <td>es</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Il testo di questa voce pare esser scopiazzato...</td>\n      <td>it</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Vale. Sólo expongo mi pasado. Todo tiempo pasa...</td>\n      <td>es</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Bu maddenin alt başlığı olarak  uluslararası i...</td>\n      <td>tr</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Belçika nın şehirlerinin yanında ilçe ve belde...</td>\n      <td>tr</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"   id                                            content lang\n0   0  Doctor Who adlı viki başlığına 12. doctor olar...   tr\n1   1   Вполне возможно, но я пока не вижу необходимо...   ru\n2   2  Quindi tu sei uno di quelli   conservativi  , ...   it\n3   3  Malesef gerçekleştirilmedi ancak şöyle bir şey...   tr\n4   4  :Resim:Seldabagcan.jpg resminde kaynak sorunu ...   tr","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>content</th>\n      <th>lang</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Doctor Who adlı viki başlığına 12. doctor olar...</td>\n      <td>tr</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Вполне возможно, но я пока не вижу необходимо...</td>\n      <td>ru</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Quindi tu sei uno di quelli   conservativi  , ...</td>\n      <td>it</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Malesef gerçekleştirilmedi ancak şöyle bir şey...</td>\n      <td>tr</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>:Resim:Seldabagcan.jpg resminde kaynak sorunu ...</td>\n      <td>tr</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop columns in train that are not required\ntrain.drop(['severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1, inplace=True)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Size of train', train.shape)\nprint('Size of validation', validation.shape)\nprint('Size of test', test.shape)","execution_count":9,"outputs":[{"output_type":"stream","text":"Size of train (223549, 3)\nSize of validation (8000, 4)\nSize of test (63812, 3)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Train data size is large. For now, sample a fraction for the dataset, to train the model faster"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sample = train.sample(12000)\nprint('Sample train shape', train_sample.shape)","execution_count":10,"outputs":[{"output_type":"stream","text":"Sample train shape (12000, 3)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Look at the length of the comment to decide on what would be an appropriate length for the sequence"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Max length of a comment in the input train text is', train_sample['comment_text'].apply(lambda x: len(str(x).split())).max())","execution_count":11,"outputs":[{"output_type":"stream","text":"Max length of a comment in the input train text is 1250\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Looks like keeping a sequence length of 500 should be sufficient"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train_sample['comment_text'].apply(lambda x: len(str(x).split())).values)","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f0b1c066c10>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"Split the train into train, validation tests"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nxtrain, xvalid, ytrain, yvalid = train_test_split(train_sample['comment_text'].values, train_sample['toxic'].values, \n                                                    test_size=0.2, random_state=2020,\n                                                    stratify = train_sample['toxic'].values)","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the keras tokenizer here"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Max sequence length\nmax_len=500","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfrom keras.preprocessing import text,sequence\n\nkeras_tokenizer = text.Tokenizer(num_words = None) # no restriction on number of words to keep, based on word frequency\n\nkeras_tokenizer.fit_on_texts(list(xtrain)+ list(xvalid))\n\n# convert to sequences of integers\nxtrain_seq = keras_tokenizer.texts_to_sequences(list(xtrain))\nxvalid_seq = keras_tokenizer.texts_to_sequences(list(xvalid))\n\n# pad sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen= max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen= max_len)\n\n# getting the word to index mapping based on the frequency\nword_index = keras_tokenizer.word_index","execution_count":15,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"CPU times: user 2.24 s, sys: 43.1 ms, total: 2.28 s\nWall time: 2.34 s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Using a simpe RNN model using keras sequential model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Input, Dense, Embedding, GRU, SimpleRNN, LSTM, Bidirectional, Dropout\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, SpatialDropout1D\nfrom keras.optimizers import Adam\nfrom keras.models import Model","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model = Sequential()\n    model.add(Embedding(input_dim = len(word_index)+1, output_dim = 300, input_length=max_len))\n    model.add(SimpleRNN(units =100))\n    model.add(Dense(units=1, activation='sigmoid'))\n    \n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \nprint(model.summary())","execution_count":17,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 500, 300)          13949100  \n_________________________________________________________________\nsimple_rnn (SimpleRNN)       (None, 100)               40100     \n_________________________________________________________________\ndense (Dense)                (None, 1)                 101       \n=================================================================\nTotal params: 13,989,301\nTrainable params: 13,989,301\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# multiply by strategy to run on TPUs or GPUs\n\n#model.fit(xtrain_pad, ytrain, batch_size= 64 * strategy.num_replicas_in_sync, epochs=3, validation_data=[xvalid_pad, yvalid])\nmodel.fit(xtrain_pad, ytrain, batch_size= 64 * strategy.num_replicas_in_sync, epochs=3)","execution_count":18,"outputs":[{"output_type":"stream","text":"Epoch 1/3\n19/19 [==============================] - 3s 159ms/step - loss: 0.3670 - accuracy: 0.8743\nEpoch 2/3\n19/19 [==============================] - 1s 32ms/step - loss: 0.2912 - accuracy: 0.9058\nEpoch 3/3\n19/19 [==============================] - 1s 32ms/step - loss: 0.2399 - accuracy: 0.9073\nCPU times: user 3.65 s, sys: 284 ms, total: 3.93 s\nWall time: 10.8 s\n","name":"stdout"},{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f0a76503c10>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict(xvalid_pad)\nprint('ROC-AUC score', metrics.roc_auc_score(yvalid, prediction))","execution_count":19,"outputs":[{"output_type":"stream","text":"ROC-AUC score 0.7814639626804308\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nEMBEDDING_FILE = '../input/glove840b300dtxt/glove.840B.300d.txt'\n\nembedding_index = {}\n\nwith open(EMBEDDING_FILE, 'r', encoding='utf8') as fp:\n    for line in tqdm(fp):\n        values = line.split(' ')\n        word = values[0]\n        embedding_vector = np.asarray(values[1:], dtype='float32')\n        embedding_index[word] = embedding_vector\n        \nprint('Found {} word vectors', len(embedding_index))        ","execution_count":20,"outputs":[{"output_type":"stream","text":"2196018it [05:35, 6554.65it/s]","name":"stderr"},{"output_type":"stream","text":"Found {} word vectors 2196017\nCPU times: user 5min 34s, sys: 17.8 s, total: 5min 52s\nWall time: 5min 35s\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"### Using LSTM on keras\n\nlets use the glove embeddings in our Embedding layers instead of learning from scratch"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# getting the word embedding for the word in the vocabulary / word_index\n# embedding index is the mapping of the word index to the embedding vector\n\nembed_size = 300\n\nembedding_matrix = np.zeros((len(word_index)+1, embed_size))\n\nfor word, index in tqdm(word_index.items()):\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector\n\nprint('Found embedding for {} words from the vocabulary'.format(len(word_index)))","execution_count":21,"outputs":[{"output_type":"stream","text":"100%|██████████| 46496/46496 [00:00<00:00, 170894.54it/s]","name":"stderr"},{"output_type":"stream","text":"Found embedding for 46496 words from the vocabulary\nCPU times: user 193 ms, sys: 90.6 ms, total: 283 ms\nWall time: 276 ms\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    \n    model = Sequential()\n    model.add(Embedding(input_dim = len(word_index)+1, output_dim=embed_size,\n                       input_length = max_len,\n                       weights = [embedding_matrix], trainable=False))\n    \n    model.add(LSTM(units=100, dropout=0.25, recurrent_dropout=0.25))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    model.compile(loss= 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \nprint(model.summary())","execution_count":22,"outputs":[{"output_type":"stream","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 500, 300)          13949100  \n_________________________________________________________________\nlstm (LSTM)                  (None, 100)               160400    \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 101       \n=================================================================\nTotal params: 14,109,601\nTrainable params: 160,501\nNon-trainable params: 13,949,100\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel.fit(xtrain_pad, ytrain, epochs=3, batch_size=64 * strategy.num_replicas_in_sync, validation_data = [xvalid_pad, yvalid])","execution_count":23,"outputs":[{"output_type":"stream","text":"Epoch 1/3\n19/19 [==============================] - 4s 189ms/step - loss: 0.3613 - accuracy: 0.8633 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 2/3\n19/19 [==============================] - 1s 67ms/step - loss: 0.2439 - accuracy: 0.9170 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 3/3\n19/19 [==============================] - 1s 68ms/step - loss: 0.1848 - accuracy: 0.9308 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nCPU times: user 6.8 s, sys: 386 ms, total: 7.18 s\nWall time: 12.9 s\n","name":"stdout"},{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f0a534b5a10>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict(xvalid_pad)\nprint('ROC-AUC score with LSTM using glove embeddings is ', metrics.roc_auc_score(yvalid, prediction))","execution_count":24,"outputs":[{"output_type":"stream","text":"ROC-AUC score with LSTM using glove embeddings is  0.9343162556683574\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Using GRU(Gated Recurrent Units)\n\nGRU's are a variation on the LSTM because both are designed similarly and, in some cases, produce equally excellent results . GRU's were designed to be simpler and faster than LSTM's and in most cases produce equally good results and thus there is no clear winner."},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    \n    model = Sequential()\n    model.add(Embedding(input_dim = len(word_index) + 1, output_dim=embed_size,\n                       input_length = max_len,\n                       weights = [embedding_matrix], trainable=False))\n    \n    model.add(SpatialDropout1D(0.25))\n    model.add(GRU(units=200))\n    model.add(Dropout(0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    \n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \nprint(model.summary())","execution_count":25,"outputs":[{"output_type":"stream","text":"Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, 500, 300)          13949100  \n_________________________________________________________________\nspatial_dropout1d (SpatialDr (None, 500, 300)          0         \n_________________________________________________________________\ngru (GRU)                    (None, 200)               301200    \n_________________________________________________________________\ndropout (Dropout)            (None, 200)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 201       \n=================================================================\nTotal params: 14,250,501\nTrainable params: 301,401\nNon-trainable params: 13,949,100\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel.fit(xtrain_pad, ytrain, epochs=3, batch_size = 64 * strategy.num_replicas_in_sync)","execution_count":26,"outputs":[{"output_type":"stream","text":"Epoch 1/3\n19/19 [==============================] - 2s 85ms/step - loss: 0.3400 - accuracy: 0.8768\nEpoch 2/3\n19/19 [==============================] - 1s 33ms/step - loss: 0.2211 - accuracy: 0.9258\nEpoch 3/3\n19/19 [==============================] - 1s 33ms/step - loss: 0.1649 - accuracy: 0.9429\nCPU times: user 3.77 s, sys: 251 ms, total: 4.03 s\nWall time: 8.09 s\n","name":"stdout"},{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f0a524c9c10>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict(xvalid_pad)\nprint('ROC-AUC score for GRU on glove embedding is', metrics.roc_auc_score(yvalid, prediction))","execution_count":27,"outputs":[{"output_type":"stream","text":"ROC-AUC score for GRU on glove embedding is 0.9259124325292474\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Using Bidirectional LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model = Sequential()\n    model.add(Embedding(input_dim = len(word_index) + 1, output_dim=embed_size,\n                       input_length = max_len,\n                       weights = [embedding_matrix], trainable=False))\n    model.add(Bidirectional(LSTM(units=300, dropout =0.25, recurrent_dropout=0.25)))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n    \nprint(model.summary())","execution_count":28,"outputs":[{"output_type":"stream","text":"Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_3 (Embedding)      (None, 500, 300)          13949100  \n_________________________________________________________________\nbidirectional (Bidirectional (None, 600)               1442400   \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 601       \n=================================================================\nTotal params: 15,392,101\nTrainable params: 1,443,001\nNon-trainable params: 13,949,100\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel.fit(xtrain_pad, ytrain, epochs=3, batch_size = 64 * strategy.num_replicas_in_sync)\n\npredicton = model.predict(xvalid_pad)\nprint('ROC-AUC score for Bidirectional LSTM on glove embedding is', metrics.roc_auc_score(yvalid, prediction))","execution_count":29,"outputs":[{"output_type":"stream","text":"Epoch 1/3\n19/19 [==============================] - 5s 250ms/step - loss: 0.3131 - accuracy: 0.9003\nEpoch 2/3\n19/19 [==============================] - 3s 148ms/step - loss: 0.1837 - accuracy: 0.9341\nEpoch 3/3\n19/19 [==============================] - 3s 148ms/step - loss: 0.1480 - accuracy: 0.9464\nROC-AUC score for Bidirectional LSTM on glove embedding is 0.9259124325292474\nCPU times: user 7.41 s, sys: 420 ms, total: 7.83 s\nWall time: 22.9 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"16054"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Using BERT model\n\nThe main steps involved here are:\n    * Data preparation: tokenization and encoding of data\n    * Configuring TPU's\n    * Model training and adding an output layer for classification"},{"metadata":{},"cell_type":"markdown","source":"**Tokenizers** (https://github.com/huggingface/tokenizers)\n\nWe will be using the tokenizers from huggingface, which provides an implementation of today's most used tokenizers, with a focus on performance and versatility.\n\n* They are **fast tokenizers** and do all the preprocessing like *truncate*, *pad* and add the *special tokens* the model needs\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tokenizers import BertWordPieceTokenizer\nimport transformers","execution_count":31,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"### Create the fast tokenizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading the BERT tokenizer\n\nbert_tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n\n# save the model locally\n\n#bert_tokenizer.save_pretrained('.')\n\nbert_fast_tokenizer = BertWordPieceTokenizer('../input/bert-multilingual/bert_multi_from_tfhub/assets/vocab.txt',lowercase=False)\nprint(bert_fast_tokenizer)","execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5bce9d6253647939f359d68c582a52a"}},"metadata":{}},{"output_type":"stream","text":"\nTokenizer(vocabulary_size=119547, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=True, lowercase=False, wordpieces_prefix=##)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Using the `tf.data` API. Refer [here](https://www.tensorflow.org/guide/data_performance)\n\nThe tf.data API provides the tf.data.Dataset.prefetch transformation. It can be used to decouple the time when data is produced from the time when data is consumed. In particular, the transformation uses a background thread and an internal buffer to prefetch elements from the input dataset ahead of the time they are requested. *The number of elements to prefetch should be equal to (or possibly greater than) the number of batches consumed by a single training step. *You could either manually tune this value, or set it to **tf.data.experimental.AUTOTUNE** which will prompt the **tf.data** runtime to tune the value dynamically at runtime."},{"metadata":{"trusted":true},"cell_type":"code","source":"#IMP DATA FOR CONFIG\n\nAUTO = tf.data.experimental.AUTOTUNE\n\n\n# Configuration\nEPOCHS = 3\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 192","execution_count":33,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Helper functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    \n    \"\"\"\n    Encoder for encoding the text into sequence of integers for BERT Input\n    \"\"\"\n    \n    tokenizer.enable_truncation(max_length = maxlen)\n    tokenizer.enable_padding(max_length = maxlen)\n    all_ids =[]\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i: i+chunk_size].tolist()\n        # encode the chunk of texts\n        encode_chunk = tokenizer.encode_batch(text_chunk)\n        #use extend to add the chunks to the end of the list\n        all_ids.extend([enc.ids for enc in encode_chunk])\n        \n    \n    #print(type(all_ids))\n    return np.asarray(all_ids)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nx_train = fast_encode(train['comment_text'].astype('str'), bert_fast_tokenizer, maxlen = MAX_LEN)\nx_valid = fast_encode(validation['comment_text'].astype('str'), bert_fast_tokenizer, maxlen = MAX_LEN)\nx_test = fast_encode(test['content'].astype('str'), bert_fast_tokenizer, maxlen = MAX_LEN)","execution_count":35,"outputs":[{"output_type":"stream","text":"100%|██████████| 874/874 [00:31<00:00, 27.55it/s]\n100%|██████████| 32/32 [00:01<00:00, 25.50it/s]\n100%|██████████| 250/250 [00:10<00:00, 24.26it/s]","name":"stderr"},{"output_type":"stream","text":"CPU times: user 2min 26s, sys: 2.66 s, total: 2min 29s\nWall time: 56.4 s\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train['toxic'].values\ny_valid = validation['toxic'].values","execution_count":36,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating the datasets for BERT**\n\nFor more details, refer [here](https://www.tensorflow.org/guide/data)\n\n- .from_tensor_slices() - Creates a Dataset whose elements are slices of the given tensors.The given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the dataset dimension. All input tensors must have the same size in their first dimensions.\n\n- .shuffle() transformation maintains a fixed-size buffer and chooses the next element uniformly at random from that buffer\n- .repeat() - Repeats this dataset so each original value is seen 'count' times or repeat the same datatset for 'n' epochs\n- .prefetch() - Creates a Dataset that prefetches elements from this dataset.Most dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n- .cache() - Caches the elements in this dataset.The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(BATCH_SIZE)\n)","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# building the model\n\ndef build_model(transformer, max_len = 512):\n    \n    input_word_ids = Input(shape =(max_len, ), dtype = tf.int32, name = 'input_words')\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token  = sequence_output[:, 0, :]\n    out = Dense(1, activation ='sigmoid')(cls_token)\n    \n    model = Model(inputs = input_word_ids, outputs = out)\n    model.compile(loss = 'binary_crossentropy', optimizer=Adam(lr=1e-5), metrics=['accuracy'])\n    \n    return model","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    \n    # defining the transformer layer\n    transformer_layer = (\n        transformers.TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased')\n                )\n    \n    model = build_model(transformer_layer, max_len = MAX_LEN)\n\nprint(model.summary())","execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20104976cb5f449ca8119a71e52ec5ff"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=910749124.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9d025bd69634a77adee52edc6d99b06"}},"metadata":{}},{"output_type":"stream","text":"\nModel: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_words (InputLayer)     [(None, 192)]             0         \n_________________________________________________________________\ntf_distil_bert_model (TFDist ((None, 192, 768),)       134734080 \n_________________________________________________________________\ntf_op_layer_strided_slice (T [(None, 768)]             0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 1)                 769       \n=================================================================\nTotal params: 134,734,849\nTrainable params: 134,734,849\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# get the number of steps per epoch\nnsteps = x_train.shape[0] // BATCH_SIZE\n\n\ntrain_history = model.fit(\n                train_dataset,\n                steps_per_epoch = nsteps,\n                validation_data = valid_dataset,\n                epochs = EPOCHS\n                )","execution_count":40,"outputs":[{"output_type":"stream","text":"Epoch 1/3\n1746/1746 [==============================] - 177s 101ms/step - loss: 0.1250 - accuracy: 0.9509 - val_loss: 0.4256 - val_accuracy: 0.8495\nEpoch 2/3\n1746/1746 [==============================] - 166s 95ms/step - loss: 0.0915 - accuracy: 0.9623 - val_loss: 0.4706 - val_accuracy: 0.8499\nEpoch 3/3\n1746/1746 [==============================] - 166s 95ms/step - loss: 0.0788 - accuracy: 0.9674 - val_loss: 0.4767 - val_accuracy: 0.8511\nCPU times: user 1min 27s, sys: 8.18 s, total: 1min 35s\nWall time: 9min 4s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')\nsubmission","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"          id  toxic\n0          0    0.5\n1          1    0.5\n2          2    0.5\n3          3    0.5\n4          4    0.5\n...      ...    ...\n63807  63807    0.5\n63808  63808    0.5\n63809  63809    0.5\n63810  63810    0.5\n63811  63811    0.5\n\n[63812 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>toxic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>63807</th>\n      <td>63807</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>63808</th>\n      <td>63808</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>63809</th>\n      <td>63809</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>63810</th>\n      <td>63810</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>63811</th>\n      <td>63811</td>\n      <td>0.5</td>\n    </tr>\n  </tbody>\n</table>\n<p>63812 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict(test_dataset, verbose=1)\nsubmission['toxic'] = prediction\n\nsubmission.sample(20)","execution_count":42,"outputs":[{"output_type":"stream","text":"499/499 [==============================] - 16s 33ms/step\n","name":"stdout"},{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"          id     toxic\n7744    7744  0.008468\n27737  27737  0.004970\n58731  58731  0.221745\n59640  59640  0.006215\n36553  36553  0.034163\n4295    4295  0.000324\n46955  46955  0.000699\n61340  61340  0.099451\n18816  18816  0.012826\n48947  48947  0.025843\n61506  61506  0.012310\n7827    7827  0.006577\n20410  20410  0.001038\n20554  20554  0.003990\n59915  59915  0.015122\n7465    7465  0.000305\n46811  46811  0.002005\n15504  15504  0.033783\n23009  23009  0.016352\n48103  48103  0.000300","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>toxic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7744</th>\n      <td>7744</td>\n      <td>0.008468</td>\n    </tr>\n    <tr>\n      <th>27737</th>\n      <td>27737</td>\n      <td>0.004970</td>\n    </tr>\n    <tr>\n      <th>58731</th>\n      <td>58731</td>\n      <td>0.221745</td>\n    </tr>\n    <tr>\n      <th>59640</th>\n      <td>59640</td>\n      <td>0.006215</td>\n    </tr>\n    <tr>\n      <th>36553</th>\n      <td>36553</td>\n      <td>0.034163</td>\n    </tr>\n    <tr>\n      <th>4295</th>\n      <td>4295</td>\n      <td>0.000324</td>\n    </tr>\n    <tr>\n      <th>46955</th>\n      <td>46955</td>\n      <td>0.000699</td>\n    </tr>\n    <tr>\n      <th>61340</th>\n      <td>61340</td>\n      <td>0.099451</td>\n    </tr>\n    <tr>\n      <th>18816</th>\n      <td>18816</td>\n      <td>0.012826</td>\n    </tr>\n    <tr>\n      <th>48947</th>\n      <td>48947</td>\n      <td>0.025843</td>\n    </tr>\n    <tr>\n      <th>61506</th>\n      <td>61506</td>\n      <td>0.012310</td>\n    </tr>\n    <tr>\n      <th>7827</th>\n      <td>7827</td>\n      <td>0.006577</td>\n    </tr>\n    <tr>\n      <th>20410</th>\n      <td>20410</td>\n      <td>0.001038</td>\n    </tr>\n    <tr>\n      <th>20554</th>\n      <td>20554</td>\n      <td>0.003990</td>\n    </tr>\n    <tr>\n      <th>59915</th>\n      <td>59915</td>\n      <td>0.015122</td>\n    </tr>\n    <tr>\n      <th>7465</th>\n      <td>7465</td>\n      <td>0.000305</td>\n    </tr>\n    <tr>\n      <th>46811</th>\n      <td>46811</td>\n      <td>0.002005</td>\n    </tr>\n    <tr>\n      <th>15504</th>\n      <td>15504</td>\n      <td>0.033783</td>\n    </tr>\n    <tr>\n      <th>23009</th>\n      <td>23009</td>\n      <td>0.016352</td>\n    </tr>\n    <tr>\n      <th>48103</th>\n      <td>48103</td>\n      <td>0.000300</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}