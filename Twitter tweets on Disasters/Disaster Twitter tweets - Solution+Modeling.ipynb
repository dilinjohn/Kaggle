{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport re # regex\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":204,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":205,"outputs":[{"output_type":"stream","text":"/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/trained-models-and-stored-list/gradientboostingmodel.pkl\n/kaggle/input/trained-models-and-stored-list/knnclassifiermodel.pkl\n/kaggle/input/trained-models-and-stored-list/decisiontreemodel.pkl\n/kaggle/input/trained-models-and-stored-list/valid_words_freq1.pkl\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 1. Load the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ndata_test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ndata_train.head()","execution_count":206,"outputs":[{"output_type":"execute_result","execution_count":206,"data":{"text/plain":"   id keyword location  \\\n0   1     NaN      NaN   \n1   4     NaN      NaN   \n2   5     NaN      NaN   \n3   6     NaN      NaN   \n4   7     NaN      NaN   \n\n                                                                                                  text  \\\n0                                Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n1                                                               Forest fire near La Ronge Sask. Canada   \n2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...   \n3                                    13,000 people receive #wildfires evacuation orders in California    \n4             Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation orders in California</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## 2.Check for missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Size of train data', data_train.shape)\ndata_train.isnull().sum()","execution_count":207,"outputs":[{"output_type":"stream","text":"Size of train data (7613, 5)\n","name":"stdout"},{"output_type":"execute_result","execution_count":207,"data":{"text/plain":"id             0\nkeyword       61\nlocation    2533\ntext           0\ntarget         0\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop columns: 'keyword', 'location', 'id'\ndata_train.drop(['keyword', 'location', 'id'], axis=1, inplace=True)\nprint('Columns {}, {} and {} have been dropped'.format('keyword', 'location', 'id'))","execution_count":208,"outputs":[{"output_type":"stream","text":"Columns keyword, location and id have been dropped\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train","execution_count":209,"outputs":[{"output_type":"execute_result","execution_count":209,"data":{"text/plain":"                                                                                                     text  \\\n0                                   Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n1                                                                  Forest fire near La Ronge Sask. Canada   \n2     All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...   \n3                                       13,000 people receive #wildfires evacuation orders in California    \n4                Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n...                                                                                                   ...   \n7608                  Two giant cranes holding a bridge collapse into nearby homes http://t.co/STfMbbZFB5   \n7609  @aria_ahrary @TheTawniest The out of control wild fires in California even in the Northern part ...   \n7610                                    M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http://t.co/zDtoyd8EbJ   \n7611  Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffer...   \n7612       The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d   \n\n      target  \n0          1  \n1          1  \n2          1  \n3          1  \n4          1  \n...      ...  \n7608       1  \n7609       1  \n7610       1  \n7611       1  \n7612       1  \n\n[7613 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13,000 people receive #wildfires evacuation orders in California</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7608</th>\n      <td>Two giant cranes holding a bridge collapse into nearby homes http://t.co/STfMbbZFB5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7609</th>\n      <td>@aria_ahrary @TheTawniest The out of control wild fires in California even in the Northern part ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7610</th>\n      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http://t.co/zDtoyd8EbJ</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7611</th>\n      <td>Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffer...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7612</th>\n      <td>The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>7613 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## 3. Preprocessing of text data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\n# to stem derived words from the root word\nfrom nltk.stem.porter import PorterStemmer\nimport re","execution_count":210,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing text: Stage 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = set(stopwords.words('english'))\npstem = PorterStemmer()\n\n\ndef stop_stem_lower(tweet):\n    try:\n        regex = re.compile('[^a-zA-Z]')\n        tweet = regex.sub(\" \", tweet)\n        tweet = tweet.lower()\n\n        # check for stop words and then stem\n        #tweet = [pstem.stem(word) for word in tweet.split() if word not in stop and len(word)>2]\n        tweet = [pstem.stem(word) for word in tweet.split() if word not in stop]\n        # join the words back\n        tweet = ' '.join(tweet)\n        return tweet\n    except:\n        return 0","execution_count":211,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# try on a sample\nsample = data_train.sample(frac=0.01, random_state=1)\nsample['Cleaned text'] = sample['text'].apply(lambda x: stop_stem_lower(x))\nsample","execution_count":212,"outputs":[{"output_type":"execute_result","execution_count":212,"data":{"text/plain":"                                                                                                     text  \\\n3228  Goulburn man Henry Van Bilsen missing: Emergency services are searching for a Goulburn man who d...   \n3706  The things we fear most in organizations--fluctuations disturbances imbalances--are the primary ...   \n6957                                                                              @tsunami_esh ?? hey Esh   \n2887  @POTUS you until you drown by water entering the lungs. You being alive has caused this great co...   \n7464                                                  Crawling in my skin\\nThese wounds they will not hea   \n...                                                                                                   ...   \n299   The latest from @BryanSinger reveals #Storm is a queen in #Apocalypse @RuPaul @AlexShipppp http:...   \n1873                                                        sevenfigz has a crush: http://t.co/20B3PnQxMD   \n1660                            Look: #I have collapsed #after attempting to munch an endangered species.   \n6929                          why is it trouble@niallhariss / @simply_vain live on http://t.co/iAhJj0agq6   \n4493                                                        @eggalie haha I love hurricane because of you   \n\n      target  \\\n3228       1   \n3706       0   \n6957       0   \n2887       0   \n7464       1   \n...      ...   \n299        1   \n1873       1   \n1660       0   \n6929       0   \n4493       0   \n\n                                                                                         Cleaned text  \n3228  goulburn man henri van bilsen miss emerg servic search goulburn man disappear http co z pkjztrp  \n3706                  thing fear organ fluctuat disturb imbal primari sourc creativ margaret wheatley  \n6957                                                                              tsunami esh hey esh  \n2887                              potu drown water enter lung aliv caus great countri fall shit pussi  \n7464                                                                             crawl skin wound hea  \n...                                                                                               ...  \n299                latest bryansing reveal storm queen apocalyps rupaul alexshipppp http co oqw jx rt  \n1873                                                                 sevenfigz crush http co b pnqxmd  \n1660                                                          look collaps attempt munch endang speci  \n6929                                            troubl niallhariss simpli vain live http co iahjj agq  \n4493                                                                        eggali haha love hurrican  \n\n[76 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n      <th>Cleaned text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3228</th>\n      <td>Goulburn man Henry Van Bilsen missing: Emergency services are searching for a Goulburn man who d...</td>\n      <td>1</td>\n      <td>goulburn man henri van bilsen miss emerg servic search goulburn man disappear http co z pkjztrp</td>\n    </tr>\n    <tr>\n      <th>3706</th>\n      <td>The things we fear most in organizations--fluctuations disturbances imbalances--are the primary ...</td>\n      <td>0</td>\n      <td>thing fear organ fluctuat disturb imbal primari sourc creativ margaret wheatley</td>\n    </tr>\n    <tr>\n      <th>6957</th>\n      <td>@tsunami_esh ?? hey Esh</td>\n      <td>0</td>\n      <td>tsunami esh hey esh</td>\n    </tr>\n    <tr>\n      <th>2887</th>\n      <td>@POTUS you until you drown by water entering the lungs. You being alive has caused this great co...</td>\n      <td>0</td>\n      <td>potu drown water enter lung aliv caus great countri fall shit pussi</td>\n    </tr>\n    <tr>\n      <th>7464</th>\n      <td>Crawling in my skin\\nThese wounds they will not hea</td>\n      <td>1</td>\n      <td>crawl skin wound hea</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>299</th>\n      <td>The latest from @BryanSinger reveals #Storm is a queen in #Apocalypse @RuPaul @AlexShipppp http:...</td>\n      <td>1</td>\n      <td>latest bryansing reveal storm queen apocalyps rupaul alexshipppp http co oqw jx rt</td>\n    </tr>\n    <tr>\n      <th>1873</th>\n      <td>sevenfigz has a crush: http://t.co/20B3PnQxMD</td>\n      <td>1</td>\n      <td>sevenfigz crush http co b pnqxmd</td>\n    </tr>\n    <tr>\n      <th>1660</th>\n      <td>Look: #I have collapsed #after attempting to munch an endangered species.</td>\n      <td>0</td>\n      <td>look collaps attempt munch endang speci</td>\n    </tr>\n    <tr>\n      <th>6929</th>\n      <td>why is it trouble@niallhariss / @simply_vain live on http://t.co/iAhJj0agq6</td>\n      <td>0</td>\n      <td>troubl niallhariss simpli vain live http co iahjj agq</td>\n    </tr>\n    <tr>\n      <th>4493</th>\n      <td>@eggalie haha I love hurricane because of you</td>\n      <td>0</td>\n      <td>eggali haha love hurrican</td>\n    </tr>\n  </tbody>\n</table>\n<p>76 rows Ã— 3 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply stop_stem_lower(tweet) on the entire train\ntemp = pd.DataFrame()\n\n# changing the width of the column\npd.set_option('display.max_colwidth', 100)\n\nimport time\nstart= time.time()\ntemp['original text'] = data_train['text'].copy()\ntemp['cleaned_text'] = data_train['text'].apply(lambda x: stop_stem_lower(x))\nprint('time taken: ', time.time() - start)\ntemp.head(10)","execution_count":213,"outputs":[{"output_type":"stream","text":"time taken:  3.639029026031494\n","name":"stdout"},{"output_type":"execute_result","execution_count":213,"data":{"text/plain":"                                                                                         original text  \\\n0                                Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n1                                                               Forest fire near La Ronge Sask. Canada   \n2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...   \n3                                    13,000 people receive #wildfires evacuation orders in California    \n4             Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n5  #RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAf...   \n6      #flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas   \n7                                          I'm on top of the hill and I can see a fire in the woods...   \n8                      There's an emergency evacuation happening now in the building across the street   \n9                                                 I'm afraid that the tornado is coming to our area...   \n\n                                                                    cleaned_text  \n0                                      deed reason earthquak may allah forgiv us  \n1                                           forest fire near la rong sask canada  \n2          resid ask shelter place notifi offic evacu shelter place order expect  \n3                                    peopl receiv wildfir evacu order california  \n4                           got sent photo rubi alaska smoke wildfir pour school  \n5  rockyfir updat california hwi close direct due lake counti fire cafir wildfir  \n6     flood disast heavi rain caus flash flood street manit colorado spring area  \n7                                                         top hill see fire wood  \n8                                         emerg evacu happen build across street  \n9                                                       afraid tornado come area  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original text</th>\n      <th>cleaned_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n      <td>deed reason earthquak may allah forgiv us</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>forest fire near la rong sask canada</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...</td>\n      <td>resid ask shelter place notifi offic evacu shelter place order expect</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13,000 people receive #wildfires evacuation orders in California</td>\n      <td>peopl receiv wildfir evacu order california</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n      <td>got sent photo rubi alaska smoke wildfir pour school</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>#RockyFire Update =&gt; California Hwy. 20 closed in both directions due to Lake County fire - #CAf...</td>\n      <td>rockyfir updat california hwi close direct due lake counti fire cafir wildfir</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas</td>\n      <td>flood disast heavi rain caus flash flood street manit colorado spring area</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>I'm on top of the hill and I can see a fire in the woods...</td>\n      <td>top hill see fire wood</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>There's an emergency evacuation happening now in the building across the street</td>\n      <td>emerg evacu happen build across street</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>I'm afraid that the tornado is coming to our area...</td>\n      <td>afraid tornado come area</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to store the cleaned text\n# to use in the next stage for data preprocessing\n\ncorpus = temp['cleaned_text'].values.tolist()\nprint(corpus[:10])\ndel temp","execution_count":214,"outputs":[{"output_type":"stream","text":"['deed reason earthquak may allah forgiv us', 'forest fire near la rong sask canada', 'resid ask shelter place notifi offic evacu shelter place order expect', 'peopl receiv wildfir evacu order california', 'got sent photo rubi alaska smoke wildfir pour school', 'rockyfir updat california hwi close direct due lake counti fire cafir wildfir', 'flood disast heavi rain caus flash flood street manit colorado spring area', 'top hill see fire wood', 'emerg evacu happen build across street', 'afraid tornado come area']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing text: Stage 2\n* **Check for words that are occur very less in our tweets**\n* **They should be removed from our Bag of words to reduce dimensionality**\n"},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Using CountVectorizer"},{"metadata":{},"cell_type":"markdown","source":"### CountVectorizer does the following by default:\n- lowercases your text (set lowercase=false if you donâ€™t want lowercasing)\n- uses utf-8 encoding\n- performs tokenization (converts raw text to smaller units of text)\n- uses word level tokenization (meaning each word is treated as a separate token)\n- ignores single characters during tokenization (say goodbye to words like â€˜aâ€™ and â€˜Iâ€™)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\ncvect = CountVectorizer(stop_words='english') # the stop words were already removed, but i am just adding the paramater here\n\nstart = time.time()\ncvect_fit = cvect.fit_transform(corpus) # this return a 'scipy.sparse.csr.csr_matrix' (Compressed Sparsed row matrix)\nprint(type(cvect_fit))\nprint('Shape of the CSR matrix after the fit and transform method: ', cvect_fit.shape)\nprint('Number of documents in the train corpus: ', data_train.shape[0])\nprint('time taken for CountVectorizer on the cleaned corpus is', time.time() - start)","execution_count":215,"outputs":[{"output_type":"stream","text":"<class 'scipy.sparse.csr.csr_matrix'>\nShape of the CSR matrix after the fit and transform method:  (7613, 18739)\nNumber of documents in the train corpus:  7613\ntime taken for CountVectorizer on the cleaned corpus is 0.25030970573425293\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Each row in the CSR matrix represents a document and each column is a feature or a word\n- **Each column entry is the count for that feature in that document**\n\n### CountVectorizer methods:\n- After CountVectorizer has been fit on the corpus:\n    - CountVectorizer().vocabulary_ : returns the unique words and their positions in the sparse CSR matrix\n    - CountVectorizer().get_feature_names() : will list all the features or unique words\n    - CountVectorizer().fit_transform(corpus).toarray(): converts the sparse matrix into an array, where each row is a document in the corpus and each column entry is the occurence of the word in the document\n    - CountVectorizer().fit_transform(corpus).sum(axis=0): returns the frequency of the words/features in the entire corpus. Shape: (1 X number of features)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# the numbers are not 'counts', but the positions in the sparse vector\ndisplay(cvect.vocabulary_)\n\nword_freq_csr = cvect_fit.sum(axis=0) # get the row wise sum of the frequencies of each feature/words across all the documents","execution_count":216,"outputs":[{"output_type":"display_data","data":{"text/plain":"{'deed': 3525,\n 'reason': 13115,\n 'earthquak': 4284,\n 'allah': 389,\n 'forgiv': 5410,\n 'forest': 5404,\n 'near': 10686,\n 'la': 8797,\n 'rong': 13605,\n 'sask': 13938,\n 'canada': 2118,\n 'resid': 13296,\n 'ask': 788,\n 'shelter': 14290,\n 'place': 12132,\n 'notifi': 10995,\n 'offic': 11267,\n 'evacu': 4783,\n 'order': 11512,\n 'expect': 4858,\n 'peopl': 11917,\n 'receiv': 13131,\n 'wildfir': 17530,\n 'california': 2085,\n 'got': 6117,\n 'sent': 14157,\n 'photo': 12026,\n 'rubi': 13728,\n 'alaska': 343,\n 'smoke': 14606,\n 'pour': 12310,\n 'school': 14011,\n 'rockyfir': 13560,\n 'updat': 16649,\n 'hwi': 7111,\n 'close': 2674,\n 'direct': 3774,\n 'lake': 8824,\n 'counti': 3011,\n 'cafir': 2058,\n 'flood': 5308,\n 'disast': 3789,\n 'heavi': 6621,\n 'rain': 12983,\n 'caus': 2250,\n 'flash': 5274,\n 'street': 15093,\n 'manit': 9623,\n 'colorado': 2800,\n 'spring': 14863,\n 'area': 686,\n 'hill': 6754,\n 'wood': 17669,\n 'emerg': 4520,\n 'happen': 6476,\n 'build': 1919,\n 'afraid': 201,\n 'tornado': 16002,\n 'come': 2817,\n 'die': 3739,\n 'heat': 6617,\n 'wave': 17319,\n 'far': 4995,\n 'haha': 6416,\n 'south': 14762,\n 'tampa': 15426,\n 'hah': 6415,\n 'wait': 17239,\n 'second': 14100,\n 'live': 9184,\n 'gonna': 6088,\n 'fvck': 5647,\n 'florida': 5315,\n 'tampabay': 15427,\n 'day': 3434,\n 'lost': 9342,\n 'count': 3007,\n 'bago': 1059,\n 'myanmar': 10519,\n 'arriv': 733,\n 'damag': 3338,\n 'bu': 1892,\n 'multi': 10440,\n 'car': 2164,\n 'crash': 3071,\n 'break': 1762,\n 'man': 9608,\n 'love': 9353,\n 'fruit': 5566,\n 'summer': 15192,\n 'fast': 5011,\n 'goooooooaaaaaal': 6103,\n 'ridicul': 13433,\n 'london': 9299,\n 'cool': 2951,\n 'ski': 14491,\n 'wonder': 17664,\n 'looooool': 9317,\n 'way': 17321,\n 'eat': 4299,\n 'shit': 14314,\n 'nyc': 11137,\n 'week': 17390,\n 'girlfriend': 5960,\n 'cooool': 2954,\n 'like': 9126,\n 'pasta': 11798,\n 'end': 4575,\n 'bbcmtd': 1176,\n 'wholesal': 17502,\n 'market': 9673,\n 'ablaz': 41,\n 'http': 7019,\n 'lhyxeohi': 9079,\n 'alway': 436,\n 'tri': 16147,\n 'bring': 1807,\n 'metal': 9909,\n 'rt': 13706,\n 'yao': 18157,\n 'xngw': 17993,\n 'africanbaz': 205,\n 'news': 10753,\n 'nigeria': 10817,\n 'flag': 5269,\n 'set': 14190,\n 'aba': 18,\n 'nndbgwyei': 10911,\n 'cri': 3109,\n 'plu': 12184,\n 'look': 9313,\n 'sky': 14507,\n 'night': 10822,\n 'qqsmshaj': 12814,\n 'phdsquar': 12004,\n 'mufc': 10427,\n 'built': 1922,\n 'hype': 7137,\n 'new': 10740,\n 'acquisit': 105,\n 'doubt': 4001,\n 'epl': 4660,\n 'season': 14088,\n 'inec': 7452,\n 'abia': 37,\n 'imaomknna': 7349,\n 'barbado': 1112,\n 'bridgetown': 1794,\n 'jamaica': 7791,\n 'santa': 13921,\n 'cruz': 3150,\n 'head': 6588,\n 'st': 14928,\n 'elizabeth': 4485,\n 'polic': 12245,\n 'superintend': 15226,\n 'wdueaj': 17358,\n 'lord': 9326,\n 'check': 2431,\n 'roi': 13575,\n 'nsmejj': 11063,\n 'tj': 15882,\n 'zjin': 18574,\n 'yduixefip': 18197,\n 'lxtjc': 9491,\n 'kl': 8582,\n 'nsfw': 11056,\n 'outsid': 11600,\n 'aliv': 387,\n 'dead': 3477,\n 'insid': 7509,\n 'awesom': 962,\n 'time': 15850,\n 'visit': 17029,\n 'cfc': 2346,\n 'ancop': 510,\n 'site': 14451,\n 'thank': 15660,\n 'tita': 15872,\n 'vida': 16978,\n 'care': 2175,\n 'soooo': 14732,\n 'pump': 12577,\n 'southridgelif': 14770,\n 'want': 17262,\n 'chicago': 2460,\n 'preach': 12367,\n 'hotel': 6944,\n 'qknbfofx': 12755,\n 'gain': 5719,\n 'follow': 5370,\n 'know': 8627,\n 'stat': 14972,\n 'grow': 6250,\n 'tiyulif': 15881,\n 'west': 17419,\n 'burn': 1954,\n 'thousand': 15783,\n 'alon': 416,\n 'vl': 17061,\n 'tbr': 15486,\n 'wbr': 17336,\n 'perfect': 11923,\n 'tracklist': 16071,\n 'life': 9103,\n 'leav': 8961,\n 'retain': 13327,\n 'quit': 12875,\n 'weird': 17401,\n 'better': 1347,\n 'use': 16712,\n 'wear': 17370,\n 'everi': 4796,\n 'singl': 14425,\n 'year': 18202,\n 'deputi': 3615,\n 'shot': 14336,\n 'brighton': 1803,\n 'home': 6876,\n 'gwnrhmso': 6370,\n 'wife': 17523,\n 'jail': 7783,\n 'niec': 10814,\n 'ev': 4778,\n 'ahoucza': 260,\n 'lanford': 8851,\n 'salmon': 13872,\n 'vplr': 17119,\n 'hka': 6796,\n 'sxhw': 15339,\n 'tnnlf': 15930,\n 'arsonist': 738,\n 'deliber': 3562,\n 'black': 1507,\n 'church': 2548,\n 'north': 10966,\n 'carolina': 2199,\n 'pcxarbh': 11865,\n 'noch': 10928,\n 'el': 4448,\n 'bestia': 1337,\n 'alexi': 369,\n 'sanchez': 13899,\n 'happi': 6477,\n 'teammat': 15529,\n 'train': 16089,\n 'hard': 6485,\n 'goodnight': 6094,\n 'gunner': 6333,\n 'uc': 16398,\n 'jhvgr': 7966,\n 'kurd': 8731,\n 'trampl': 16095,\n 'turkmen': 16266,\n 'later': 8879,\n 'vandal': 16830,\n 'diyala': 3860,\n 'izfdyc': 7747,\n 'cg': 2353,\n 'truck': 16202,\n 'voortrekk': 17105,\n 'ave': 935,\n 'tambo': 15423,\n 'intl': 7562,\n 'cargo': 2181,\n 'section': 14104,\n 'kscqkfkkf': 8708,\n 'heart': 6607,\n 'citi': 2584,\n 'gift': 5935,\n 'skylin': 14512,\n 'kiss': 8552,\n 'lip': 9157,\n 'cyompz': 3287,\n 'tonight': 15976,\n 'lo': 9262,\n 'angel': 522,\n 'ig': 7255,\n 'fb': 5032,\n 'sunset': 15218,\n 'peep': 11899,\n 'icsjgz': 7202,\n 'te': 15518,\n 'climat': 2659,\n 'energi': 4587,\n 'fxmn': 5680,\n 'bd': 1209,\n 'revel': 13347,\n 'wmv': 17630,\n 'video': 16979,\n 'mean': 9816,\n 'mac': 9529,\n 'farewel': 4997,\n 'en': 4563,\n 'rout': 13644,\n 'dvd': 4204,\n 'gtxrwm': 6291,\n 'progress': 12464,\n 'greet': 6209,\n 'month': 10271,\n 'student': 15126,\n 'pen': 11906,\n 'torch': 15998,\n 'public': 12555,\n 'fxpixqujt': 5681,\n 'rene': 13250,\n 'amp': 485,\n 'jacinta': 7760,\n 'secret': 14102,\n 'fallen': 4966,\n 'edit': 4344,\n 'mar': 9644,\n 'mlmsuzv': 10166,\n 'navista': 10628,\n 'steve': 15012,\n 'someth': 14706,\n 'els': 4499,\n 'tinderbox': 15860,\n 'clown': 2684,\n 'hood': 6909,\n 'nowplay': 11013,\n 'ian': 7161,\n 'buff': 1912,\n 'magnitud': 9569,\n 'av': 930,\n 'jsjfftc': 8154,\n 'edm': 4349,\n 'nxwestmidland': 11132,\n 'huge': 7036,\n 'rwzbfvnxer': 13795,\n 'talk': 15416,\n 'make': 9590,\n 'work': 17680,\n 'kid': 8501,\n 'cuz': 3238,\n 'bicycl': 1402,\n 'accid': 72,\n 'split': 14840,\n 'testicl': 15616,\n 'imposs': 7383,\n 'michael': 9975,\n 'father': 5019,\n 'nashvilletraff': 10601,\n 'traffic': 16080,\n 'slower': 14568,\n 'usual': 16726,\n 'ghk': 5910,\n 'egj': 4397,\n 'center': 2322,\n 'lane': 8850,\n 'block': 1555,\n 'santaclara': 13922,\n 'nb': 10635,\n 'great': 6192,\n 'america': 465,\n 'pkwi': 12129,\n 'bayarea': 1167,\n 'pmlohzurwr': 12207,\n 'gkye': 5994,\n 'gjtk': 5980,\n 'personalinjuri': 11946,\n 'read': 13092,\n 'advic': 167,\n 'solicitor': 14695,\n 'help': 6653,\n 'otleyhour': 11563,\n 'stloui': 15039,\n 'caraccidentlawy': 2166,\n 'speed': 14812,\n 'teen': 15554,\n 'zomof': 18618,\n 'kxvm': 8774,\n 'cba': 2260,\n 'tee': 15551,\n 'report': 13270,\n 'motor': 10326,\n 'vehicl': 16900,\n 'curri': 3223,\n 'herman': 6668,\n 'rd': 13073,\n 'stephenson': 15005,\n 'involv': 7581,\n 'overturn': 11631,\n 'pleas': 12162,\n 'ybjezkurw': 18173,\n 'bigrigradio': 1419,\n 'awar': 957,\n 'mile': 10016,\n 'marker': 9672,\n 'mooresvil': 10284,\n 'iredel': 7625,\n 'ramp': 13001,\n 'pm': 12194,\n 'sleepjunki': 14542,\n 'sleep': 14540,\n 'pill': 12065,\n 'doubl': 3998,\n 'risk': 13462,\n 'nm': 10895,\n 'fict': 5171,\n 'knew': 8613,\n 'gon': 6085,\n 'ysxun': 18370,\n 'vceh': 16864,\n 'cabrillo': 2043,\n 'magellan': 9560,\n 'mir': 10071,\n 'congest': 2891,\n 'pastor': 11800,\n 'scene': 14001,\n 'owner': 11650,\n 'rang': 13012,\n 'rover': 13649,\n 'mom': 10244,\n 'wish': 17576,\n 'spilt': 14828,\n 'mayonnais': 9754,\n 'horribl': 6931,\n 'past': 11797,\n 'sunday': 15209,\n 'final': 5201,\n 'abl': 40,\n 'god': 6061,\n 'piss': 12092,\n 'donni': 3969,\n 'tell': 15568,\n 'anoth': 559,\n 'truckcrash': 16203,\n 'fortworth': 5428,\n 'interst': 7556,\n 'rs': 13689,\n 'lj': 9205,\n 'qfp': 12721,\n 'click': 2655,\n 'gt': 6279,\n 'ld': 8937,\n 'uniyw': 16591,\n 'ashvil': 781,\n 'sb': 13973,\n 'sr': 14903,\n 'hylmo': 7136,\n 'wgfi': 17451,\n 'motorcyclist': 10329,\n 'cross': 3130,\n 'median': 9831,\n 'motorcycl': 10328,\n 'rider': 13430,\n 'travel': 16115,\n 'lzrlmi': 9521,\n 'fyi': 5688,\n 'cad': 2048,\n 'properti': 12486,\n 'nh': 10790,\n 'piner': 12072,\n 'horndal': 6924,\n 'dr': 4034,\n 'naayf': 10556,\n 'turn': 16267,\n 'chandane': 2385,\n 'magu': 9572,\n 'mma': 10171,\n 'taxi': 15467,\n 'ram': 12998,\n 'halfway': 6435,\n 'everyon': 4803,\n 'conf': 2882,\n 'left': 8976,\n 'manchest': 9610,\n 'eddi': 4335,\n 'stop': 15056,\n 'delay': 3558,\n 'min': 10037,\n 'oia': 11317,\n 'fxi': 5678,\n 'gm': 6036,\n 'wpd': 17721,\n 'th': 15655,\n 'injuri': 7492,\n 'willi': 17535,\n 'foreman': 5402,\n 'vckit': 16870,\n 'edev': 4338,\n 'aashiqui': 13,\n 'actress': 121,\n 'anu': 578,\n 'aggarw': 229,\n 'fatal': 5016,\n 'otfp': 11557,\n 'lqw': 9389,\n 'suffield': 15179,\n 'alberta': 349,\n 'bptmlf': 1720,\n 'backup': 1035,\n 'right': 13438,\n 'exit': 4850,\n 'langtre': 8854,\n 'consid': 2908,\n 'nc': 10653,\n 'altern': 430,\n 'chang': 2389,\n 'determin': 3650,\n 'option': 11482,\n 'financi': 5204,\n 'support': 15237,\n 'plan': 12135,\n 'treatment': 16125,\n 'deadli': 3480,\n 'hagerstown': 6414,\n 'today': 15939,\n 'state': 14973,\n 'whag': 17466,\n 'flowri': 5321,\n 'marinad': 9668,\n 'fuck': 5599,\n 'mf': 9935,\n 'drive': 4078,\n 'norwaymfa': 10979,\n 'bahrain': 1061,\n 'previous': 12418,\n 'road': 13524,\n 'kill': 8522,\n 'explos': 4874,\n 'gfjfgtodad': 5881,\n 'heard': 6604,\n 'leader': 8949,\n 'kenya': 8429,\n 'forward': 5429,\n 'comment': 2829,\n 'issu': 7668,\n 'disciplinari': 3791,\n 'measur': 9821,\n 'arrestpastornganga': 732,\n 'aftershock': 216,\n 'delo': 3568,\n 'scuf': 14055,\n 'ps': 12516,\n 'game': 5735,\n 'cya': 3271,\n 'effort': 4383,\n 'pain': 11696,\n 'win': 17545,\n 'roger': 13568,\n 'bannist': 1100,\n 'ir': 7618,\n 'icemoon': 7187,\n 'ynxnvvkcda': 18300,\n 'djicemoon': 3866,\n 'dubstep': 4153,\n 'trapmus': 16111,\n 'dnb': 3921,\n 'danc': 3354,\n 'ice': 7184,\n 'weqpesenku': 17415,\n 'victori': 16972,\n 'bargain': 1123,\n 'basement': 1142,\n 'price': 12420,\n 'dwight': 4219,\n 'david': 3419,\n 'eisenhow': 4428,\n 'vam': 16825,\n 'podgyw': 12231,\n 'zevakjapcz': 18514,\n 'nobodi': 10927,\n 'rememb': 13236,\n 'came': 2097,\n 'charl': 2406,\n 'schulz': 14014,\n 'im': 7342,\n 'speak': 14794,\n 'someon': 14705,\n 'xb': 17884,\n 'harder': 6491,\n 'conflict': 2888,\n 'gloriou': 6026,\n 'triumph': 16174,\n 'thoma': 15773,\n 'growingupspoil': 6253,\n 'clay': 2634,\n 'pigeon': 12056,\n 'shoot': 14327,\n 'guess': 6307,\n 'actual': 122,\n 'free': 5507,\n 'tc': 15491,\n 'terrifi': 15605,\n 'best': 1333,\n 'roller': 13584,\n 'coaster': 2732,\n 'disclaim': 3792,\n 'xmwodfmtui': 17989,\n 'jdzmgjow': 7885,\n 'uhasfkbv': 16462,\n 'epzhoth': 4665,\n 'kjforday': 8566,\n 'thyzomvwu': 15823,\n 'joo': 8097,\n 'xk': 17960,\n 'wisdomw': 17573,\n 'bonu': 1655,\n 'minut': 10063,\n 'daili': 3322,\n 'habit': 6409,\n 'realli': 13109,\n 'improv': 7386,\n 'mani': 9620,\n 'alreadi': 423,\n 'lifehack': 9106,\n 'tbm': 15483,\n 'fqb': 5469,\n 'cw': 3248,\n 'protect': 12496,\n 'profit': 12460,\n 'global': 6017,\n 'meltdown': 9868,\n 'wiedem': 17521,\n 'wztz': 17872,\n 'hgmvq': 6699,\n 'moment': 10245,\n 'scari': 13993,\n 'guy': 6350,\n 'scream': 14043,\n 'bloodi': 1565,\n 'murder': 10456,\n 'silverwood': 14407,\n 'stream': 15090,\n 'youtub': 18333,\n 'vve': 17176,\n 'usesgf': 16715,\n 'book': 1657,\n 'ntuc': 11070,\n 'esquireattir': 4724,\n 'sometim': 14708,\n 'face': 4934,\n 'difficulti': 3749,\n 'wrong': 17767,\n 'joel': 8059,\n 'osteen': 11551,\n 'thing': 15752,\n 'stand': 14948,\n 'dream': 4057,\n 'belief': 1283,\n 'possibl': 12295,\n 'brown': 1851,\n 'prais': 12356,\n 'ministri': 10055,\n 'wdyouth': 17362,\n 'biblestudi': 1396,\n 'ujk': 16483,\n 'gbcc': 5789,\n 'avoid': 945,\n 'trap': 16109,\n 'think': 15754,\n 'lose': 9338,\n 'job': 8051,\n 'orang': 11500,\n 'onfireand': 11416,\n 'bb': 1170,\n 'jv': 8224,\n 'ppkhji': 12331,\n 'kick': 8500,\n 'say': 13969,\n 'interrupt': 7552,\n 'georg': 5847,\n 'bernard': 1327,\n 'shaw': 14273,\n 'oyster': 11667,\n 'shell': 14288,\n 'andrew': 516,\n 'carnegi': 2196,\n 'anyon': 586,\n 'need': 10695,\n 'play': 12149,\n 'hybrid': 7132,\n 'slayer': 14536,\n 'eu': 4761,\n 'hmu': 6827,\n 'cod': 2748,\n 'sandscrim': 13907,\n 'empirikgam': 4546,\n 'codawscrim': 2749,\n 'tp': 16044,\n 'kotc': 8663,\n 'tpfa': 16047,\n 'org': 11516,\n 'expert': 4863,\n 'franc': 5489,\n 'begin': 1264,\n 'examin': 4830,\n 'airplan': 288,\n 'debri': 3498,\n 'reunion': 13339,\n 'island': 7661,\n 'french': 5531,\n 'air': 280,\n 'yvvpznzmxg': 18411,\n 'strict': 15102,\n 'liabil': 9081,\n 'context': 2929,\n 'pilot': 12068,\n 'error': 4696,\n 'common': 2836,\n 'compon': 2859,\n 'aviat': 942,\n 'cr': 3054,\n 'cz': 3296,\n 'bohrd': 1631,\n 'crobscarla': 3125,\n 'lifetim': 9112,\n 'odd': 11233,\n 'wedn': 17382,\n 'bkpfpogysi': 1498,\n 'alexalltimelow': 363,\n 'awwww': 979,\n 'cuti': 3232,\n 'good': 6089,\n 'famili': 4973,\n 'member': 9869,\n 'osama': 11539,\n 'bin': 1442,\n 'laden': 8808,\n 'iron': 7637,\n 'mhmmm': 9966,\n 'gov': 6126,\n 'suspect': 15269,\n 'goe': 6068,\n 'engin': 4590,\n 'tyjxrfd': 16349,\n 'wing': 17557,\n 'kztevb': 8796,\n 'cessna': 2340,\n 'ocampo': 11217,\n 'coahuila': 2726,\n 'mexico': 9930,\n 'juli': 8186,\n 'men': 9876,\n 'includ': 7407,\n 'govern': 6128,\n 'offici': 11269,\n 'watchthevideo': 17303,\n 'xrvgjik': 18048,\n 'lsmx': 9401,\n 'vwr': 17188,\n 'wednesday': 17384,\n 'began': 1262,\n 'kca': 8374,\n 'votejkt': 17109,\n 'id': 7208,\n 'mbataweel': 9763,\n 'rip': 13456,\n 'binladen': 1446,\n 'cowork': 3030,\n 'nude': 11077,\n 'mode': 10210,\n 'mickinyman': 9983,\n 'theatlant': 15676,\n 'wreck': 17750,\n 'polit': 12250,\n 'tagzbcxfj': 15396,\n 'mlb': 10156,\n 'unbeliev': 16535,\n 'insan': 7505,\n 'airport': 289,\n 'aircraft': 282,\n 'aeroplan': 178,\n 'runway': 13752,\n 'freaki': 5501,\n 'cezhq': 2343,\n 'czll': 3301,\n 'wq': 17731,\n 'wjsgphl': 17597,\n 'tfcdronra': 15630,\n 'usama': 16703,\n 'ladin': 8810,\n 'natur': 10618,\n 'plane': 12136,\n 'festiv': 5126,\n 'kq': 8681,\n 'ae': 174,\n 'ap': 603,\n 'death': 3491,\n 'carfest': 2180,\n 'gibyqhhkpk': 5927,\n 'dtn': 4138,\n 'brazil': 1751,\n 'exp': 4855,\n 'lq': 9383,\n 'smaeslk': 14581,\n 'wtf': 17795,\n 'believ': 1284,\n 'eye': 4902,\n 'ffylajwp': 5145,\n 'nicol': 10811,\n 'fletcher': 5295,\n 'victim': 16970,\n 'ago': 237,\n 'littl': 9179,\n 'bit': 1463,\n 'trauma': 16113,\n 'omg': 11401,\n 'xdxdprcpn': 17910,\n 'bro': 1824,\n 'jetengin': 7924,\n 'turbojet': 16262,\n 'bo': 1619,\n 'kxxnszp': 8776,\n 'nk': 10869,\n 'phone': 12025,\n 'ship': 14308,\n 'terribl': 15603,\n 'statist': 14978,\n 'cop': 2957,\n 'hous': 6952,\n 'colombia': 2795,\n 'zhjlflbhzl': 18545,\n 'iecc': 7236,\n 'jdoub': 7880,\n 'drone': 4090,\n 'worri': 17698,\n 'esp': 4719,\n 'vicin': 16965,\n 'kz': 8790,\n 'rgngjf': 13386,\n 'earli': 4277,\n 'wake': 17241,\n 'sister': 14449,\n 'beg': 1261,\n 'ride': 13428,\n 'ambul': 454,\n 'hospit': 6938,\n 'rodkiai': 13565,\n 'ay': 993,\n 'zzcupnz': 18729,\n 'twelv': 16298,\n 'fear': 5079,\n 'pakistani': 11705,\n 'helicopt': 6641,\n 'sc': 13984,\n 'dn': 3919,\n 'mc': 9770,\n 'seriou': 14180,\n 'lorri': 9333,\n 'pfeaqeski': 11972,\n 'fntg': 5354,\n 'rnkx': 13520,\n 'emsn': 4554,\n 'reuter': 13343,\n 'mdnugvubwn': 9808,\n 'yugvani': 18386,\n 'lead': 8948,\n 'servic': 14185,\n 'boss': 1683,\n 'welcom': 17403,\n 'chariti': 2405,\n 'mj': 10127,\n 'jq': 8131,\n 'psv': 12531,\n 'aberystwyth': 33,\n 'shrewsburi': 14348,\n 'incid': 7402,\n 'halt': 6441,\n 'shrew': 14347,\n 'xum': 18076,\n 'ylcb': 18281,\n 'sprinter': 14866,\n 'automat': 925,\n 'frontlin': 5557,\n 'choic': 2499,\n 'lez': 9046,\n 'compliant': 2857,\n 'ebay': 4306,\n 'evttqpeia': 4814,\n 'nanotech': 10582,\n 'devic': 3667,\n 'target': 15448,\n 'destroy': 3644,\n 'blood': 1562,\n 'clot': 2679,\n 'hfi': 6686,\n 'slbb': 14537,\n 'skyhawkmm': 14509,\n 'traplord': 16110,\n 'fredosantana': 5506,\n 'lilrees': 9135,\n 'hella': 6644,\n 'crazi': 3078,\n 'fight': 5188,\n 'coupl': 3015,\n 'mosh': 10313,\n 'pit': 12093,\n 'run': 13745,\n 'lucki': 9430,\n 'justsay': 8217,\n 'randomthought': 13009,\n 'bfe': 1361,\n 'twbzt': 16288,\n 'til': 15844,\n 'dna': 3920,\n 'xglah': 17934,\n 'zl': 18584,\n 'thmblaatzp': 15769,\n 'tanslash': 15435,\n 'fouseytub': 5442,\n 'ok': 11345,\n 'hahahah': 6419,\n 'zsberqnn': 18658,\n 'ivrzojzv': 7710,\n 'pakistan': 11704,\n 'ry': 13799,\n 'ebmf': 4311,\n 'thenissonian': 15719,\n 'rejectdcartoon': 13210,\n 'nissan': 10841,\n 'medic': 9832,\n 'assist': 812,\n 'em': 4510,\n 'ny': 11133,\n 'emt': 4555,\n 'petit': 11965,\n 'hour': 6950,\n 'minimum': 10052,\n 'wage': 17233,\n 'oa': 11170,\n 'swlxmr': 15321,\n 'paramed': 11759,\n 'fcqmkffflw': 5057,\n 'vayaymbngu': 16851,\n 'brme': 1823,\n 'sn': 14618,\n 'ujrx': 16487,\n 'kgawp': 8468,\n 'kp': 8670,\n 'lf': 9047,\n 'aut': 912,\n 'kiwi': 8559,\n 'karyn': 8334,\n 'park': 11771,\n 'lot': 9343,\n 'said': 13851,\n 'john': 8069,\n 'hpvodud': 6971,\n 'ip': 7596,\n 'lt': 9408,\n 'shzpyiqok': 14361,\n 'pwwpum': 12636,\n 'rbj': 13053,\n 'yspon': 18366,\n 'qo': 12787,\n 'leoblakecart': 9019,\n 'dog': 3949,\n 'mg': 9946,\n 'lpgr': 9379,\n 'rm': 13511,\n 'natasha': 10607,\n 'rideout': 13429,\n 'hatzolah': 6534,\n 'respond': 13311,\n 'dual': 4146,\n 'siren': 14439,\n 'sek': 14127,\n 'mq': 10360,\n 'njf': 10855,\n 'fuerk': 5609,\n 'gwui': 6372,\n 'mv': 10488,\n 'ggglmvc': 5893,\n 'yeuylt': 18227,\n 'ugrmd': 16456,\n 'ywbbeet': 18415,\n 'worldnew': 17690,\n 'qsjod': 12830,\n 'number': 11089,\n 'lesotho': 9031,\n 'bodi': 1629,\n 'aac': 5,\n 'surpris': 15256,\n 'standardis': 14950,\n 'clinic': 2662,\n 'practic': 12351,\n 'trust': 16213,\n 'tyt': 16363,\n 'xrrk': 18045,\n 'nazoi': 10634,\n 'walk': 17248,\n 'pass': 11792,\n 'hate': 6530,\n 'episod': 4655,\n 'trunk': 16212,\n 'annihil': 548,\n 'freiza': 5529,\n 'cleanest': 2639,\n 'nigga': 10819,\n 'merci': 9889,\n 'shall': 14248,\n 'petebest': 11957,\n 'dessic': 3638,\n 'laid': 8820,\n 'bare': 1121,\n 'kneel': 8609,\n 'urib': 16688,\n ...}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word frequency\n# the number here indicates the actual frequency of the word in the entire corpus of documents\nword_freq = [(word, word_freq_csr[0, indx]) for word, indx in cvect.vocabulary_.items()]","execution_count":217,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finding the most frequent words"},{"metadata":{"trusted":true},"cell_type":"code","source":"word_freq_sorted = sorted(word_freq, key = lambda x: x[1], reverse=True)\n# the 10 most frequent words\nword_freq_sorted[:10]","execution_count":218,"outputs":[{"output_type":"execute_result","execution_count":218,"data":{"text/plain":"[('http', 4721),\n ('like', 411),\n ('amp', 344),\n ('bomb', 239),\n ('new', 228),\n ('news', 213),\n ('peopl', 201),\n ('time', 183),\n ('kill', 181),\n ('burn', 180)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Using Counter to count the number of frequent words\n- Ref: https://stackoverflow.com/questions/27488446/how-do-i-get-word-frequency-in-a-corpus-using-scikit-learn-countvectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus[:10]","execution_count":219,"outputs":[{"output_type":"execute_result","execution_count":219,"data":{"text/plain":"['deed reason earthquak may allah forgiv us',\n 'forest fire near la rong sask canada',\n 'resid ask shelter place notifi offic evacu shelter place order expect',\n 'peopl receiv wildfir evacu order california',\n 'got sent photo rubi alaska smoke wildfir pour school',\n 'rockyfir updat california hwi close direct due lake counti fire cafir wildfir',\n 'flood disast heavi rain caus flash flood street manit colorado spring area',\n 'top hill see fire wood',\n 'emerg evacu happen build across street',\n 'afraid tornado come area']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Split the words in seach sentence and add them to a list, so that we can apply the Counter()"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Sample\n\ntemp=[]\nfor sent in corpus[:10]:\n    [temp.append(w) for w in sent.split()]\n    \nprint(temp)","execution_count":220,"outputs":[{"output_type":"stream","text":"['deed', 'reason', 'earthquak', 'may', 'allah', 'forgiv', 'us', 'forest', 'fire', 'near', 'la', 'rong', 'sask', 'canada', 'resid', 'ask', 'shelter', 'place', 'notifi', 'offic', 'evacu', 'shelter', 'place', 'order', 'expect', 'peopl', 'receiv', 'wildfir', 'evacu', 'order', 'california', 'got', 'sent', 'photo', 'rubi', 'alaska', 'smoke', 'wildfir', 'pour', 'school', 'rockyfir', 'updat', 'california', 'hwi', 'close', 'direct', 'due', 'lake', 'counti', 'fire', 'cafir', 'wildfir', 'flood', 'disast', 'heavi', 'rain', 'caus', 'flash', 'flood', 'street', 'manit', 'colorado', 'spring', 'area', 'top', 'hill', 'see', 'fire', 'wood', 'emerg', 'evacu', 'happen', 'build', 'across', 'street', 'afraid', 'tornado', 'come', 'area']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for the entire train corpus\ntemp_counter= []\n\nfor sent in corpus:\n    [temp_counter.append(w) for w in sent.split()]\n    \n#print(temp_counter)\nprint('Number of words in the corpus is :', len(temp_counter))","execution_count":221,"outputs":[{"output_type":"stream","text":"Number of words in the corpus is : 88103\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Using the Counter function"},{"metadata":{},"cell_type":"markdown","source":"### Using the Counter function: Method 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"# On a sample\n\nfrom collections import Counter\nCounter(temp).most_common(10)","execution_count":222,"outputs":[{"output_type":"execute_result","execution_count":222,"data":{"text/plain":"[('fire', 3),\n ('evacu', 3),\n ('wildfir', 3),\n ('shelter', 2),\n ('place', 2),\n ('order', 2),\n ('california', 2),\n ('flood', 2),\n ('street', 2),\n ('area', 2)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# on the complete train\n# temp_counter has the complete list\n\nCounter(temp_counter).most_common(10)","execution_count":223,"outputs":[{"output_type":"execute_result","execution_count":223,"data":{"text/plain":"[('co', 4746),\n ('http', 4721),\n ('like', 411),\n ('fire', 363),\n ('amp', 344),\n ('get', 311),\n ('bomb', 239),\n ('new', 228),\n ('via', 220),\n ('u', 216)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Using the Counter function: Method 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"# On a sample\n\ncounter = Counter()\ncounter.update(temp)\ncounter.most_common(10)","execution_count":224,"outputs":[{"output_type":"execute_result","execution_count":224,"data":{"text/plain":"[('fire', 3),\n ('evacu', 3),\n ('wildfir', 3),\n ('shelter', 2),\n ('place', 2),\n ('order', 2),\n ('california', 2),\n ('flood', 2),\n ('street', 2),\n ('area', 2)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# on the complete train\n# temp_counter has the complete list\n\ncounter.update(temp_counter)\ncounter.most_common(10)","execution_count":225,"outputs":[{"output_type":"execute_result","execution_count":225,"data":{"text/plain":"[('co', 4746),\n ('http', 4721),\n ('like', 411),\n ('fire', 366),\n ('amp', 344),\n ('get', 311),\n ('bomb', 239),\n ('new', 228),\n ('via', 220),\n ('u', 216)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Word frequency using custom code or function"},{"metadata":{"trusted":true},"cell_type":"code","source":"WordFrequency=dict()\nfor sent in corpus:\n    for word in sent.split():\n        if word in WordFrequency.keys():\n            WordFrequency[word]+=1\n        else:\n            WordFrequency[word]=1\n            \n\n# convert dictionary to DataFrame\nWordFrequency_df = pd.DataFrame(WordFrequency.items(), columns=['word', 'frequency'])\nWordFrequency_df.sort_values('frequency', ascending=False).head(10)","execution_count":226,"outputs":[{"output_type":"execute_result","execution_count":226,"data":{"text/plain":"      word  frequency\n123     co       4746\n122   http       4721\n115   like        411\n8     fire        363\n341    amp        344\n74     get        311\n2089  bomb        239\n155    new        228\n784    via        220\n256      u        216","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>123</th>\n      <td>co</td>\n      <td>4746</td>\n    </tr>\n    <tr>\n      <th>122</th>\n      <td>http</td>\n      <td>4721</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>like</td>\n      <td>411</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>fire</td>\n      <td>363</td>\n    </tr>\n    <tr>\n      <th>341</th>\n      <td>amp</td>\n      <td>344</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>get</td>\n      <td>311</td>\n    </tr>\n    <tr>\n      <th>2089</th>\n      <td>bomb</td>\n      <td>239</td>\n    </tr>\n    <tr>\n      <th>155</th>\n      <td>new</td>\n      <td>228</td>\n    </tr>\n    <tr>\n      <th>784</th>\n      <td>via</td>\n      <td>220</td>\n    </tr>\n    <tr>\n      <th>256</th>\n      <td>u</td>\n      <td>216</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.1 From the list of words, consider removing some words or features to reduce dimensions:\n    - There are some words like 'co' and 'http' which doesn not add any valuable information\n    - There are also other words like 'via', 'u, which could also have been removed in the intial regex compilaton for selecting words that have a minimal length of 3\n    - Also we should consider removing words that are very rare as they do not add any value"},{"metadata":{"trusted":true},"cell_type":"code","source":"WordFrequency_df.sort_values('frequency', ascending=True)","execution_count":227,"outputs":[{"output_type":"execute_result","execution_count":227,"data":{"text/plain":"             word  frequency\n18888        rskq          1\n8913   gbvdnczjou          1\n8914         hise          1\n8915           dy          1\n8916      ersdcrh          1\n...           ...        ...\n341           amp        344\n8            fire        363\n115          like        411\n122          http       4721\n123            co       4746\n\n[18889 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>18888</th>\n      <td>rskq</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8913</th>\n      <td>gbvdnczjou</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8914</th>\n      <td>hise</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8915</th>\n      <td>dy</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8916</th>\n      <td>ersdcrh</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>341</th>\n      <td>amp</td>\n      <td>344</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>fire</td>\n      <td>363</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>like</td>\n      <td>411</td>\n    </tr>\n    <tr>\n      <th>122</th>\n      <td>http</td>\n      <td>4721</td>\n    </tr>\n    <tr>\n      <th>123</th>\n      <td>co</td>\n      <td>4746</td>\n    </tr>\n  </tbody>\n</table>\n<p>18889 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.2 ** from the histogram plot, there are more than 13K words which have a frequency of 1 **"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(WordFrequency_df['frequency'], range=(0,10))","execution_count":228,"outputs":[{"output_type":"execute_result","execution_count":228,"data":{"text/plain":"(array([    0., 13094.,  1754.,   835.,   550.,   419.,   291.,   212.,\n          186.,   222.]),\n array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]),\n <a list of 10 Patch objects>)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"?plt.hist","execution_count":229,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ** More than 69% of the words in the entire train corpus have a word frequency of only 1 **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# verifying the results\ndisplay(WordFrequency_df['frequency'].value_counts()[:10])\n\n# to see the results in percentages\ndisplay(WordFrequency_df['frequency'].value_counts(normalize=True)[:10])","execution_count":230,"outputs":[{"output_type":"display_data","data":{"text/plain":"1     13094\n2      1754\n3       835\n4       550\n5       419\n6       291\n7       212\n8       186\n9       138\n11       86\nName: frequency, dtype: int64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1     0.693208\n2     0.092858\n3     0.044206\n4     0.029117\n5     0.022182\n6     0.015406\n7     0.011223\n8     0.009847\n9     0.007306\n11    0.004553\nName: frequency, dtype: float64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"WordFrequency_df[WordFrequency_df['frequency'] == 1]['word'][:100]","execution_count":231,"outputs":[{"output_type":"execute_result","execution_count":231,"data":{"text/plain":"11          rong\n12          sask\n18        notifi\n51         manit\n80          fvck\n         ...    \n445          mir\n446      congest\n447       pastor\n454        spilt\n455    mayonnais\nName: word, Length: 100, dtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import words as nltk_words","execution_count":232,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk_words.words()[:10]","execution_count":233,"outputs":[{"output_type":"execute_result","execution_count":233,"data":{"text/plain":"['A',\n 'a',\n 'aa',\n 'aal',\n 'aalii',\n 'aam',\n 'Aani',\n 'aardvark',\n 'aardwolf',\n 'Aaron']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Check if words with frequency: 1 are valid English words as per English dictionary and if they can be found from the NLTK corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"# start = time.time()\n# freq1 = WordFrequency_df[WordFrequency_df['frequency'] == 1]['word'].values.tolist()\n# valid_words_freq1 = [w for w in freq1 if w in nltk_words.words()]\n# print('time taken to find valid words with a frequency of 1 is', time.time() - start)\n# print('Number of valid words with a frequency of 1 is', len(valid_words_freq1))","execution_count":234,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# valid_words_freq1","execution_count":235,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the contents of this list for later use\n# since the search is extensive from the previous cell: output as below\n# time taken to find valid words with a frequency of 1 is 1677.8927783966064 seconds\n# Number of valid words with a frequency of 1 is 1623\n\nimport pickle\n# pickle.dump(valid_words_freq1, open('valid_words_freq1.pkl', 'wb'))\n# /kaggle/input/nlp-getting-started/train.csv","execution_count":236,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# valid_words_freq1 = pickle.load(open('valid_words_freq1.pkl', 'wb'))\n# valid_words_freq1","execution_count":237,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.3  For the below code, I will consider only those words that have frequency of > 20"},{"metadata":{"trusted":true},"cell_type":"code","source":"WordFrequency_df20 = WordFrequency_df[WordFrequency_df['frequency'] >= 20]\nprint(WordFrequency_df20.shape)","execution_count":238,"outputs":[{"output_type":"stream","text":"(787, 2)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"WordFrequency_df20.sort_values('frequency', inplace=True, ascending=False)\nWordFrequency_df20.set_index('word', inplace=True)\nWordFrequency_df20","execution_count":239,"outputs":[{"output_type":"execute_result","execution_count":239,"data":{"text/plain":"       frequency\nword            \nco          4746\nhttp        4721\nlike         411\nfire         363\namp          344\n...          ...\nbeach         20\narriv         20\npilot         20\nvs            20\ncarri         20\n\n[787 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>frequency</th>\n    </tr>\n    <tr>\n      <th>word</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>co</th>\n      <td>4746</td>\n    </tr>\n    <tr>\n      <th>http</th>\n      <td>4721</td>\n    </tr>\n    <tr>\n      <th>like</th>\n      <td>411</td>\n    </tr>\n    <tr>\n      <th>fire</th>\n      <td>363</td>\n    </tr>\n    <tr>\n      <th>amp</th>\n      <td>344</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>beach</th>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>arriv</th>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>pilot</th>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>vs</th>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>carri</th>\n      <td>20</td>\n    </tr>\n  </tbody>\n</table>\n<p>787 rows Ã— 1 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## 3. Create a Bag of words, using a sparse matrix\n- Use a CountVectorizer, with max_features = number of unique words"},{"metadata":{"trusted":true},"cell_type":"code","source":"countVec = CountVectorizer(max_features=WordFrequency_df20.shape[0])\nstart = time.time()\ncountVec_fit = countVec.fit_transform(corpus)\nprint('time taken:', time.time() - start)","execution_count":240,"outputs":[{"output_type":"stream","text":"time taken: 0.2305307388305664\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"countVec_fit","execution_count":241,"outputs":[{"output_type":"execute_result","execution_count":241,"data":{"text/plain":"<7613x787 sparse matrix of type '<class 'numpy.int64'>'\n\twith 45346 stored elements in Compressed Sparse Row format>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### To view the contents of the CSR matrix, use either:\n - ** toarray() method **\n - ** todense() method **"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(countVec_fit.toarray()), display(countVec_fit.todense())\nbagOfwords = countVec_fit.toarray()","execution_count":242,"outputs":[{"output_type":"display_data","data":{"text/plain":"array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [1, 0, 0, ..., 0, 0, 0]])"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"matrix([[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [1, 0, 0, ..., 0, 0, 0]])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## 4. Machine Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = bagOfwords\ny = data_train['target']","execution_count":243,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score","execution_count":244,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Decision Tree Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n#dt = DecisionTreeClassifier(max_depth= 10, min_samples_split=10)\nstart = time.time()\ndt = DecisionTreeClassifier()\ndt_f1_scores = cross_val_score(dt, X, y, cv=5, scoring='f1')\ndt_roc_auc_scores = cross_val_score(dt, X, y, cv=5, scoring='roc_auc')\nprint('Time take for DecisionTreeClassifier is: ', time.time() - start)\nprint('Mean f1 score for DecisionTreeClassifier is: ', dt_f1_scores.mean())\nprint('Mean roc_auc_score for DecisionTreeClassifier is: ', dt_roc_auc_scores.mean())","execution_count":111,"outputs":[{"output_type":"stream","text":"Time take for DecisionTreeClassifier is:  30.284107446670532\nMean f1 score for DecisionTreeClassifier is:  0.5403934194518951\nMean roc_auc_score for DecisionTreeClassifier is:  0.5947832421194901\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the model\npickle.dump(dt, open('decisiontreemodel.pkl', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to view the parameters\n?DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Gradient Boosting model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier()\nstart = time.time()\ngbc_f1_scores = cross_val_score(gbc, X, y, cv=5, scoring='f1')\ngbc_roc_auc_scores = cross_val_score(gbc, X, y, cv=5, scoring='roc_auc')\nprint('Time take for GradientBoostingClassifier is: ', time.time() - start)\nprint('Mean f1 score for GradientBoostingClassifier is: ', gbc_f1_scores.mean())\nprint('Mean roc_auc_score for GradientBoostingClassifier is: ', gbc_roc_auc_scores.mean())","execution_count":112,"outputs":[{"output_type":"stream","text":"Time take for GradientBoostingClassifier is:  182.14430499076843\nMean f1 score for GradientBoostingClassifier is:  0.4410258027518254\nMean roc_auc_score for GradientBoostingClassifier is:  0.6787460850812115\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the model\npickle.dump(gbc, open('gradientboostingmodel.pkl', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"?GradientBoostingClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 K-Nearest Neighbors Classifier(KNN)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nstart = time.time()\nknn_f1_scores = cross_val_score(knn, X, y, cv=5, scoring='f1')\nknn_roc_auc_scores = cross_val_score(knn, X, y, cv=5, scoring='roc_auc')\nprint('Time take for KNeighborsClassifier is: ', time.time() - start)\nprint('Mean f1 score for KNeighborsClassifier is: ', knn_f1_scores.mean())\nprint('Mean roc_auc_score for KNeighborsClassifier is: ', knn_roc_auc_scores.mean())","execution_count":113,"outputs":[{"output_type":"stream","text":"Time take for KNeighborsClassifier is:  146.09851336479187\nMean f1 score for KNeighborsClassifier is:  0.2833056064734908\nMean roc_auc_score for KNeighborsClassifier is:  0.6198815537679512\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the model\npickle.dump(knn, open('knnclassifiermodel.pkl', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"?KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4 Logisitic Regression Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(random_state=1)\nstart = time.time()\nlogreg_f1_scores = cross_val_score(logreg, X, y, cv=5, scoring='f1')\nlogreg_roc_auc_scores = cross_val_score(logreg, X, y, cv=5, scoring='roc_auc')\nprint('Time take for LogisticRegression is: ', time.time() - start)\nprint('Mean f1 score for LogisticRegression is: ', logreg_f1_scores.mean())\nprint('Mean roc_auc_score for LogisticRegression is: ', logreg_roc_auc_scores.mean())","execution_count":114,"outputs":[{"output_type":"stream","text":"Time take for LogisticRegression is:  4.648926019668579\nMean f1 score for LogisticRegression is:  0.5923094187085449\nMean roc_auc_score for LogisticRegression is:  0.7152582169451955\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the model\npickle.dump(knn, open('logregressionclassifiermodel.pkl', 'wb'))","execution_count":245,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"?LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.5 Stochastic Gradient Descent Classifier (SGD Classifier)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nsgdc = SGDClassifier()\nstart = time.time()\nsgdc_f1_scores = cross_val_score(sgdc, X, y, cv=5, scoring='f1')\nsgdc_roc_auc_scores = cross_val_score(sgdc, X, y, cv=5, scoring='roc_auc')\nprint('Time take for SGDClassifier is: ', time.time() - start)\nprint('Mean f1 score for SGDClassifier is: ', sgdc_f1_scores.mean())\nprint('Mean roc_auc_score for SGDClassifier is: ', sgdc_roc_auc_scores.mean())","execution_count":115,"outputs":[{"output_type":"stream","text":"Time take for SGDClassifier is:  9.551866292953491\nMean f1 score for SGDClassifier is:  0.5724026156539803\nMean roc_auc_score for SGDClassifier is:  0.6958211860112506\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the model\npickle.dump(sgdc, open('SGDClassifier.pkl', 'wb'))","execution_count":246,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"?SGDClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.6 Support Vector Machine Classifier (SVM Classifier)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc = SVC()\nstart = time.time()\nsvc_f1_scores = cross_val_score(svc, X, y, cv=5, scoring='f1')\nsvc_roc_auc_scores = cross_val_score(svc, X, y, cv=5, scoring='roc_auc')\nprint('Time take for Support Vector Classifier is: ', time.time() - start)\nprint('Mean f1 score for Support Vector Classifier is: ', svc_f1_scores.mean())\nprint('Mean roc_auc_score for Support Vector Classifier is: ', svc_roc_auc_scores.mean())","execution_count":116,"outputs":[{"output_type":"stream","text":"Time take for Support Vector Classifier is:  444.10755586624146\nMean f1 score for Support Vector Classifier is:  0.6031671387662147\nMean roc_auc_score for Support Vector Classifier is:  0.7393644657231447\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the model\npickle.dump(svc, open('SVCClassifier.pkl', 'wb'))","execution_count":247,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"?SVC","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.6 Bernoulli Naive Bayes Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB\nbernoulliNB = BernoulliNB()\nstart = time.time()\nbernoulliNB_f1_scores = cross_val_score(bernoulliNB, X, y, cv=5, scoring='f1')\nbernoulliNB_roc_auc_scores = cross_val_score(bernoulliNB, X, y, cv=5, scoring='roc_auc')\nprint('Time take for BernoulliNB Classifier is: ', time.time() - start)\nprint('Mean f1 score for BernoulliNB Classifier is: ', bernoulliNB_f1_scores.mean())\nprint('Mean roc_auc_score for BernoulliNB Classifier is: ', bernoulliNB_roc_auc_scores.mean())","execution_count":117,"outputs":[{"output_type":"stream","text":"Time take for BernoulliNB Classifier is:  1.8536269664764404\nMean f1 score for BernoulliNB Classifier is:  0.6512203292253967\nMean roc_auc_score for BernoulliNB Classifier is:  0.7547409127894995\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the model\npickle.dump(bernoulliNB, open('bernoulliNB.pkl', 'wb'))","execution_count":248,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.7 Multinomial Naive Bayes Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nmultinomialNB = MultinomialNB()\nstart = time.time()\nmultinomialNB_f1_scores = cross_val_score(multinomialNB, X, y, cv=5, scoring='f1')\nmultinomialNB_roc_auc_scores = cross_val_score(gaussianNB, X, y, cv=5, scoring='roc_auc')\nprint('Time take for MultinomialNB Classifier is: ', time.time() - start)\nprint('Mean f1 score for MultinomialNB Classifier is: ', multinomialNB_f1_scores.mean())\nprint('Mean roc_auc_score for MultinomialNB Classifier is: ', multinomialNB_roc_auc_scores.mean())","execution_count":118,"outputs":[{"output_type":"stream","text":"Time take for MultinomialNB Classifier is:  1.4072155952453613\nMean f1 score for MultinomialNB Classifier is:  0.633068602970552\nMean roc_auc_score for MultinomialNB Classifier is:  0.6276010150798197\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### 4.8 Gaussian Naive Bayes Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\ngaussianNB = GaussianNB()\nstart = time.time()\ngaussianNB_f1_scores = cross_val_score(gaussianNB, X, y, cv=5, scoring='f1')\ngaussianNB_roc_auc_scores = cross_val_score(gaussianNB, X, y, cv=5, scoring='roc_auc')\nprint('Time take for GaussianNB Classifier is: ', time.time() - start)\nprint('Mean f1 score for GaussianNB Classifier is: ', gaussianNB_f1_scores.mean())\nprint('Mean roc_auc_score for GaussianNB Classifier is: ', gaussianNB_roc_auc_scores.mean())","execution_count":119,"outputs":[{"output_type":"stream","text":"Time take for GaussianNB Classifier is:  1.232212781906128\nMean f1 score for GaussianNB Classifier is:  0.5834560748202027\nMean roc_auc_score for GaussianNB Classifier is:  0.6276010150798197\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### 4.9 Voting Classifier\n- A Voting Classifier is a machine learning model that trains on an ensemble of numerous models and predicts an output (class) based on their highest probability of chosen class as the output\n- It simply aggregates the findings of each classifier passed into Voting Classifier and predicts the output class based on the highest majority of voting\n- The idea is instead of creating separate dedicated models and finding the accuracy for each them, we create a single model which trains by these models and predicts output based on their combined majority of voting for each output class\n\n- Voting Classifier supports two types of votings:\n    - Hard voting: the predicted output class is a class with the highest majority of votes\n    - Soft voting: the prediction is based on the average of probability given to that class\n- Ref: https://www.geeksforgeeks.org/ml-voting-classifier-using-sklearn/"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nsvc = SVC(kernel='linear', probability=True)\n\nestimators = [('LogisiticRegression', logreg),\n              ('Support Vector Classifier', svc),\n              ('Bernoulli NB', bernoulliNB),\n              ('Gradient Boosting Classifier', gbc) \n             ]\n\nvotingclassifier = VotingClassifier(voting='soft', estimators = estimators)\nstart = time.time()\nvotingclassifier_f1_scores = cross_val_score(votingclassifier, X, y, cv=5, scoring='f1')\nvotingclassifier_roc_auc_scores = cross_val_score(votingclassifier, X, y, cv=5, scoring='roc_auc')\nprint('Time take for VotingClassifier is: ', time.time() - start)\nprint('Mean f1 score for VotingClassifier is: ', votingclassifier_f1_scores.mean())\nprint('Mean roc_auc_score for VotingClassifier is: ', votingclassifier_roc_auc_scores.mean())","execution_count":108,"outputs":[{"output_type":"stream","text":"Time take for VotingClassifier is:  1590.8315262794495\nMean f1 score for VotingClassifier is:  0.6224697722916686\nMean roc_auc_score for VotingClassifier is:  0.7366762456757117\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"* For the SGDClassifier() Error: Cannot use loss ='hinge'with SGDClassifer,hence I have removed it. \n* Using loss=\"log\" or loss=\"modified_huber\" enables the predict_proba method, which gives a vector of probability estimates per sample\n* Ref: https://scikit-learn.org/stable/modules/sgd.html\n\n### Adding SGDClassifier to the Voting Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(kernel='linear', probability=True) # setting \"probability=True\" enables the predict_proba method which is needed when using cross_validation\nsgdc = SGDClassifier(loss ='log') # this enables the predict_proba method\n\nestimators = [('LogisiticRegression', logreg),\n              ('Support Vector Classifier', svc),\n              ('Stochastic Gradient Classifier',sgdc),\n              ('Bernoulli NB', bernoulliNB),\n              ('Gradient Boosting Classifier', gbc)\n             ]\n\nvotingclassifier = VotingClassifier(voting='soft', estimators = estimators)\nstart = time.time()\nvotingclassifier_f1_scores = cross_val_score(votingclassifier, X, y, cv=5, scoring='f1')\nvotingclassifier_roc_auc_scores = cross_val_score(votingclassifier, X, y, cv=5, scoring='roc_auc')\nprint('Time take for VotingClassifier is: ', time.time() - start)\nprint('Mean f1 score for VotingClassifier is: ', votingclassifier_f1_scores.mean())\nprint('Mean roc_auc_score for VotingClassifier is: ', votingclassifier_roc_auc_scores.mean())","execution_count":109,"outputs":[{"output_type":"stream","text":"Time take for VotingClassifier is:  1686.5071635246277\nMean f1 score for VotingClassifier is:  0.6097540120641219\nMean roc_auc_score for VotingClassifier is:  0.731559031928656\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\npickle.dump(votingclassifier, open('votingclassifier_crossval.pkl', 'wb'))","execution_count":110,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to a get a quick idea of the score using train, test splits\nfrom sklearn.metrics import f1_score, roc_auc_score\nvotingclassifier.fit(X_train, y_train)\nprediction = votingclassifier.predict(X_test)\nprint('f1 score: ', f1_score(y_test, prediction))\nprint('ROC-AUC score: ', roc_auc_score(y_test, prediction))","execution_count":107,"outputs":[{"output_type":"stream","text":"f1 score:  0.7362171331636981\nROC-AUC score:  0.7795766252418804\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### 4.10 Light GBM Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgbm\ndtrain = lgbm.Dataset(X_train, label=y_train)\ndtest = lgbm.Dataset(X_test)\nnum_rounds=50\nparams = {'learning_rate': 0.001,\n          'objective': 'binary',\n          'metric': 'binary_logloss',\n          'max_depth': 30,\n          'num_leaves': 50,\n          'boosting_type': 'gbdt'\n         }","execution_count":133,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start= time.time()\nlgbm_model = lgbm.train(params, dtrain, num_boost_round=100)\nprint('time taken for the LGB Classifier model is: ', time.time() - start)\nprediction = lgbm_model.predict(X_test)\nprediction\n# print('f1 score: ', f1_score(y_test, prediction))\n# print('ROC-AUC score: ', roc_auc_score(y_test, prediction))","execution_count":173,"outputs":[{"output_type":"stream","text":"time taken for the LGB Classifier model is:  1.1758060455322266\n","name":"stdout"},{"output_type":"execute_result","execution_count":173,"data":{"text/plain":"array([0.42704108, 0.41008092, 0.41008092, ..., 0.41008092, 0.42704108,\n       0.41008092])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since the output of the prediction is an array of probabilities, I will convert that it to binary using threshold of 0.43\n# this threshold gives me the highest score\n\ny_pred =[]\nfor i in range(len(prediction)):\n    if prediction[i] >= 0.43:\n        prediction[i] = 1\n    else:\n        prediction[i] =0\n        \nprint('f1 score: ', f1_score(y_test, prediction))\nprint('ROC-AUC score: ', roc_auc_score(y_test, prediction))","execution_count":172,"outputs":[{"output_type":"stream","text":"f1 score:  0.6024785510009533\nROC-AUC score:  0.6943356645830459\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Using cross-validation with LightGBM\n* for cross validation I have to use the LightGBMClassifier as this has the fit method"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nLightGBMclf = LGBMClassifier()\nstart = time.time()\nLightGBMClassifier_f1_scores = cross_val_score(LightGBMclf, X, y, cv=5, scoring='f1')\nLightGBMClassifier_roc_auc_scores = cross_val_score(LightGBMclf, X, y, cv=5, scoring='roc_auc')\nprint('Time take for LightGBMClassifier is: ', time.time() - start)\nprint('Mean f1 score for LightGBMClassifier is: ', LightGBMClassifier_f1_scores.mean())\nprint('Mean roc_auc_score for LightGBMClassifier is: ', LightGBMClassifier_roc_auc_scores.mean())","execution_count":175,"outputs":[{"output_type":"stream","text":"Time take for LightGBMClassifier is:  4.462429523468018\nMean f1 score for LightGBMClassifier is:  0.5335340764194282\nMean roc_auc_score for LightGBMClassifier is:  0.6993626753534776\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"?LGBMClassifier","execution_count":176,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.10 XGBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nxgboostclf = XGBClassifier()\nstart = time.time()\nXGBClassifier_f1_scores = cross_val_score(xgboostclf, X, y, cv=5, scoring='f1')\nXGBClassifier_roc_auc_scores = cross_val_score(xgboostclf, X, y, cv=5, scoring='roc_auc')\nprint('Time take for XGBClassifier is: ', time.time() - start)\nprint('Mean f1 score for XGBClassifier is: ', XGBClassifier_f1_scores.mean())\nprint('Mean roc_auc_score for XGBClassifier is: ', XGBClassifier_roc_auc_scores.mean())","execution_count":178,"outputs":[{"output_type":"stream","text":"Time take for XGBClassifier is:  85.08437299728394\nMean f1 score for XGBClassifier is:  0.5126469098256271\nMean roc_auc_score for XGBClassifier is:  0.6797554298640806\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the model\npickle.dump(xgboostclf, open('XGBClassifier.pkl', 'wb'))","execution_count":249,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbscores = {'f1': XGBClassifier_f1_scores.mean(), 'roc-auc': XGBClassifier_roc_auc_scores.mean()}\nlgbmscores = {'f1': LightGBMClassifier_f1_scores.mean(), 'roc-auc': LightGBMClassifier_roc_auc_scores.mean()}\nmultinomialscores = {'f1': multinomialNB_f1_scores.mean(), 'roc-auc': multinomialNB_roc_auc_scores.mean()}\nbernoulliscores = {'f1': bernoulliNB_f1_scores.mean(), 'roc-auc': bernoulliNB_roc_auc_scores.mean()}\nsvcscores = {'f1': svc_f1_scores.mean(), 'roc-auc': svc_roc_auc_scores.mean()}\nsgdcscores = {'f1': sgdc_f1_scores.mean(), 'roc-auc': sgdc_roc_auc_scores.mean()}\nlogregscores = {'f1': logreg_f1_scores.mean(), 'roc-auc': logreg_roc_auc_scores.mean()}\ngbcscores = {'f1': gbc_f1_scores.mean(), 'roc-auc': gbc_roc_auc_scores.mean()}\ndtscores = {'f1': dt_f1_scores.mean(), 'roc-auc': dt_roc_auc_scores.mean()}\n\nmodel_scores_list = [xgbscores,lgbmscores, multinomialscores, bernoulliscores, svcscores, sgdcscores, logregscores, gbcscores, dtscores]\nmodel_name =['xgboost', 'LightGBM', 'MultionomialNB', 'BernoulliNB', 'SVC', 'SGD', 'LogRegression', 'GradientBoosting', 'DecisionTree']\nsummary_df = pd.DataFrame(model_scores_list, index=model_name)\nsummary_df\nsummary_df.sort_values('roc-auc', ascending=False).to_csv('model_score.csv', index=True, header=True)","execution_count":251,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Summary\n* Bernoulli NB Algorithm has given the highest 'ROC-AUC' score and also the 'f1' score\n* From the base models with no hyperparamter tuning this is the best choice\n\n- Repeat the preprocessing steps for the given test data and submit to the model for prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}