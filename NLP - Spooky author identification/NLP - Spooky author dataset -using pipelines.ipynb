{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!unzip '/kaggle/input/spooky-author-identification/train.zip'\n!unzip '/kaggle/input/spooky-author-identification/test.zip'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk(os.getcwd()):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nimport gc\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Advantages of using pipelines:\n1. train and test set tranformations are taken care automatically\n2. hyperparameter tuning is made easier. Set new parameters on any estimator in the pipeline and refit - all this is one line. Or use GridSearch on pipeline\n3. Model description is easier"},{"metadata":{},"cell_type":"markdown","source":"## 1. Data preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data\ntrain = pd.read_csv('/kaggle/working/train.csv')\ntest = pd.read_csv('/kaggle/working/test.csv')\n\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode labels: 'author'\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train['author'])\nprint('Classes encoded by Label encode', le.classes_)\n\ntrain['author'] = le.transform(train['author'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Split the train into train and validation sets__"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection, metrics\n\nX = train['text']\ny = train['author']\n\n# use stratify to have equal proportion of author classes in the train and validation splits\nX_train, X_val, y_train, y_val =  model_selection.train_test_split(X, y, test_size=0.3, shuffle=True, stratify = train['author'], random_state= 2020)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### __Create the first pipeline__\n\nTo understand the difference between `TfidfVectorizer` and `TfidfTransformer` refer [here](https://stackoverflow.com/questions/54745482/what-is-the-difference-between-tfidf-vectorizer-and-tfidf-transformer)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\nfrom sklearn.linear_model import LogisticRegression\n\npipe1 = Pipeline([\n                ('cv', CountVectorizer()),\n                ('tfidf_transformer', TfidfTransformer()),\n                ('logit', LogisticRegression())              \n            ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting the model\nSince our `pipe1` object has all the properties of an estimator, we can treat it as one. Hence we can use the `fit()`, `transform` and `fit_transform` method if it is defined for the `class`. For `Logistic Regression`, which is the final model in the pipeline, we will use only the fit method"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe1.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Steps of the pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe1.steps","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accessing each step's parameters by name, methods and properties\n* `pipeline.named_steps`: returns a dictionary with the keys as model names"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Coefficient of the Logistic Regression model\npipe1.named_steps['logit'].coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Making predictions:**\n- Use predict()\n- Use predict_proba()"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_val = pipe1.predict_proba(X_val)\nprint('Log loss on pipeline using cv-tfidftransformer-logit is : ', metrics.log_loss(y_val, pred_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Parameter tuning in the pipeline\n* We can get the parameters in each step by it's name"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe1.named_steps['logit'].get_params()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Getting the parameters all at once for all the steps in the pipeline\n* paramters are prepended with the step name for easier identification"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe1.get_params()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Set the parameters within the pipeline\n* set_params()"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# set params and train\npipe1.set_params(cv__max_df = 0.8, cv__min_df = 6, cv__ngram_range=(1,3)).fit(X_train, y_train)\npred_val = pipe1.predict_proba(X_val)\nprint(metrics.log_loss(y_val, pred_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Use a Naive Bayes model in the pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n\npipe2 = Pipeline([\n                    ('cv', CountVectorizer()),\n                    ('tfidf_transformer', TfidfTransformer()),\n                    ('bern_nb', BernoulliNB())\n                ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe2.fit(X_train, y_train)\nprint(metrics.log_loss(y_val, pipe2.predict_proba(X_val)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Union\n* Helps to combine new features created as part of EDA\n\n- Now we will collect some meta features from the text like punctuation_count, parts of speech tag count"},{"metadata":{},"cell_type":"markdown","source":"### NLTK - POS(parts of speech) tagger"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nsample = train['text'].sample(1).values.tolist()[0]\nprint('text: \\n', sample)\ntokens = nltk.word_tokenize(sample)\npos_tag = nltk.pos_tag(tokens)\n\nprint(pos_tag)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df = train.sample(frac=0.001)\nsample_df['text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nCounter(tag for word, tag in pos_tag)\n\n\ndef cnt_pos(sentence):\n    return Counter(tag for word, tag in nltk.pos_tag(nltk.word_tokenize(sentence)))\n\nsample_df['text'].apply(cnt_pos).apply(pd.Series).fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__More on the tags and what they represent__"},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.help.upenn_tagset()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 'Tag' text in the pipeline\nInorder to allow for 'tag' of text within the pipeline, we need to create a `transformer` or `estimator class`, which will `inherit` some `base classes` and `overload` a few functions that will need to be used"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import base classes\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom collections import Counter\n\n# define a new class, inherit from the base classes\nclass posTagMatrix(BaseEstimator, TransformerMixin):\n    \n    # use a custom tokenizer if not passed\n    # normalize = True, divide by the total number of tags in the sentence\n    \n    def __init__(self, tokenizer = lambda x: x.split(), normalize = True):\n        self.tokenizer = tokenizer\n        self.normalize = normalize\n        \n    # helper function to tokenizer and count pos tags\n    def count_pos(self, sentence):\n        return Counter(tag for word, tag in nltk.pos_tag(self.tokenizer(sentence)))\n    \n    \n    # this doesn't do anything and this makes it possible to use the fit() method\n    def fit(self, X, y=None):\n        return self\n    \n    \n    # all the transformation is done here\n    def transform(self, X):\n        # the fit method needs to be applied to a series on text\n        # this returns the count for individual tags in each sentence\n        X_tagged = X.apply(self.count_pos).apply(pd.Series).fillna(0)\n        # sum the count of all tags in the sentence\n        X_tagged['total_pos_tags'] = X_tagged.apply(sum, axis=1)\n        if self.normalize:\n            X_tagged = X_tagged.divide(X_tagged['total_pos_tags'], axis=0)\n            \n        return X_tagged","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### New pipeline with the pos tags included"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import FeatureUnion\n\npipe3 = Pipeline([\n                    ('u1', FeatureUnion([\n                                            ('tfidf_features', Pipeline([\n                                                                        ('cv', CountVectorizer()),\n                                                                        ('tfidf_transformer', TfidfTransformer())\n                                                                        ])),\n                                            ('pos_features', Pipeline([\n                                                                        ('pos', posTagMatrix(tokenizer= nltk.word_tokenize))\n                                                                    ]))\n                                        ])),\n                \n                    ('logit', LogisticRegression())\n      \n                ])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npipe3.fit(X_train, y_train)\npred_val = pipe3.predict_proba(X_val)\nprint(metrics.log_loss(y_val, pred_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Overload the 'CountVectorizer' class\n* this way the transformations can be applied to both 'train' and 'test' sets together\n* refer this [link](https://stackoverflow.com/questions/400739/what-does-asterisk-mean-in-python) on what `*args` and `**kwargs` means. Basically they are used to pack extra arguments to a function\n\n    * def f1(*a): 'a' will be a tuple of extra parameters\n    * def f2(**a): 'a' will be a dictionary of extra parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# inherit from the base class: CountVectorizer\nclass CountVectorizerplus(CountVectorizer):\n    \n    def __init__(self, *args, add_test=None, **kwargs):\n        self.add_test = add_test\n        # initialize using the super class or parent class\n        super().__init__(*args, **kwargs)   \n    \n    \n    def transform(self, X):\n        # use the transform method from the super class or parent class\n        U = super().transform(X)\n        return U\n    \n    def fit_transform(self, X , y=None):\n        if self.add_test is not None:\n            # add the test along with train\n            X_new = pd.concat([X, self.add_test])\n        else:\n            X_new = X\n            \n        # Call the CountVectorizer.fit_transform() method\n        # or using the method from the super or parent class\n        \n        # apply the fit_transform to both train and test, if test is also provided\n        # using only the fit() method cause out of memory probaly because of the huge vocabulary from train and test combined\n        # if that is the case, the other options to limit the number of max_features learned\n        super().fit_transform(X_new, y)\n        \n        U = self.transform(X)\n        \n        return U","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe4 = Pipeline([\n                    ('cv', CountVectorizerplus(add_test = test['text'])),\n                    ('tfidf_transformer', TfidfTransformer()),\n                    ('ber_nb', BernoulliNB())\n                ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npipe4.fit(X_train, y_train)\npred_val = pipe4.predict_proba(X_val)\n\nprint('Metric log loss', metrics.log_loss(y_val, pred_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}