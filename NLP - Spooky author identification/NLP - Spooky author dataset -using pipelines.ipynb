{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/spooky-author-identification/train.zip\n/kaggle/input/spooky-author-identification/sample_submission.zip\n/kaggle/input/spooky-author-identification/test.zip\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!unzip '/kaggle/input/spooky-author-identification/train.zip'\n!unzip '/kaggle/input/spooky-author-identification/test.zip'","execution_count":2,"outputs":[{"output_type":"stream","text":"Archive:  /kaggle/input/spooky-author-identification/train.zip\n  inflating: train.csv               \nArchive:  /kaggle/input/spooky-author-identification/test.zip\n  inflating: test.csv                \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk(os.getcwd()):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":3,"outputs":[{"output_type":"stream","text":"/kaggle/working/train.csv\n/kaggle/working/test.csv\n/kaggle/working/__notebook_source__.ipynb\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nimport gc\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Advantages of using pipelines:\n1. train and test set tranformations are taken care automatically\n2. hyperparameter tuning is made easier. Set new parameters on any estimator in the pipeline and refit - all this is one line. Or use GridSearch on pipeline\n3. Model description is easier"},{"metadata":{},"cell_type":"markdown","source":"## 1. Data preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data\ntrain = pd.read_csv('/kaggle/working/train.csv')\ntest = pd.read_csv('/kaggle/working/test.csv')\n\ntrain.head(10)","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"        id                                               text author\n0  id26305  This process, however, afforded me no means of...    EAP\n1  id17569  It never once occurred to me that the fumbling...    HPL\n2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n3  id27763  How lovely is spring As we looked from Windsor...    MWS\n4  id12958  Finding nothing else, not even gold, the Super...    HPL\n5  id22965  A youth passed in solitude, my best years spen...    MWS\n6  id09674  The astronomer, perhaps, at this point, took r...    EAP\n7  id13515        The surcingle hung in ribands from my body.    EAP\n8  id19322  I knew that you could not say to yourself 'ste...    EAP\n9  id00912  I confess that neither the structure of langua...    MWS","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>author</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id26305</td>\n      <td>This process, however, afforded me no means of...</td>\n      <td>EAP</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id17569</td>\n      <td>It never once occurred to me that the fumbling...</td>\n      <td>HPL</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id11008</td>\n      <td>In his left hand was a gold snuff box, from wh...</td>\n      <td>EAP</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id27763</td>\n      <td>How lovely is spring As we looked from Windsor...</td>\n      <td>MWS</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id12958</td>\n      <td>Finding nothing else, not even gold, the Super...</td>\n      <td>HPL</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>id22965</td>\n      <td>A youth passed in solitude, my best years spen...</td>\n      <td>MWS</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>id09674</td>\n      <td>The astronomer, perhaps, at this point, took r...</td>\n      <td>EAP</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>id13515</td>\n      <td>The surcingle hung in ribands from my body.</td>\n      <td>EAP</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>id19322</td>\n      <td>I knew that you could not say to yourself 'ste...</td>\n      <td>EAP</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>id00912</td>\n      <td>I confess that neither the structure of langua...</td>\n      <td>MWS</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode labels: 'author'\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train['author'])\nprint('Classes encoded by Label encode', le.classes_)\n\ntrain['author'] = le.transform(train['author'])","execution_count":6,"outputs":[{"output_type":"stream","text":"Classes encoded by Label encode ['EAP' 'HPL' 'MWS']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"        id                                               text  author\n0  id26305  This process, however, afforded me no means of...       0\n1  id17569  It never once occurred to me that the fumbling...       1\n2  id11008  In his left hand was a gold snuff box, from wh...       0\n3  id27763  How lovely is spring As we looked from Windsor...       2\n4  id12958  Finding nothing else, not even gold, the Super...       1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>author</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id26305</td>\n      <td>This process, however, afforded me no means of...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id17569</td>\n      <td>It never once occurred to me that the fumbling...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id11008</td>\n      <td>In his left hand was a gold snuff box, from wh...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id27763</td>\n      <td>How lovely is spring As we looked from Windsor...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id12958</td>\n      <td>Finding nothing else, not even gold, the Super...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"__Split the train into train and validation sets__"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection, metrics\n\nX = train['text']\ny = train['author']\n\n# use stratify to have equal proportion of author classes in the train and validation splits\nX_train, X_val, y_train, y_val =  model_selection.train_test_split(X, y, test_size=0.3, shuffle=True, stratify = train['author'], random_state= 2020)","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### __Create the first pipeline__\n\nTo understand the difference between `TfidfVectorizer` and `TfidfTransformer` refer [here](https://stackoverflow.com/questions/54745482/what-is-the-difference-between-tfidf-vectorizer-and-tfidf-transformer)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\nfrom sklearn.linear_model import LogisticRegression\n\npipe1 = Pipeline([\n                ('cv', CountVectorizer()),\n                ('tfidf_transformer', TfidfTransformer()),\n                ('logit', LogisticRegression())              \n            ])","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting the model\nSince our `pipe1` object has all the properties of an estimator, we can treat it as one. Hence we can use the `fit()`, `transform` and `fit_transform` method if it is defined for the `class`. For `Logistic Regression`, which is the final model in the pipeline, we will use only the fit method"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe1.fit(X_train, y_train)","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"Pipeline(memory=None,\n         steps=[('cv',\n                 CountVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=True, max_df=1.0,\n                                 max_features=None, min_df=1,\n                                 ngram_range=(1, 1), preprocessor=None,\n                                 stop_words=None, strip_accents=None,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=None, vocabulary=None)),\n                ('tfidf_transformer',\n                 TfidfTransformer(norm='l2', smooth_idf=True,\n                                  sublinear_tf=False, use_idf=True)),\n                ('logit',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='auto', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='lbfgs', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Steps of the pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe1.steps","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"[('cv',\n  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n                  dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n                  lowercase=True, max_df=1.0, max_features=None, min_df=1,\n                  ngram_range=(1, 1), preprocessor=None, stop_words=None,\n                  strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                  tokenizer=None, vocabulary=None)),\n ('tfidf_transformer',\n  TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n ('logit',\n  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                     intercept_scaling=1, l1_ratio=None, max_iter=100,\n                     multi_class='auto', n_jobs=None, penalty='l2',\n                     random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                     warm_start=False))]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Accessing each step's parameters by name, methods and properties\n* `pipeline.named_steps`: returns a dictionary with the keys as model names"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Coefficient of the Logistic Regression model\npipe1.named_steps['logit'].coef_","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"array([[ 0.032849  ,  0.01721665,  0.17807575, ...,  0.06896037,\n        -0.01142085, -0.23908224],\n       [-0.02068407, -0.00707508, -0.08608624, ..., -0.0125657 ,\n         0.02025188,  0.24707146],\n       [-0.01216493, -0.01014157, -0.09198951, ..., -0.05639467,\n        -0.00883103, -0.00798922]])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**Making predictions:**\n- Use predict()\n- Use predict_proba()"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_val = pipe1.predict_proba(X_val)\nprint('Log loss on pipeline using cv-tfidftransformer-logit is : ', metrics.log_loss(y_val, pred_val))","execution_count":13,"outputs":[{"output_type":"stream","text":"Log loss on pipeline using cv-tfidftransformer-logit is :  0.5532037740068139\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Parameter tuning in the pipeline\n* We can get the parameters in each step by it's name"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe1.named_steps['logit'].get_params()","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"{'C': 1.0,\n 'class_weight': None,\n 'dual': False,\n 'fit_intercept': True,\n 'intercept_scaling': 1,\n 'l1_ratio': None,\n 'max_iter': 100,\n 'multi_class': 'auto',\n 'n_jobs': None,\n 'penalty': 'l2',\n 'random_state': None,\n 'solver': 'lbfgs',\n 'tol': 0.0001,\n 'verbose': 0,\n 'warm_start': False}"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Getting the parameters all at once for all the steps in the pipeline\n* paramters are prepended with the step name for easier identification"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe1.get_params()","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"{'memory': None,\n 'steps': [('cv',\n   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                   tokenizer=None, vocabulary=None)),\n  ('tfidf_transformer',\n   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n  ('logit',\n   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n                      multi_class='auto', n_jobs=None, penalty='l2',\n                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                      warm_start=False))],\n 'verbose': False,\n 'cv': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                 tokenizer=None, vocabulary=None),\n 'tfidf_transformer': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n 'logit': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n                    multi_class='auto', n_jobs=None, penalty='l2',\n                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                    warm_start=False),\n 'cv__analyzer': 'word',\n 'cv__binary': False,\n 'cv__decode_error': 'strict',\n 'cv__dtype': numpy.int64,\n 'cv__encoding': 'utf-8',\n 'cv__input': 'content',\n 'cv__lowercase': True,\n 'cv__max_df': 1.0,\n 'cv__max_features': None,\n 'cv__min_df': 1,\n 'cv__ngram_range': (1, 1),\n 'cv__preprocessor': None,\n 'cv__stop_words': None,\n 'cv__strip_accents': None,\n 'cv__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n 'cv__tokenizer': None,\n 'cv__vocabulary': None,\n 'tfidf_transformer__norm': 'l2',\n 'tfidf_transformer__smooth_idf': True,\n 'tfidf_transformer__sublinear_tf': False,\n 'tfidf_transformer__use_idf': True,\n 'logit__C': 1.0,\n 'logit__class_weight': None,\n 'logit__dual': False,\n 'logit__fit_intercept': True,\n 'logit__intercept_scaling': 1,\n 'logit__l1_ratio': None,\n 'logit__max_iter': 100,\n 'logit__multi_class': 'auto',\n 'logit__n_jobs': None,\n 'logit__penalty': 'l2',\n 'logit__random_state': None,\n 'logit__solver': 'lbfgs',\n 'logit__tol': 0.0001,\n 'logit__verbose': 0,\n 'logit__warm_start': False}"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Set the parameters within the pipeline\n* set_params()"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# set params and train\npipe1.set_params(cv__max_df = 0.8, cv__min_df = 6, cv__ngram_range=(1,3)).fit(X_train, y_train)\npred_val = pipe1.predict_proba(X_val)\nprint(metrics.log_loss(y_val, pred_val))","execution_count":16,"outputs":[{"output_type":"stream","text":"0.5650860396812747\nCPU times: user 9.76 s, sys: 622 ms, total: 10.4 s\nWall time: 8.36 s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Use a Naive Bayes model in the pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n\npipe2 = Pipeline([\n                    ('cv', CountVectorizer()),\n                    ('tfidf_transformer', TfidfTransformer()),\n                    ('bern_nb', BernoulliNB())\n                ])","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe2.fit(X_train, y_train)\nprint(metrics.log_loss(y_val, pipe2.predict_proba(X_val)))","execution_count":18,"outputs":[{"output_type":"stream","text":"0.5409585440753816\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Feature Union\n* Helps to combine new features created as part of EDA\n\n- Now we will collect some meta features from the text like punctuation_count, parts of speech tag count"},{"metadata":{},"cell_type":"markdown","source":"### NLTK - POS(parts of speech) tagger"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nsample = train['text'].sample(1).values.tolist()[0]\nprint('text: \\n', sample)\ntokens = nltk.word_tokenize(sample)\npos_tag = nltk.pos_tag(tokens)\n\nprint(pos_tag)","execution_count":19,"outputs":[{"output_type":"stream","text":"text: \n Yet I thank God that I have lived I thank God, that I have beheld his throne, the heavens, and earth, his footstool.\n[('Yet', 'RB'), ('I', 'PRP'), ('thank', 'VBP'), ('God', 'NNP'), ('that', 'IN'), ('I', 'PRP'), ('have', 'VBP'), ('lived', 'VBN'), ('I', 'PRP'), ('thank', 'VBP'), ('God', 'NNP'), (',', ','), ('that', 'IN'), ('I', 'PRP'), ('have', 'VBP'), ('beheld', 'VBN'), ('his', 'PRP$'), ('throne', 'NN'), (',', ','), ('the', 'DT'), ('heavens', 'NNS'), (',', ','), ('and', 'CC'), ('earth', 'NN'), (',', ','), ('his', 'PRP$'), ('footstool', 'NN'), ('.', '.')]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df = train.sample(frac=0.001)\nsample_df['text']","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"3322     It was a long dog trot to the ruined station, ...\n1733     I hear a noise at the door, as of some immense...\n13517                    I was then only twenty years old.\n13474    I would have escorted her myself, but felt tha...\n10821    We were all equal now; magnificent dwellings, ...\n12508    With this, she took my arm, and I attended her...\n411      Then too, we had spoken to him in advance of o...\n5106     It is rather fortunate that Elwood was not in ...\n4414     Most of these, however, soon shewed their pove...\n18441    Into the granite city of Teloth wandered the y...\n13107    He was not, as the other traveller seemed to b...\n6790     Why need I paint, Charmion, the now disenchain...\n6731            I often said to myself, my father is dead.\n4504     There was a hideous screaming which echoed abo...\n8453     \"That is, sailors that didn't hail from Innsmo...\n15933    P. Still, there is one of your expressions whi...\n10292    It had not been their intention to return; but...\n16331    Mr. Czanek did not like to wait so long in the...\n15711    As the moon climbed higher in the sky, I began...\n14749    In short, it seemed to my uncle and me that an...\nName: text, dtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nCounter(tag for word, tag in pos_tag)\n\n\ndef cnt_pos(sentence):\n    return Counter(tag for word, tag in nltk.pos_tag(nltk.word_tokenize(sentence)))\n\nsample_df['text'].apply(cnt_pos).apply(pd.Series).fillna(0)","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"       PRP  VBD   DT    JJ    NN   TO  VBN    ,   CC  VBZ  ...  WRB    :  POS  \\\n3322   2.0  2.0  4.0   4.0   5.0  1.0  1.0  1.0  1.0  1.0  ...  0.0  0.0  0.0   \n1733   2.0  0.0  3.0   1.0   4.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0   \n13517  1.0  1.0  0.0   1.0   0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n13474  4.0  3.0  2.0   1.0   2.0  1.0  1.0  3.0  1.0  0.0  ...  1.0  0.0  0.0   \n10821  1.0  2.0  2.0   3.0   0.0  1.0  1.0  3.0  1.0  0.0  ...  0.0  1.0  0.0   \n12508  3.0  2.0  1.0   0.0   2.0  0.0  0.0  2.0  1.0  0.0  ...  0.0  0.0  0.0   \n411    5.0  1.0  1.0   2.0   3.0  1.0  1.0  1.0  2.0  0.0  ...  0.0  1.0  1.0   \n5106   1.0  1.0  2.0   4.0   2.0  0.0  1.0  0.0  0.0  1.0  ...  1.0  0.0  0.0   \n4414   2.0  2.0  4.0   3.0   7.0  1.0  0.0  3.0  3.0  0.0  ...  0.0  1.0  0.0   \n18441  0.0  1.0  4.0   3.0  10.0  0.0  2.0  2.0  1.0  1.0  ...  0.0  0.0  0.0   \n13107  1.0  2.0  4.0   2.0   4.0  1.0  0.0  3.0  1.0  0.0  ...  0.0  0.0  0.0   \n6790   1.0  0.0  1.0   0.0   2.0  0.0  1.0  2.0  0.0  0.0  ...  1.0  0.0  0.0   \n6731   2.0  1.0  0.0   1.0   1.0  1.0  0.0  1.0  0.0  1.0  ...  0.0  0.0  0.0   \n4504   0.0  2.0  5.0   2.0   6.0  0.0  2.0  1.0  3.0  0.0  ...  0.0  0.0  1.0   \n8453   0.0  1.0  1.0   0.0   0.0  0.0  0.0  1.0  0.0  1.0  ...  0.0  0.0  0.0   \n15933  2.0  0.0  1.0   2.0   2.0  1.0  0.0  1.0  0.0  1.0  ...  0.0  0.0  0.0   \n10292  2.0  2.0  2.0   1.0   4.0  2.0  1.0  2.0  1.0  0.0  ...  0.0  1.0  0.0   \n16331  0.0  1.0  2.0   1.0   2.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n15711  2.0  4.0  4.0   1.0   3.0  1.0  1.0  1.0  0.0  0.0  ...  0.0  0.0  0.0   \n14749  2.0  2.0  5.0  10.0   6.0  3.0  1.0  2.0  5.0  0.0  ...  0.0  1.0  0.0   \n\n       NNP  JJS   EX  WDT   ``   ''  PDT  \n3322   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n1733   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n13517  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n13474  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n10821  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n12508  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n411    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n5106   2.0  0.0  0.0  0.0  0.0  0.0  0.0  \n4414   0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n18441  2.0  0.0  0.0  0.0  0.0  0.0  0.0  \n13107  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n6790   1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n6731   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n4504   1.0  0.0  1.0  1.0  0.0  0.0  0.0  \n8453   1.0  0.0  0.0  1.0  1.0  0.0  0.0  \n15933  1.0  0.0  1.0  1.0  1.0  1.0  0.0  \n10292  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n16331  2.0  0.0  0.0  0.0  0.0  0.0  0.0  \n15711  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n14749  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n\n[20 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PRP</th>\n      <th>VBD</th>\n      <th>DT</th>\n      <th>JJ</th>\n      <th>NN</th>\n      <th>TO</th>\n      <th>VBN</th>\n      <th>,</th>\n      <th>CC</th>\n      <th>VBZ</th>\n      <th>...</th>\n      <th>WRB</th>\n      <th>:</th>\n      <th>POS</th>\n      <th>NNP</th>\n      <th>JJS</th>\n      <th>EX</th>\n      <th>WDT</th>\n      <th>``</th>\n      <th>''</th>\n      <th>PDT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3322</th>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1733</th>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>13517</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>13474</th>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>10821</th>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>12508</th>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>411</th>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5106</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4414</th>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>7.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>18441</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>13107</th>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6790</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6731</th>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4504</th>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>8453</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>15933</th>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>10292</th>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>16331</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>15711</th>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>14749</th>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>10.0</td>\n      <td>6.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>20 rows × 31 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"__More on the tags and what they represent__"},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.help.upenn_tagset()","execution_count":22,"outputs":[{"output_type":"stream","text":"$: dollar\n    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n'': closing quotation mark\n    ' ''\n(: opening parenthesis\n    ( [ {\n): closing parenthesis\n    ) ] }\n,: comma\n    ,\n--: dash\n    --\n.: sentence terminator\n    . ! ?\n:: colon or ellipsis\n    : ; ...\nCC: conjunction, coordinating\n    & 'n and both but either et for less minus neither nor or plus so\n    therefore times v. versus vs. whether yet\nCD: numeral, cardinal\n    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n    fifteen 271,124 dozen quintillion DM2,000 ...\nDT: determiner\n    all an another any both del each either every half la many much nary\n    neither no some such that the them these this those\nEX: existential there\n    there\nFW: foreign word\n    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n    terram fiche oui corporis ...\nIN: preposition or conjunction, subordinating\n    astride among uppon whether out inside pro despite on by throughout\n    below within for towards near behind atop around if like until below\n    next into if beside ...\nJJ: adjective or numeral, ordinal\n    third ill-mannered pre-war regrettable oiled calamitous first separable\n    ectoplasmic battery-powered participatory fourth still-to-be-named\n    multilingual multi-disciplinary ...\nJJR: adjective, comparative\n    bleaker braver breezier briefer brighter brisker broader bumper busier\n    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n    cozier creamier crunchier cuter ...\nJJS: adjective, superlative\n    calmest cheapest choicest classiest cleanest clearest closest commonest\n    corniest costliest crassest creepiest crudest cutest darkest deadliest\n    dearest deepest densest dinkiest ...\nLS: list item marker\n    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n    SP-44007 Second Third Three Two * a b c d first five four one six three\n    two\nMD: modal auxiliary\n    can cannot could couldn't dare may might must need ought shall should\n    shouldn't will would\nNN: noun, common, singular or mass\n    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n    investment slide humour falloff slick wind hyena override subhumanity\n    machinist ...\nNNP: noun, proper, singular\n    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n    Shannon A.K.C. Meltex Liverpool ...\nNNPS: noun, proper, plural\n    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n    Apache Apaches Apocrypha ...\nNNS: noun, common, plural\n    undergraduates scotches bric-a-brac products bodyguards facets coasts\n    divestitures storehouses designs clubs fragrances averages\n    subjectivists apprehensions muses factory-jobs ...\nPDT: pre-determiner\n    all both half many quite such sure this\nPOS: genitive marker\n    ' 's\nPRP: pronoun, personal\n    hers herself him himself hisself it itself me myself one oneself ours\n    ourselves ownself self she thee theirs them themselves they thou thy us\nPRP$: pronoun, possessive\n    her his mine my our ours their thy your\nRB: adverb\n    occasionally unabatingly maddeningly adventurously professedly\n    stirringly prominently technologically magisterially predominately\n    swiftly fiscally pitilessly ...\nRBR: adverb, comparative\n    further gloomier grander graver greater grimmer harder harsher\n    healthier heavier higher however larger later leaner lengthier less-\n    perfectly lesser lonelier longer louder lower more ...\nRBS: adverb, superlative\n    best biggest bluntest earliest farthest first furthest hardest\n    heartiest highest largest least less most nearest second tightest worst\nRP: particle\n    aboard about across along apart around aside at away back before behind\n    by crop down ever fast for forth from go high i.e. in into just later\n    low more off on open out over per pie raising start teeth that through\n    under unto up up-pp upon whole with you\nSYM: symbol\n    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\nTO: \"to\" as preposition or infinitive marker\n    to\nUH: interjection\n    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n    man baby diddle hush sonuvabitch ...\nVB: verb, base form\n    ask assemble assess assign assume atone attention avoid bake balkanize\n    bank begin behold believe bend benefit bevel beware bless boil bomb\n    boost brace break bring broil brush build ...\nVBD: verb, past tense\n    dipped pleaded swiped regummed soaked tidied convened halted registered\n    cushioned exacted snubbed strode aimed adopted belied figgered\n    speculated wore appreciated contemplated ...\nVBG: verb, present participle or gerund\n    telegraphing stirring focusing angering judging stalling lactating\n    hankerin' alleging veering capping approaching traveling besieging\n    encrypting interrupting erasing wincing ...\nVBN: verb, past participle\n    multihulled dilapidated aerosolized chaired languished panelized used\n    experimented flourished imitated reunifed factored condensed sheared\n    unsettled primed dubbed desired ...\nVBP: verb, present tense, not 3rd person singular\n    predominate wrap resort sue twist spill cure lengthen brush terminate\n    appear tend stray glisten obtain comprise detest tease attract\n    emphasize mold postpone sever return wag ...\nVBZ: verb, present tense, 3rd person singular\n    bases reconstructs marks mixes displeases seals carps weaves snatches\n    slumps stretches authorizes smolders pictures emerges stockpiles\n    seduces fizzes uses bolsters slaps speaks pleads ...\nWDT: WH-determiner\n    that what whatever which whichever\nWP: WH-pronoun\n    that what whatever whatsoever which who whom whosoever\nWP$: WH-pronoun, possessive\n    whose\nWRB: Wh-adverb\n    how however whence whenever where whereby whereever wherein whereof why\n``: opening quotation mark\n    ` ``\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### 'Tag' text in the pipeline\nInorder to allow for 'tag' of text within the pipeline, we need to create a `transformer` or `estimator class`, which will `inherit` some `base classes` and `overload` a few functions that will need to be used"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import base classes\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom collections import Counter\n\n# define a new class, inherit from the base classes\nclass posTagMatrix(BaseEstimator, TransformerMixin):\n    \n    # use a custom tokenizer if not passed\n    # normalize = True, divide by the total number of tags in the sentence\n    \n    def __init__(self, tokenizer = lambda x: x.split(), normalize = True):\n        self.tokenizer = tokenizer\n        self.normalize = normalize\n        \n    # helper function to tokenizer and count pos tags\n    def count_pos(self, sentence):\n        return Counter(tag for word, tag in nltk.pos_tag(self.tokenizer(sentence)))\n    \n    \n    # this doesn't do anything and this makes it possible to use the fit() method\n    def fit(self, X, y=None):\n        return self\n    \n    \n    # all the transformation is done here\n    def transform(self, X):\n        # the fit method needs to be applied to a series on text\n        # this returns the count for individual tags in each sentence\n        X_tagged = X.apply(self.count_pos).apply(pd.Series).fillna(0)\n        # sum the count of all tags in the sentence\n        X_tagged['total_pos_tags'] = X_tagged.apply(sum, axis=1)\n        if self.normalize:\n            X_tagged = X_tagged.divide(X_tagged['total_pos_tags'], axis=0)\n            \n        return X_tagged","execution_count":23,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### New pipeline with the pos tags included"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import FeatureUnion\n\npipe3 = Pipeline([\n                    ('u1', FeatureUnion([\n                                            ('tfidf_features', Pipeline([\n                                                                        ('cv', CountVectorizer()),\n                                                                        ('tfidf_transformer', TfidfTransformer())\n                                                                        ])),\n                                            ('pos_features', Pipeline([\n                                                                        ('pos', posTagMatrix(tokenizer= nltk.word_tokenize))\n                                                                    ]))\n                                        ])),\n                \n                    ('logit', LogisticRegression())\n      \n                ])\n","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npipe3.fit(X_train, y_train)\npred_val = pipe3.predict_proba(X_val)\nprint(metrics.log_loss(y_val, pred_val))","execution_count":25,"outputs":[{"output_type":"stream","text":"0.6174722434255739\nCPU times: user 1min 6s, sys: 879 ms, total: 1min 7s\nWall time: 1min 5s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Overload the 'CountVectorizer' class\n* this way the transformations can be applied to both 'train' and 'test' sets together\n* refer this [link](https://stackoverflow.com/questions/400739/what-does-asterisk-mean-in-python) on what `*args` and `**kwargs` means. Basically they are used to pack extra arguments to a function\n\n    * def f1(*a): 'a' will be a tuple of extra parameters\n    * def f2(**a): 'a' will be a dictionary of extra parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# inherit from the base class: CountVectorizer\nclass CountVectorizerplus(CountVectorizer):\n    \n    def __init__(self, *args, add_test=None, **kwargs):\n        self.add_test = add_test\n        # initialize using the super class or parent class\n        super().__init__(*args, **kwargs)   \n    \n    \n    def transform(self, X):\n        # use the transform method from the super class or parent class\n        U = super().transform(X)\n        return U\n    \n    def fit_transform(self, X , y=None):\n        if self.add_test is not None:\n            # add the test along with train\n            X_new = pd.concat([X, self.add_test])\n        else:\n            X_new = X\n            \n        # Call the CountVectorizer.fit_transform() method\n        # or using the method from the super or parent class\n        \n        # apply the fit_transform to both train and test, if test is also provided\n        # using only the fit() method cause out of memory probaly because of the huge vocabulary from train and test combined\n        # if that is the case, the other options to limit the number of max_features learned\n        #super().fit_transform(X_new, y)\n        super().fit_transform(X_new, y)\n        \n        U = self.transform(X)\n        \n        return U","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe4 = Pipeline([\n                    ('cv', CountVectorizerplus(add_test = test['text'])),\n                    ('tfidf_transformer', TfidfTransformer()),\n                    ('ber_nb', BernoulliNB())\n                ])","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npipe4.fit(X_train, y_train)\npred_val = pipe4.predict_proba(X_val)\n\nprint('Metric log loss', metrics.log_loss(y_val, pred_val))","execution_count":28,"outputs":[{"output_type":"stream","text":"Metric log loss 0.5614716252475621\nCPU times: user 2.15 s, sys: 0 ns, total: 2.15 s\nWall time: 2.15 s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"__Limiting the number of max_features learned__\n\n    * By limiting the number of `max_features' in CountVectorizer, we get a slightly better score\n    * This parameters is passed and intiliazed using the based class of Countvectorizer and not is not blanketed under args and kwargs"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npipe5 = Pipeline([\n                    ('cv', CountVectorizerplus(add_test = test['text'], max_features =15000)),\n                    ('tfidf_transformer', TfidfTransformer()),\n                    ('ber_nb', BernoulliNB())\n                ])\n\n\npipe5.fit(X_train, y_train)\npred_val = pipe5.predict_proba(X_val)\n\nprint('Metric log loss', metrics.log_loss(y_val, pred_val))","execution_count":29,"outputs":[{"output_type":"stream","text":"Metric log loss 0.517943439619949\nCPU times: user 2.12 s, sys: 3.87 ms, total: 2.12 s\nWall time: 2.12 s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Stacking pipelines\nPipelines cannot have more than one final estimator. So the below cannot execute"},{"metadata":{"trusted":true},"cell_type":"code","source":"# pipe6 = Pipeline([\n#                     ('u1', FeatureUnion([\n#                                             ('tfidf_features', Pipeline([\n#                                                                         ('cv', CountVectorizer()),\n#                                                                         ('tfidf', TfidfTransformer()),\n#                                                                         ('tfidf_logit', LogisticRegression())\n#                                                                         ])),\n#                                             ('pos_features', Pipeline([\n#                                                                         ('pos', PosTagMatrix(tokenizer = nltk.word_tokenize)),\n#                                                                         ('pos_logit', LogistiRegression())\n#                                                                         ])),\n#                     ('xgb', XGBClassifier())\n\n#                                         ]))\n#         ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stacking trick:"},{"metadata":{},"cell_type":"markdown","source":"To makes use of the classifier in multiple stages of pipeline, create a `wrapper` around the `classifier`. By doing so we will convert it the `classifier` into a `transformer` class, which will transform the input data into predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Classifierwrapper(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, estimator, verbose = None, fit_params = None, use_proba = True, scoring = None):\n        \n        self.estimator = estimator\n        \n        # control verbosity: # 0 means no verbose, 1 - moderately verbose, 2 - more verbose\n        # self.verbose = verbose\n        \n        if verbose is None:\n            self.verbose = 0\n        else:\n            self.verbose = verbose\n        \n        # if need to fit parameters\n        self.fit_params = fit_params\n        \n        # if need to use predict_proba in transform method\n        self.use_proba = use_proba\n        \n        # calculate the validation score based on the scoring function\n        self.scoring = scoring\n        \n        # variable to store the store if the scoring function is set\n        self.score = None\n        \n        \n    def fit(self, X, y):\n        fp = self.fit_params\n        \n        if self.verbose == 2:\n            print('X shape', X.shape, \"\\n fit params:\", self.fit_params)\n        \n        if fp is not None:\n            self.estimator.fit(X, y, **fp)\n        else:\n            self.estimator.fit(X, y)\n        \n        return self\n    \n    \n    def transform(self, X):\n        # is the estimator needs to use the predict_proba method\n        if self.use_proba:\n            return self.estimator.predict_proba(X)\n        else:\n            return self.estimator.predict(X)\n        \n        \n    def fit_transform(self, X, y, **kwargs):\n        self.fit(X, y)\n        p = self.transform(X)\n        return p\n    \n    \n    def predict(self, X):\n        return self.estimator.predict(X)\n    \n    def predict_proba(self, X):\n        return self.estimator.predict_proba(X)","execution_count":31,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using XGBClassifier as the final estimator in the pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxgb_params = {\n                'objective': 'multi:softprob',\n                'max_depth': 3,\n                'eta': 0.1,\n                'silent': 1,\n                'min_child_weight': 1,\n                'subsample': 0.8,\n                'colsample_by_tree': 0.7,\n                'seed': 2020,\n                'num_rounds': 2000\n}","execution_count":34,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All estimators within the `FeatureUnion` of  2 pipelines should be able to implement fit and transform method.XGBClassifier doesn't have a transform method. So it has to be added as part of the pipeline but outside the `FeatureUnion`"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe7 = Pipeline([\n                    ('u1', FeatureUnion([\n                                            ('tfidf_features', Pipeline([\n                                                                        ('cv', CountVectorizer()),\n                                                                        ('tfidf', TfidfTransformer()),\n                                                                        ('tfidf_logit', Classifierwrapper(LogisticRegression()))\n                                                                        ])),\n                                            ('pos_features', Pipeline([\n                                                                        ('pos', posTagMatrix(tokenizer = nltk.word_tokenize)),\n                                                                        ('pos_logit', Classifierwrapper(LogisticRegression()))\n                                                                        ]))\n                                        ])),\n    \n                    ('xgb', XGBClassifier(**xgb_params)),\n\n                                        \n        ])","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe7.fit(X_train, y_train)\npred_val = pipe7.predict_proba(X_val)\n\nprint('Metric log loss', metrics.log_loss(y_val, pred_val))","execution_count":38,"outputs":[{"output_type":"stream","text":"Metric log loss 0.5661913624741971\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Submission\n\nNow that we have create a pipeline it becomes easier to train the model on the full train set without data leakage to test"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# fit the model again on the enire train set\npipe7.fit(train['text'], le.fit_transform(train['author']))\n\n# predictions on the actual test\npred = pipe7.predict_proba(test['text'])","execution_count":41,"outputs":[{"output_type":"stream","text":"CPU times: user 1min 42s, sys: 1.26 s, total: 1min 44s\nWall time: 1min 32s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(pred, columns = ['EAP','HPL', 'MWS'], index = test.id)\nsubmission.to_csv('submission_pipelines.csv', index=id)","execution_count":48,"outputs":[{"output_type":"execute_result","execution_count":48,"data":{"text/plain":"              EAP       HPL       MWS\nid                                   \nid02310  0.115672  0.007282  0.877046\nid24541  0.987783  0.010618  0.001599\nid00134  0.024235  0.970655  0.005111\nid27757  0.948460  0.046909  0.004631\nid04081  0.877755  0.014504  0.107741\n...           ...       ...       ...\nid11749  0.952656  0.003652  0.043691\nid10526  0.001740  0.000468  0.997792\nid13477  0.990142  0.005216  0.004643\nid13761  0.099599  0.007766  0.892635\nid04282  0.105624  0.890682  0.003694\n\n[8392 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EAP</th>\n      <th>HPL</th>\n      <th>MWS</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>id02310</th>\n      <td>0.115672</td>\n      <td>0.007282</td>\n      <td>0.877046</td>\n    </tr>\n    <tr>\n      <th>id24541</th>\n      <td>0.987783</td>\n      <td>0.010618</td>\n      <td>0.001599</td>\n    </tr>\n    <tr>\n      <th>id00134</th>\n      <td>0.024235</td>\n      <td>0.970655</td>\n      <td>0.005111</td>\n    </tr>\n    <tr>\n      <th>id27757</th>\n      <td>0.948460</td>\n      <td>0.046909</td>\n      <td>0.004631</td>\n    </tr>\n    <tr>\n      <th>id04081</th>\n      <td>0.877755</td>\n      <td>0.014504</td>\n      <td>0.107741</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>id11749</th>\n      <td>0.952656</td>\n      <td>0.003652</td>\n      <td>0.043691</td>\n    </tr>\n    <tr>\n      <th>id10526</th>\n      <td>0.001740</td>\n      <td>0.000468</td>\n      <td>0.997792</td>\n    </tr>\n    <tr>\n      <th>id13477</th>\n      <td>0.990142</td>\n      <td>0.005216</td>\n      <td>0.004643</td>\n    </tr>\n    <tr>\n      <th>id13761</th>\n      <td>0.099599</td>\n      <td>0.007766</td>\n      <td>0.892635</td>\n    </tr>\n    <tr>\n      <th>id04282</th>\n      <td>0.105624</td>\n      <td>0.890682</td>\n      <td>0.003694</td>\n    </tr>\n  </tbody>\n</table>\n<p>8392 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}