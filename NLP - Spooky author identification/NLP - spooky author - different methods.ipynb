{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/glove-840b300d-dj/glove.840B.300d.txt\n/kaggle/input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl\n/kaggle/input/spooky-author-identification/test.zip\n/kaggle/input/spooky-author-identification/train.zip\n/kaggle/input/spooky-author-identification/sample_submission.zip\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!unzip '/kaggle/input/spooky-author-identification/train.zip'\n!unzip '/kaggle/input/spooky-author-identification/test.zip'\n!unzip '/kaggle/input/spooky-author-identification/sample_submission.zip'","execution_count":2,"outputs":[{"output_type":"stream","text":"Archive:  /kaggle/input/spooky-author-identification/train.zip\n  inflating: train.csv               \nArchive:  /kaggle/input/spooky-author-identification/test.zip\n  inflating: test.csv                \nArchive:  /kaggle/input/spooky-author-identification/sample_submission.zip\n  inflating: sample_submission.csv   \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk(os.getcwd()):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":3,"outputs":[{"output_type":"stream","text":"/kaggle/working/train.csv\n/kaggle/working/test.csv\n/kaggle/working/__notebook_source__.ipynb\n/kaggle/working/sample_submission.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nimport gc\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn import preprocessing, model_selection, metrics, decomposition\n\nimport logging\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data\ntrain = pd.read_csv('/kaggle/working/train.csv')\ntest = pd.read_csv('/kaggle/working/test.csv')\n\ntrain.head(10)","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"        id                                               text author\n0  id26305  This process, however, afforded me no means of...    EAP\n1  id17569  It never once occurred to me that the fumbling...    HPL\n2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n3  id27763  How lovely is spring As we looked from Windsor...    MWS\n4  id12958  Finding nothing else, not even gold, the Super...    HPL\n5  id22965  A youth passed in solitude, my best years spen...    MWS\n6  id09674  The astronomer, perhaps, at this point, took r...    EAP\n7  id13515        The surcingle hung in ribands from my body.    EAP\n8  id19322  I knew that you could not say to yourself 'ste...    EAP\n9  id00912  I confess that neither the structure of langua...    MWS","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>author</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id26305</td>\n      <td>This process, however, afforded me no means of...</td>\n      <td>EAP</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id17569</td>\n      <td>It never once occurred to me that the fumbling...</td>\n      <td>HPL</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id11008</td>\n      <td>In his left hand was a gold snuff box, from wh...</td>\n      <td>EAP</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id27763</td>\n      <td>How lovely is spring As we looked from Windsor...</td>\n      <td>MWS</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id12958</td>\n      <td>Finding nothing else, not even gold, the Super...</td>\n      <td>HPL</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>id22965</td>\n      <td>A youth passed in solitude, my best years spen...</td>\n      <td>MWS</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>id09674</td>\n      <td>The astronomer, perhaps, at this point, took r...</td>\n      <td>EAP</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>id13515</td>\n      <td>The surcingle hung in ribands from my body.</td>\n      <td>EAP</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>id19322</td>\n      <td>I knew that you could not say to yourself 'ste...</td>\n      <td>EAP</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>id00912</td>\n      <td>I confess that neither the structure of langua...</td>\n      <td>MWS</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(5)","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"        id                                               text\n0  id02310  Still, as I urged our leaving Ireland with suc...\n1  id24541  If a fire wanted fanning, it could readily be ...\n2  id00134  And when they had broken down the frail door t...\n3  id27757  While I was thinking how I should possibly man...\n4  id04081  I am not sure to what limit his knowledge may ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id02310</td>\n      <td>Still, as I urged our leaving Ireland with suc...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id24541</td>\n      <td>If a fire wanted fanning, it could readily be ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id00134</td>\n      <td>And when they had broken down the frail door t...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id27757</td>\n      <td>While I was thinking how I should possibly man...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id04081</td>\n      <td>I am not sure to what limit his knowledge may ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Use label encoder to encode the text labels to integers"},{"metadata":{"trusted":true},"cell_type":"code","source":"le = preprocessing.LabelEncoder()\ny = le.fit_transform(train['author'])","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train['text']\nX_train, X_valid, y_train, y_valid = model_selection.train_test_split(X, y, test_size=0.1, random_state= 2020, shuffle=True, stratify = train['author'])\nprint(X_train.shape)\nprint(X_valid.shape)","execution_count":8,"outputs":[{"output_type":"stream","text":"(17621,)\n(1958,)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Some base models first"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\ntfidf_vec = TfidfVectorizer(min_df = 3, max_df = 0.8, analyzer='word', ngram_range =(1,3), token_pattern = r'\\w{1,}',\n                        use_idf=True, smooth_idf=True, sublinear_tf=True)\n\n# fit to both train and valid sets\ntfidf_vec.fit(X_train.values.tolist() + X_valid.values.tolist())\n\n# transformed\nX_train_tfv = tfidf_vec.transform(X_train.values.tolist())\nX_valid_tfv = tfidf_vec.transform(X_valid.values.tolist())","execution_count":9,"outputs":[{"output_type":"stream","text":"CPU times: user 8.39 s, sys: 207 ms, total: 8.6 s\nWall time: 8.63 s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Fit a Logisitic Regression model on tfidf"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=0.9)\n\nlogreg.fit(X_train_tfv, y_train)\npredictions = logreg.predict_proba(X_valid_tfv)\n\nprint('Log loss using Logisitic Regression on Tfidf Vectorizer is : ', metrics.log_loss(y_valid, predictions))","execution_count":10,"outputs":[{"output_type":"stream","text":"Log loss using Logisitic Regression on Tfidf Vectorizer is :  0.5664610734198464\nCPU times: user 8.58 s, sys: 136 ms, total: 8.72 s\nWall time: 4.52 s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Use `wordcount` as features  instead of TFIDF using CountVectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount_vec = CountVectorizer(min_df = 3, max_df = 0.8, ngram_range=(1,3), stop_words='english', analyzer='word', token_pattern = r'\\w{1,}')\n\ncount_vec.fit(X_train.values.tolist() + X_valid.values.tolist())\nX_train_cv = count_vec.transform(X_train.values.tolist())\nX_valid_cv = count_vec.transform(X_valid.values.tolist())","execution_count":11,"outputs":[{"output_type":"stream","text":"CPU times: user 4.64 s, sys: 70.4 ms, total: 4.71 s\nWall time: 4.53 s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Fit a simple Logisitc regression model on Count Vectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlogreg.fit(X_train_cv, y_train)\nprediction = logreg.predict_proba(X_valid_cv)\nprint('Log loss using Logisitic Regression on Count Vectorizer is : ', metrics.log_loss(y_valid, prediction))","execution_count":12,"outputs":[{"output_type":"stream","text":"Log loss using Logisitic Regression on Count Vectorizer is :  0.4757834779740475\nCPU times: user 3.22 s, sys: 44.7 ms, total: 3.27 s\nWall time: 1.67 s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayes model on Count Vectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.naive_bayes import MultinomialNB\n\nmnb = MultinomialNB()\nmnb.fit(X_train_cv, y_train)\nprediction = mnb.predict_proba(X_valid_cv)\nprint('Log loss using Multinomial NB on Count Vectorizer is : ', metrics.log_loss(y_valid, prediction))","execution_count":13,"outputs":[{"output_type":"stream","text":"Log loss using Multinomial NB on Count Vectorizer is :  0.45400575892684447\nCPU times: user 29.8 ms, sys: 3.68 ms, total: 33.4 ms\nWall time: 16.8 ms\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayes model on Tfidf Vectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.naive_bayes import MultinomialNB\n\nmnb = MultinomialNB()\nmnb.fit(X_train_tfv, y_train)\nprediction = mnb.predict_proba(X_valid_tfv)\nprint('Log loss using Multinomial NB on Tfidf Vectorizer is : ', metrics.log_loss(y_valid, prediction))","execution_count":14,"outputs":[{"output_type":"stream","text":"Log loss using Multinomial NB on Tfidf Vectorizer is :  0.565051212698791\nCPU times: user 45.8 ms, sys: 1.99 ms, total: 47.8 ms\nWall time: 23 ms\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### SVM(Support Vector Machine) on TFIDF - using SVD\nSince SVM takes a lot of time to on this high dimensional dataset, we will reduce the dimension using SVD (Singular Value Decomposition) befor applying SVM\n\nAlso it is important to standardise the data prior to applying SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfrom sklearn.decomposition import TruncatedSVD\n\nn_comp = 20\n\nsvd = TruncatedSVD(n_components = n_comp)\n# fit the SVD on tf-id vector\nsvd.fit(X_train_tfv)\nX_train_tf_svd = svd.transform(X_train_tfv)\nX_valid_tf_svd = svd.transform(X_valid_tfv)\n\n# scale the data prior to applying SVM\nscaler = preprocessing.StandardScaler()\nscaler.fit(X_train_tf_svd)\nX_train_tf_svd_scaled = scaler.transform(X_train_tf_svd)\nX_valid_tf_svd_scaled = scaler.transform(X_valid_tf_svd)","execution_count":15,"outputs":[{"output_type":"stream","text":"CPU times: user 1.57 s, sys: 40.8 ms, total: 1.61 s\nWall time: 857 ms\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Apply a simple SVM classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.svm import SVC\n\nsvc = SVC(C=0.9, probability = True)\nsvc.fit(X_train_tf_svd_scaled, y_train)\nprediction = svc.predict_proba(X_valid_tf_svd_scaled)\nprint('Log loss using SVC on TF-IDF Vectorizer with SVD is : ', metrics.log_loss(y_valid, prediction))","execution_count":16,"outputs":[{"output_type":"stream","text":"Log loss using SVC on TF-IDF Vectorizer with SVD is :  0.8525357164606708\nCPU times: user 2min 2s, sys: 1.35 s, total: 2min 3s\nWall time: 2min 3s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Using XGboost on the data\n\nFit the model on the original high dimension tf-idf vector. The vector will be compressed into 'csc' or 'csr' format before applying the fit method of xgboost\n\nFor more details on sparse matrices, refer [here](https://rushter.com/blog/scipy-sparse-matrices/)\n * CSR - Compressed Sparse Row - usually used when the number of rows is less than the number of columns\n * CSC - Compressed Sparse Column - usually when there are lesser number of columns than rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nimport xgboost as xgb\nfrom scipy.sparse import csr_matrix # to convert the input into \n\nxgb_clf = xgb.XGBClassifier(n_estimators = 200, max_depth =7, learning_rate = 0.1, verbose = 2, colsample_bytree = 0.8, subsample =0.8, n_jobs=-1, nthread=10)\n\n# convert the input into Compressed Sparse Column format\nxgb_clf.fit(X_train_tfv.tocsc(), y_train)\npredictions = xgb_clf.predict_proba(X_valid_tfv.tocsc())\n\nprint('Log loss using Xgboost on the original TF-IDF Vectorizer is : ', metrics.log_loss(y_valid, prediction))","execution_count":17,"outputs":[{"output_type":"stream","text":"Log loss using Xgboost on the original TF-IDF Vectorizer is :  0.8525357164606708\nCPU times: user 6min 50s, sys: 5min 41s, total: 12min 32s\nWall time: 3min 13s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Using Xgboost on the CountVectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# convert the input into Compressed Sparse Column format\n\nxgb_clf = xgb.XGBClassifier(n_estimators = 200, max_depth =7, learning_rate = 0.1, verbose = 2, colsample_bytree = 0.8, subsample =0.8, n_jobs=-1, nthread=10)\n\nxgb_clf.fit(X_train_cv.tocsc(), y_train)\npredictions = xgb_clf.predict_proba(X_valid_cv.tocsc())\n\nprint('Log loss using Xgboost on the original Count Vectorizer is : ', metrics.log_loss(y_valid, prediction))","execution_count":18,"outputs":[{"output_type":"stream","text":"Log loss using Xgboost on the original Count Vectorizer is :  0.8525357164606708\nCPU times: user 1min 36s, sys: 2min 6s, total: 3min 42s\nWall time: 56.9 s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Using Xgboost on the Tf-idf SVD features"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# convert the input into Compressed Sparse Column format\n\nxgb_clf.fit(X_train_tf_svd, y_train)\npredictions = xgb_clf.predict_proba(X_valid_tf_svd)\n\nprint('Log loss using Xgboost on the original Count Vectorizer is : %0.3f' % metrics.log_loss(y_valid, prediction))","execution_count":19,"outputs":[{"output_type":"stream","text":"Log loss using Xgboost on the original Count Vectorizer is : 0.853\nCPU times: user 1min 38s, sys: 1min 24s, total: 3min 3s\nWall time: 47.1 s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Using GridSearch"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a scoring function if you using a custom scorer\n\nlogloss_scorer = metrics.make_scorer(metrics.log_loss, greater_is_better = False, needs_proba = True)","execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\n\n# intialize SVD\nsvd = TruncatedSVD()\n\n# initialize Standard Scaler\nscaler = preprocessing.StandardScaler()\n\n# logistic regression\nlogreg = LogisticRegression()\n\n# Create a pipeline with Logistic Regression as the final estimator\n\npipe1 = Pipeline([\n                    ('svd', svd),\n                    ('scaler', scaler),\n                    ('logreg', logreg)\n                ])","execution_count":21,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* To get a view of the steps in the pipeline, use **pipe.named_steps()**\n* To access the parameters in the pipelines, use **pipe.get_params()**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe1.get_params()","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"{'memory': None,\n 'steps': [('svd',\n   TruncatedSVD(algorithm='randomized', n_components=2, n_iter=5,\n                random_state=None, tol=0.0)),\n  ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n  ('logreg',\n   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n                      multi_class='auto', n_jobs=None, penalty='l2',\n                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                      warm_start=False))],\n 'verbose': False,\n 'svd': TruncatedSVD(algorithm='randomized', n_components=2, n_iter=5,\n              random_state=None, tol=0.0),\n 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n 'logreg': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n                    multi_class='auto', n_jobs=None, penalty='l2',\n                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                    warm_start=False),\n 'svd__algorithm': 'randomized',\n 'svd__n_components': 2,\n 'svd__n_iter': 5,\n 'svd__random_state': None,\n 'svd__tol': 0.0,\n 'scaler__copy': True,\n 'scaler__with_mean': True,\n 'scaler__with_std': True,\n 'logreg__C': 1.0,\n 'logreg__class_weight': None,\n 'logreg__dual': False,\n 'logreg__fit_intercept': True,\n 'logreg__intercept_scaling': 1,\n 'logreg__l1_ratio': None,\n 'logreg__max_iter': 100,\n 'logreg__multi_class': 'auto',\n 'logreg__n_jobs': None,\n 'logreg__penalty': 'l2',\n 'logreg__random_state': None,\n 'logreg__solver': 'lbfgs',\n 'logreg__tol': 0.0001,\n 'logreg__verbose': 0,\n 'logreg__warm_start': False}"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Now set the values for the parameters in the grid. Define a dictionary for the same"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n                'svd__n_components': [120, 180],\n                'logreg__C': [0.1, 1.0, 10],\n                'logreg__penalty': ['l2', 'l1']\n            }","execution_count":23,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When using Grid Search,\n1. To find the best score based on the scoring function: ** model.best_score_**\n2. Best set of parameters: ** model.best_estimator_.get_params() **"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# Intialize GridSearch Model\n\nfrom sklearn.model_selection import GridSearchCV\n\n# setting refit = True, takes the best parameters when traininf from the folds and retrains the model on the entire data using those best parameters\nmodel = GridSearchCV(estimator = pipe1, param_grid = param_grid, scoring = logloss_scorer, cv = 2, refit= True, verbose=10)\n\n# fit the gridsearch model. We can fit on the entire train, but here I will use only X_train with tf idf\nmodel.fit(X_train_tfv, y_train)\n\nprint('Best score is : %0.3f' % model.best_score_)\nprint('Best parameters set:')\nbest_parameters = model.best_estimator_.get_params()\n\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" %(param_name, best_parameters[param_name]))","execution_count":24,"outputs":[{"output_type":"stream","text":"Fitting 2 folds for each of 12 candidates, totalling 24 fits\n[CV] logreg__C=0.1, logreg__penalty=l2, svd__n_components=120 ........\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","name":"stderr"},{"output_type":"stream","text":"[CV]  logreg__C=0.1, logreg__penalty=l2, svd__n_components=120, score=-0.745, total=   3.1s\n[CV] logreg__C=0.1, logreg__penalty=l2, svd__n_components=120 ........\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.1s remaining:    0.0s\n","name":"stderr"},{"output_type":"stream","text":"[CV]  logreg__C=0.1, logreg__penalty=l2, svd__n_components=120, score=-0.739, total=   3.2s\n[CV] logreg__C=0.1, logreg__penalty=l2, svd__n_components=180 ........\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    6.4s remaining:    0.0s\n","name":"stderr"},{"output_type":"stream","text":"[CV]  logreg__C=0.1, logreg__penalty=l2, svd__n_components=180, score=-0.685, total=   5.0s\n[CV] logreg__C=0.1, logreg__penalty=l2, svd__n_components=180 ........\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   11.3s remaining:    0.0s\n","name":"stderr"},{"output_type":"stream","text":"[CV]  logreg__C=0.1, logreg__penalty=l2, svd__n_components=180, score=-0.682, total=   4.9s\n[CV] logreg__C=0.1, logreg__penalty=l1, svd__n_components=120 ........\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   16.2s remaining:    0.0s\n","name":"stderr"},{"output_type":"stream","text":"[CV]  logreg__C=0.1, logreg__penalty=l1, svd__n_components=120, score=nan, total=   3.1s\n[CV] logreg__C=0.1, logreg__penalty=l1, svd__n_components=120 ........\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   19.4s remaining:    0.0s\n","name":"stderr"},{"output_type":"stream","text":"[CV]  logreg__C=0.1, logreg__penalty=l1, svd__n_components=120, score=nan, total=   3.1s\n[CV] logreg__C=0.1, logreg__penalty=l1, svd__n_components=180 ........\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:   22.4s remaining:    0.0s\n","name":"stderr"},{"output_type":"stream","text":"[CV]  logreg__C=0.1, logreg__penalty=l1, svd__n_components=180, score=nan, total=   4.6s\n[CV] logreg__C=0.1, logreg__penalty=l1, svd__n_components=180 ........\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:   27.0s remaining:    0.0s\n","name":"stderr"},{"output_type":"stream","text":"[CV]  logreg__C=0.1, logreg__penalty=l1, svd__n_components=180, score=nan, total=   4.7s\n[CV] logreg__C=1.0, logreg__penalty=l2, svd__n_components=120 ........\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   31.7s remaining:    0.0s\n","name":"stderr"},{"output_type":"stream","text":"[CV]  logreg__C=1.0, logreg__penalty=l2, svd__n_components=120, score=-0.732, total=   3.2s\n[CV] logreg__C=1.0, logreg__penalty=l2, svd__n_components=120 ........\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   34.9s remaining:    0.0s\n","name":"stderr"},{"output_type":"stream","text":"[CV]  logreg__C=1.0, logreg__penalty=l2, svd__n_components=120, score=-0.737, total=   3.2s\n[CV] logreg__C=1.0, logreg__penalty=l2, svd__n_components=180 ........\n[CV]  logreg__C=1.0, logreg__penalty=l2, svd__n_components=180, score=-0.683, total=   4.9s\n[CV] logreg__C=1.0, logreg__penalty=l2, svd__n_components=180 ........\n[CV]  logreg__C=1.0, logreg__penalty=l2, svd__n_components=180, score=-0.674, total=   4.9s\n[CV] logreg__C=1.0, logreg__penalty=l1, svd__n_components=120 ........\n[CV]  logreg__C=1.0, logreg__penalty=l1, svd__n_components=120, score=nan, total=   3.2s\n[CV] logreg__C=1.0, logreg__penalty=l1, svd__n_components=120 ........\n[CV]  logreg__C=1.0, logreg__penalty=l1, svd__n_components=120, score=nan, total=   3.1s\n[CV] logreg__C=1.0, logreg__penalty=l1, svd__n_components=180 ........\n[CV]  logreg__C=1.0, logreg__penalty=l1, svd__n_components=180, score=nan, total=   4.7s\n[CV] logreg__C=1.0, logreg__penalty=l1, svd__n_components=180 ........\n[CV]  logreg__C=1.0, logreg__penalty=l1, svd__n_components=180, score=nan, total=   4.7s\n[CV] logreg__C=10, logreg__penalty=l2, svd__n_components=120 .........\n[CV]  logreg__C=10, logreg__penalty=l2, svd__n_components=120, score=-0.734, total=   3.2s\n[CV] logreg__C=10, logreg__penalty=l2, svd__n_components=120 .........\n[CV]  logreg__C=10, logreg__penalty=l2, svd__n_components=120, score=-0.727, total=   3.2s\n[CV] logreg__C=10, logreg__penalty=l2, svd__n_components=180 .........\n[CV]  logreg__C=10, logreg__penalty=l2, svd__n_components=180, score=-0.689, total=   4.9s\n[CV] logreg__C=10, logreg__penalty=l2, svd__n_components=180 .........\n[CV]  logreg__C=10, logreg__penalty=l2, svd__n_components=180, score=-0.683, total=   4.9s\n[CV] logreg__C=10, logreg__penalty=l1, svd__n_components=120 .........\n[CV]  logreg__C=10, logreg__penalty=l1, svd__n_components=120, score=nan, total=   3.1s\n[CV] logreg__C=10, logreg__penalty=l1, svd__n_components=120 .........\n[CV]  logreg__C=10, logreg__penalty=l1, svd__n_components=120, score=nan, total=   3.1s\n[CV] logreg__C=10, logreg__penalty=l1, svd__n_components=180 .........\n[CV]  logreg__C=10, logreg__penalty=l1, svd__n_components=180, score=nan, total=   4.9s\n[CV] logreg__C=10, logreg__penalty=l1, svd__n_components=180 .........\n[CV]  logreg__C=10, logreg__penalty=l1, svd__n_components=180, score=nan, total=   4.9s\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:  1.6min finished\n","name":"stderr"},{"output_type":"stream","text":"Best score is : -0.679\nBest parameters set:\n\tlogreg__C: 1.0\n\tlogreg__penalty: 'l2'\n\tsvd__n_components: 180\nCPU times: user 2min 24s, sys: 31.3 s, total: 2min 55s\nWall time: 1min 42s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"We will do the same using MultinomialNB model on tf-idf data"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nnb_model = MultinomialNB()\n\n# create a pipeline\n\npipe2 = Pipeline([('nb', nb_model)])\n\n# parameter grid\n# try with the alpha parameter\n# pipe2.get_params()\n\nparam_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n# intialize the Grid SearchCV model\nmodel = GridSearchCV(estimator = pipe2, param_grid = param_grid, cv = 2, refit = True, scoring = logloss_scorer, verbose=10, n_jobs=-1)\n\nmodel.fit(X_train_tfv, y_train)\nprint(\"best score is :\",format(model.best_score_))\nprint('best parameters are: ')\nbest_parameters = model.best_estimator_.get_params()\n\nfor param_name in sorted(param_grid.keys()):\n    print('\\t%s: %r' %(param_name, best_parameters[param_name]))","execution_count":25,"outputs":[{"output_type":"stream","text":"Fitting 2 folds for each of 6 candidates, totalling 12 fits\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n","name":"stderr"},{"output_type":"stream","text":"best score is : -0.4518777178062823\nbest parameters are: \n\tnb__alpha: 0.1\nCPU times: user 232 ms, sys: 112 ms, total: 344 ms\nWall time: 1.32 s\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    1.2s\n[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:    1.2s remaining:    0.8s\n[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    1.2s remaining:    0.4s\n[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    1.3s finished\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"## Word vectors\nCreate word embeddings for tokens as it is customary in NLP tasks, which will give lot more insight into our data.\nHere we will create `sentence vectors` and use them as inputs to a machine learning model. This can be implementes using different approaches such as:\n1. Word2Vec\n2. GloVe\n3. FastText\n\nThe pickled version of GloVe is available [here](https://www.kaggle.com/authman/pickled-glove840b300d-for-10sec-loading)\n\nGlove is a dictionary with the keys as the words or tokens and the values being the corresponding vector representation or word embeddings. In this version of glove the emebdding size is 300, which means for every key in this glove dictionary, the corresponding vector representation is a 300 D array"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\n# start = time.time()\n# with open('../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl', 'rb') as fp:\n#     glove = pickle.load(fp)\n# print('time taken to load the GloVe is {0:.1f} seconds:'.format(time.time()- start))\n\n# print('Length of the glove', len(glove))\n# print(type(glove))\n\n# print(list(glove.keys())[:10])\n# # word embedding for any key can be found out as \n# glove['the']\n\n# del glove\n# gc.collect()","execution_count":26,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am using the **Glove vectors** available from Standford NLP which is available [here](http://www-nlp.stanford.edu/data/glove.840B.300d.zip)\n\nClick that link to download the Glove vectors which are available in text format."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# load the GloVe vectors into a dictionary\n\n# store the word embeddings\nembedding_index ={}\n\n# create file pointer\nfp = open(r'../input/glove-840b300d-dj/glove.840B.300d.txt', 'r', encoding='utf8', errors='ignore')\n\n# read line by line from the text file\nfor line in tqdm(fp):\n    values = line.split()\n    word = ''.join(values[:-300])\n    #word = values[:-300][0] -- can use either. This is done to remove brackets as the line split results in array\n    # create an array to store the embeddings for the word\n    embed_vec = np.asarray(values[-300:], dtype= 'float32')\n    embedding_index[word] = embed_vec\n\nfp.close()\n\nprint('Found {} word vectors'.format(len(embedding_index)))","execution_count":27,"outputs":[{"output_type":"stream","text":"2196017it [05:42, 6411.94it/s]","name":"stderr"},{"output_type":"stream","text":"Found 2195892 word vectors\nCPU times: user 5min 39s, sys: 22.3 s, total: 6min 1s\nWall time: 5min 42s\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"Save the embedding dictonary for later use"},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle.dump(embedding_index, open('embedding_glove_dict.pkl', 'wb'))","execution_count":28,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a normalized vector for the entire sentence in the given text, based on the word embeddings from glove.\nUsing the `L2 norm` for normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstopwords = stopwords.words('english')","execution_count":29,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Find the sentence embedding by the find the word embeddings with each sentence and normalizing them by their L2 norm"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sent2vec(s):\n    sent = str(s).lower()\n    words = word_tokenize(sent)\n    words = [w for w in words if not w in stopwords]\n    words = [w for w in words if w.isalpha()]\n    \n    embeddings_found = []\n    \n    # store the embeddings\n    for w in words:\n        try:\n            embeddings_found.append(embedding_index[w])\n        except:\n            continue\n    \n    # convert to numpy array to perform array computation\n    embeddings_found = np.array(embeddings_found)\n    row_sum = embeddings_found.sum(axis=0)\n    \n    # if cannot find the word, fill with zeros\n    if type(row_sum) != np.ndarray:\n        return np.zeros(300)\n    else:\n        # square root of the sum of squares\n        l2_norm = np.sqrt((row_sum ** 2).sum())\n        #normalize by l2 norm\n        return row_sum / l2_norm","execution_count":30,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create sentence vector for both train and validations sets. What is needed is an array of (number of sentences X 300). 300 is the embedding size"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Can be implemented using the below as well\n\nx_train_glove = X_train.apply(sent2vec)\nx_valid_glove = X_valid.apply(sent2vec)\n\n# x_train_glove = [sent2vec(sent) for sent in tqdm(X_train)]\n# x_valid_glove = [sent2vec(sent) for sent in tqdm(X_valid)]\n\nprint(len(x_train_glove))\nprint(len(x_valid_glove))","execution_count":31,"outputs":[{"output_type":"stream","text":"17621\n1958\nCPU times: user 11.5 s, sys: 0 ns, total: 11.5 s\nWall time: 11.6 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert to array format to use in machine learning model\nx_train_glove = np.array(x_train_glove.values.tolist())\nx_valid_glove = np.array(x_valid_glove.values.tolist())\n\n# use below if using uncommenting the lines in the previous cell\n# x_train_glove = np.array(x_train_glove)\n# x_valid_glove = np.array(x_valid_glove)\n\nprint(x_train_glove.shape)\nprint(x_valid_glove.shape)","execution_count":32,"outputs":[{"output_type":"stream","text":"(17621, 300)\n(1958, 300)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Fit the xgboost model on the glove features"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nxgb_clf = xgb.XGBClassifier(nthread=-1, silent=False) ## silent controls verbosity\nxgb_clf.fit(x_train_glove, y_train)\npredictions = xgb_clf.predict_proba(x_valid_glove)\nprint('Log loss using Xgboost on the Glove vectors for sentences is : {:.3f}'.format(metrics.log_loss(y_valid, prediction)))","execution_count":33,"outputs":[{"output_type":"stream","text":"Log loss using Xgboost on the Glove vectors for sentences is : 0.853\nCPU times: user 4min 27s, sys: 0 ns, total: 4min 27s\nWall time: 4min 27s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"With some tuning of hyperparameters, rerun the xgboost classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using another xgboost model that was used earlier\nxgb_clf = xgb.XGBClassifier(n_estimators = 200, max_depth =7, learning_rate = 0.1, verbose = 2, colsample_bytree = 0.8, \n                            subsample =0.8, n_jobs=-1, nthread=10)\n\nxgb_clf.fit(x_train_glove, y_train)\npredictions = xgb_clf.predict_proba(x_valid_glove)\nprint('Log loss using Xgboost on the Glove vectors for sentences is : {:.3f}'.format(metrics.log_loss(y_valid, prediction)))","execution_count":34,"outputs":[{"output_type":"stream","text":"Log loss using Xgboost on the Glove vectors for sentences is : 0.853\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Deep learning models\nLets use some deep learning models to improve the score\n- We will train a `LSTM` and a simple `Dense network` on the `Glove features`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale the data before using in a neural network\nstd_scal = preprocessing.StandardScaler()\n\nx_train_glove_scl = std_scal.fit_transform(x_train_glove)\nx_valid_glove_scl = std_scal.transform(x_valid_glove)","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# binarize the labels into vector representations before using in the neural network\nfrom keras.utils import np_utils\n\ny_train_encd = np_utils.to_categorical(y_train)\ny_valid_encd = np_utils.to_categorical(y_valid)\n\nprint('Output after encoding the labels\\n', y_valid_encd)","execution_count":36,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"Output after encoding the labels\n [[0. 1. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n ...\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 1. 0.]]\nCPU times: user 2.77 s, sys: 599 ms, total: 3.37 s\nWall time: 7.63 s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Creating a simple **Sequential Neural Network** with 3 layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the layes from keras\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.embeddings import Embedding\n\n\nmodel= Sequential()\n\nmodel.add(Dense(300, input_dim=300, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(300, input_dim=300, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\n\n\nprint(model.summary())\n\n# compile the model\nmodel.compile(optimizer = 'adam', loss='categorical_crossentropy')","execution_count":37,"outputs":[{"output_type":"stream","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 300)               90300     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 300)               0         \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 300)               1200      \n_________________________________________________________________\ndense_2 (Dense)              (None, 300)               90300     \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 300)               0         \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 300)               1200      \n_________________________________________________________________\ndense_3 (Dense)              (None, 3)                 903       \n_________________________________________________________________\nactivation_1 (Activation)    (None, 3)                 0         \n=================================================================\nTotal params: 183,903\nTrainable params: 182,703\nNon-trainable params: 1,200\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# fit the model\nmodel.fit(x_train_glove_scl, y_train_encd, batch_size=64, epochs =5, verbose=1,\n         validation_data = (x_valid_glove_scl, y_valid_encd))","execution_count":38,"outputs":[{"output_type":"stream","text":"Train on 17621 samples, validate on 1958 samples\nEpoch 1/5\n17621/17621 [==============================] - 3s 198us/step - loss: 0.9021 - val_loss: 0.6942\nEpoch 2/5\n17621/17621 [==============================] - 2s 129us/step - loss: 0.6941 - val_loss: 0.6613\nEpoch 3/5\n17621/17621 [==============================] - 2s 125us/step - loss: 0.6384 - val_loss: 0.6494\nEpoch 4/5\n17621/17621 [==============================] - 3s 143us/step - loss: 0.5937 - val_loss: 0.6430\nEpoch 5/5\n17621/17621 [==============================] - 3s 154us/step - loss: 0.5594 - val_loss: 0.6370\nCPU times: user 26.1 s, sys: 3.48 s, total: 29.6 s\nWall time: 14.5 s\n","name":"stdout"},{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"<keras.callbacks.callbacks.History at 0x7f9734b38710>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"The scores for a simple Dense network can be further improved by adding more layers and increase drop outs. In this case a simple NN is able to get better results than xgboost"},{"metadata":{},"cell_type":"markdown","source":"### Using LSTMs\n- To use LSTMs, we need to tokenize  the text data"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom keras.preprocessing import text, sequence\n\nmax_words = None\n\n# the max length for a sentence is, if more needs to be truncated else padded\nmax_len = 70\n\ntokenizer = text.Tokenizer(num_words = max_words)\ntokenizer.fit_on_texts(list(X_train) + list(X_valid))\n\n# seqeunce of numbers associated with the indexes\nxtrain_seq = tokenizer.texts_to_sequences(list(X_train))\nxvalid_seq = tokenizer.texts_to_sequences(list(X_valid))\n\n# zero pad the sentences\nxtrain_padded = sequence.pad_sequences(xtrain_seq, maxlen = max_len)\nxvalid_padded = sequence.pad_sequences(xvalid_seq, maxlen = max_len)\n\n# mapping of word to index\nword_index = tokenizer.word_index\n\nprint('Number of tokens from the text:', len(word_index))","execution_count":39,"outputs":[{"output_type":"stream","text":"Number of tokens from the text: 25943\nCPU times: user 2.17 s, sys: 5.55 ms, total: 2.18 s\nWall time: 2.17 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_valid.values[0])\nprint(xvalid_seq[0])","execution_count":40,"outputs":[{"output_type":"stream","text":"\"I guess he's sayin' the spell,\" whispered Wheeler as he snatched back the telescope.\n[6, 1249, 2555, 15996, 1, 2814, 1165, 7714, 16, 13, 4834, 165, 1, 2635]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Next create an `embedding matrix` for the words we have in the dataset\n. This will be mapping of the word index and its word embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# each token is represented by a 300D vector or word embedding which was derived from GloVe\n# initialize the matrix with zeros\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\n\nfor word, index in tqdm(word_index.items()):\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","execution_count":41,"outputs":[{"output_type":"stream","text":"100%|██████████| 25943/25943 [00:00<00:00, 189399.79it/s]","name":"stderr"},{"output_type":"stream","text":"CPU times: user 191 ms, sys: 5.03 ms, total: 196 ms\nWall time: 195 ms\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"Implement a LSTM with glove embeddings and two dense layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import SpatialDropout1D\nfrom keras.layers.recurrent import GRU, LSTM","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nmodel = Sequential()\n\n# model.add(Embedding(vocabulary size, hidden_size, input_length=num_steps))\nmodel.add(\n        Embedding(len(word_index)+1, \n                  300,\n                  weights = [embedding_matrix],\n                  input_length = max_len, # the max length of each sequence\n                  trainable = False\n                ))\n\n# spatial dropouts implement dropout across a particular channel\nmodel.add(SpatialDropout1D(0.3))\n\n# model.add(LSTM(hidden_size, return_sequences=True, dropout = , recurrent_dropout =))\nmodel.add(LSTM(100, dropout = 0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n\nprint('Model summary', model.summary())","execution_count":43,"outputs":[{"output_type":"stream","text":"Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 70, 300)           7783200   \n_________________________________________________________________\nspatial_dropout1d_1 (Spatial (None, 70, 300)           0         \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 100)               160400    \n_________________________________________________________________\ndense_4 (Dense)              (None, 1024)              103424    \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 1024)              0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 1024)              1049600   \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 1024)              0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 3)                 3075      \n_________________________________________________________________\nactivation_2 (Activation)    (None, 3)                 0         \n=================================================================\nTotal params: 9,099,699\nTrainable params: 1,316,499\nNon-trainable params: 7,783,200\n_________________________________________________________________\nModel summary None\nCPU times: user 869 ms, sys: 11.2 ms, total: 881 ms\nWall time: 499 ms\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel.fit(xtrain_padded, y_train_encd, batch_size=512, epochs=5, verbose=1, validation_data=[xvalid_padded, y_valid_encd] )","execution_count":44,"outputs":[{"output_type":"stream","text":"Train on 17621 samples, validate on 1958 samples\nEpoch 1/5\n17621/17621 [==============================] - 22s 1ms/step - loss: 1.0724 - val_loss: 0.9561\nEpoch 2/5\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.9474 - val_loss: 0.8187\nEpoch 3/5\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.8746 - val_loss: 0.7702\nEpoch 4/5\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.8391 - val_loss: 0.7403\nEpoch 5/5\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.8202 - val_loss: 0.7351\nCPU times: user 5min 6s, sys: 25.9 s, total: 5min 32s\nWall time: 1min 43s\n","name":"stdout"},{"output_type":"execute_result","execution_count":44,"data":{"text/plain":"<keras.callbacks.callbacks.History at 0x7f9739beba90>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"May need to run this for more epochs, but I will use `early stopping` to stop if there is no improvment in the `loss`\n.Compile the model again"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\n#?EarlyStopping","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nmodel = Sequential()\n\n# model.add(Embedding(vocabulary size, hidden_size, input_length=num_steps))\nmodel.add(\n        Embedding(len(word_index)+1, \n                  300,\n                  weights = [embedding_matrix],\n                  input_length = max_len, # the max length of each sequence\n                  trainable = False\n                ))\n\n# spatial dropouts implement dropout across a particular channel\nmodel.add(SpatialDropout1D(0.3))\n\n# model.add(LSTM(hidden_size, return_sequences=True, dropout = , recurrent_dropout =))\nmodel.add(LSTM(100, dropout = 0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n\n# implement early stopping callback\n\nearlystop = EarlyStopping(monitor = 'val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n\nmodel.fit(xtrain_padded, y_train_encd, batch_size=512, epochs=20, verbose=1, validation_data=[xvalid_padded, y_valid_encd],\n         callbacks=[earlystop])","execution_count":46,"outputs":[{"output_type":"stream","text":"Train on 17621 samples, validate on 1958 samples\nEpoch 1/20\n17621/17621 [==============================] - 23s 1ms/step - loss: 1.0821 - val_loss: 0.9902\nEpoch 2/20\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.9502 - val_loss: 0.8129\nEpoch 3/20\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.8704 - val_loss: 0.7654\nEpoch 4/20\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.8425 - val_loss: 0.7496\nEpoch 5/20\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.8170 - val_loss: 0.7284\nEpoch 6/20\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.7935 - val_loss: 0.7012\nEpoch 7/20\n17621/17621 [==============================] - 21s 1ms/step - loss: 0.7753 - val_loss: 0.6883\nEpoch 8/20\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.7626 - val_loss: 0.6884\nEpoch 9/20\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.7568 - val_loss: 0.6737\nEpoch 10/20\n17621/17621 [==============================] - 21s 1ms/step - loss: 0.7441 - val_loss: 0.6555\nEpoch 11/20\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.7228 - val_loss: 0.6529\nEpoch 12/20\n17621/17621 [==============================] - 24s 1ms/step - loss: 0.7032 - val_loss: 0.6386\nEpoch 13/20\n17621/17621 [==============================] - 22s 1ms/step - loss: 0.6901 - val_loss: 0.6244\nEpoch 14/20\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.6716 - val_loss: 0.6077\nEpoch 15/20\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.6570 - val_loss: 0.6125\nEpoch 16/20\n17621/17621 [==============================] - 22s 1ms/step - loss: 0.6459 - val_loss: 0.6026\nEpoch 17/20\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.6282 - val_loss: 0.5938\nEpoch 18/20\n17621/17621 [==============================] - 21s 1ms/step - loss: 0.6113 - val_loss: 0.5923\nEpoch 19/20\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.6026 - val_loss: 0.5862\nEpoch 20/20\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.5935 - val_loss: 0.5684\nCPU times: user 20min 34s, sys: 1min 37s, total: 22min 12s\nWall time: 6min 54s\n","name":"stdout"},{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"<keras.callbacks.callbacks.History at 0x7f974d975710>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Using Bidirectional LSTMs\n\nUse a simpe bidirectional LSTM with glove embeddings and 2 dense layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(\n            Embedding(input_dim = len(word_index)+1,\n                      output_dim =300, \n                      weights = [embedding_matrix],\n                      input_length = max_len,\n                      trainable=False\n                    ))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\nmodel.fit(xtrain_padded, y_train_encd, batch_size=512, epochs=20, verbose=1, validation_data=[xvalid_padded, y_valid_encd],\n         callbacks=[earlystop])","execution_count":55,"outputs":[{"output_type":"stream","text":"Train on 17621 samples, validate on 1958 samples\nEpoch 1/20\n17621/17621 [==============================] - 20s 1ms/step - loss: 1.0773 - val_loss: 0.9829\nEpoch 2/20\n17621/17621 [==============================] - 19s 1ms/step - loss: 0.9535 - val_loss: 0.8214\nEpoch 3/20\n17621/17621 [==============================] - 19s 1ms/step - loss: 0.8775 - val_loss: 0.7671\nEpoch 4/20\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.8475 - val_loss: 0.7453\nEpoch 5/20\n17621/17621 [==============================] - 21s 1ms/step - loss: 0.8207 - val_loss: 0.7364\nEpoch 6/20\n17621/17621 [==============================] - 19s 1ms/step - loss: 0.8051 - val_loss: 0.7122\nEpoch 7/20\n17621/17621 [==============================] - 19s 1ms/step - loss: 0.7818 - val_loss: 0.6903\nEpoch 8/20\n17621/17621 [==============================] - 19s 1ms/step - loss: 0.7687 - val_loss: 0.6836\nEpoch 9/20\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.7526 - val_loss: 0.6717\nEpoch 10/20\n17621/17621 [==============================] - 19s 1ms/step - loss: 0.7384 - val_loss: 0.6761\nEpoch 11/20\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.7209 - val_loss: 0.6493\nEpoch 12/20\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.7145 - val_loss: 0.6371\nEpoch 13/20\n17621/17621 [==============================] - 19s 1ms/step - loss: 0.6905 - val_loss: 0.6262\nEpoch 14/20\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.6723 - val_loss: 0.6069\nEpoch 15/20\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.6638 - val_loss: 0.6179\nEpoch 16/20\n17621/17621 [==============================] - 19s 1ms/step - loss: 0.6455 - val_loss: 0.5975\nEpoch 17/20\n17621/17621 [==============================] - 19s 1ms/step - loss: 0.6358 - val_loss: 0.5984\nEpoch 18/20\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.6188 - val_loss: 0.5807\nEpoch 19/20\n17621/17621 [==============================] - 19s 1ms/step - loss: 0.6038 - val_loss: 0.5632\nEpoch 20/20\n17621/17621 [==============================] - 20s 1ms/step - loss: 0.5895 - val_loss: 0.5610\n","name":"stdout"},{"output_type":"execute_result","execution_count":55,"data":{"text/plain":"<keras.callbacks.callbacks.History at 0x7f972e5d5bd0>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Using GRU(gated recurrent unit) with 2 layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nmodel = Sequential()\nmodel.add(\n            Embedding(\n                        input_dim = len(word_index)+1,\n                        output_dim = 300,\n                        weights = [embedding_matrix],\n                        input_length = max_len,\n                        trainable=False\n            ))\n\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# fit the model with early stoppping\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n\nmodel.fit(xtrain_padded, y_train_encd, batch_size=512, epochs=20, verbose=1, validation_data=[xvalid_padded, y_valid_encd],\n         callbacks=[earlystop])","execution_count":56,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"positional argument follows keyword argument (<unknown>, line 5)","traceback":["Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n","  File \u001b[1;32m\"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3331\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \u001b[1;32m\"<ipython-input-56-7d75db4c5b00>\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    get_ipython().run_cell_magic('time', '', \"\\nmodel = Sequential()\\nmodel.add(\\n            Embedding(\\n                        input_dim = len(word_index)+1,\\n                        300,\\n                        weights = [embedding_matrix],\\n                        input_length = max_len,\\n                        trainable=False\\n            ))\\n\\nmodel.add(SpatialDropout1D(0.3))\\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\\n\\nmodel.add(Dense(1024, activation='relu'))\\nmodel.add(Dropout(0.8))\\n\\nmodel.add(Dense(1024, activation='relu'))\\nmodel.add(Dropout(0.8))\\n\\nmodel.add(Dense(3))\\nmodel.add(Activation('softmax'))\\n\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\n\\n# fit the model with early stoppping\\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\\n\\nmodel.fit(xtrain_padded, y_train_encd, batch_size=512, epochs=20, verbose=1, validation_data=[xvalid_padded, y_valid_encd],\\n         callbacks=[earlystop])\\n\")\n","  File \u001b[1;32m\"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2362\u001b[0m, in \u001b[1;35mrun_cell_magic\u001b[0m\n    result = fn(*args, **kwargs)\n","  File \u001b[1;32m\"<decorator-gen-61>\"\u001b[0m, line \u001b[1;32m2\u001b[0m, in \u001b[1;35mtime\u001b[0m\n","  File \u001b[1;32m\"/opt/conda/lib/python3.7/site-packages/IPython/core/magic.py\"\u001b[0m, line \u001b[1;32m187\u001b[0m, in \u001b[1;35m<lambda>\u001b[0m\n    call = lambda f, *a, **k: f(*a, **k)\n","  File \u001b[1;32m\"/opt/conda/lib/python3.7/site-packages/IPython/core/magics/execution.py\"\u001b[0m, line \u001b[1;32m1268\u001b[0m, in \u001b[1;35mtime\u001b[0m\n    expr_ast = self.shell.compile.ast_parse(expr)\n","\u001b[0;36m  File \u001b[0;32m\"/opt/conda/lib/python3.7/site-packages/IPython/core/compilerop.py\"\u001b[0;36m, line \u001b[0;32m101\u001b[0;36m, in \u001b[0;35mast_parse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\u001b[0m\n","\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}