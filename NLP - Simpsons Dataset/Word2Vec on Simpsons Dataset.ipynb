{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/dialogue-lines-of-the-simpsons/simpsons_dataset.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re                 # regex text preprocessing\nimport time\nfrom collections import Counter, defaultdict, OrderedDict # for word frequency\n\nimport spacy # open source tool for NLP\n\nimport multiprocessing\nprint('Number of cores in this machine: ', multiprocessing.cpu_count())\n\n# import logging  # setting up logging to monitor gensim\n# logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt = '%H:%M:%S', level= logging.INFO)\n\nimport logging  # Setting up the loggings to monitor gensim\nlogging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)","execution_count":2,"outputs":[{"output_type":"stream","text":"Number of cores in this machine:  4\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 1. Getting the dataset and preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/dialogue-lines-of-the-simpsons/simpsons_dataset.csv')\nprint(df.shape)\ndisplay(df.head())","execution_count":3,"outputs":[{"output_type":"stream","text":"(158314, 2)\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"        raw_character_text                                       spoken_words\n0              Miss Hoover  No, actually, it was a little of both. Sometim...\n1             Lisa Simpson                             Where's Mr. Bergstrom?\n2              Miss Hoover  I don't know. Although I'd sure like to talk t...\n3             Lisa Simpson                         That life is worth living.\n4  Edna Krabappel-Flanders  The polls will be open from now until the end ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>raw_character_text</th>\n      <th>spoken_words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Miss Hoover</td>\n      <td>No, actually, it was a little of both. Sometim...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Lisa Simpson</td>\n      <td>Where's Mr. Bergstrom?</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Miss Hoover</td>\n      <td>I don't know. Although I'd sure like to talk t...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Lisa Simpson</td>\n      <td>That life is worth living.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Edna Krabappel-Flanders</td>\n      <td>The polls will be open from now until the end ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"raw_character_text    17814\nspoken_words          26459\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop null values\n\ndf = df.dropna().reset_index(drop=True)\ndf.isnull().sum()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"raw_character_text    0\nspoken_words          0\ndtype: int64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## 2. Text preprocessing"},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Cleaning text\nRemove stop words, non-alphabetic characters and lemmatize each line of dialogue\n\nImplement [spaCy pipelines](https://spacy.io/usage/processing-pipelines) for faster processing\n* The pipeline used by the default models consists of a tagger, a parser and an entity recognizer\n* pipeline components can be disabled for faster processing"},{"metadata":{},"cell_type":"markdown","source":"Using `spaCy` model to first clean the text"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load spacy without 'Named Entity Recognition' and 'parser', for speed\nnlp = spacy.load('en', disable =['ner', 'parser'])\n\ndef cleaning(doc):\n    # lemmatize and remove stop words\n    # for this doc has to be a 'spacy object'\n    text = [token.lemma_ for token in doc if not token.is_stop]\n    \n    # word2vec uses context words to learn vector representations of target word\n    # the benefit of training is small when the sentence is only 1 or 2 words long\n    \n    if len(text) > 2:\n        return ' '.join(text)","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note: Once you convert the generator to a list, then the generator will not return any further items,which is why I have commented the line:\nMore details [here](https://stackoverflow.com/questions/24130745/convert-generator-object-to-list-for-debugging)\n> list(brief_cleaning)[:5]"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove non-alphabetic characters\n# save the result as a 'generator' on which we can iterate\nbrief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['spoken_words'])\n#list(brief_cleaning)[:5]","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taking advantage of spaCy.pipe() attribute to speed-up the cleaning process\n# convert the doc into a spaCy object on which cleaning method can be applied\n\nstart = time.time()\ntext  = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size= 5000, n_threads =-1)]\nprint('time taken to clean text is {} min'.format((time.time() - start) / 60))","execution_count":8,"outputs":[{"output_type":"stream","text":"time taken to clean text is 1.27745174964269 min\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Put the result into a dataframe to remove 'None' and duplicates"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = pd.DataFrame(text)\ndf_clean.columns=['cleaned_text']\ndf_clean = df_clean.dropna().drop_duplicates()\nprint('Shape after cleaning', df_clean.shape)\ndf_clean.head(5)","execution_count":9,"outputs":[{"output_type":"stream","text":"Shape after cleaning (85964, 1)\n","name":"stdout"},{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"                                        cleaned_text\n0  actually little disease magazine news show nat...\n2        know sure like talk touch lesson plan teach\n3                                    life worth live\n4  poll open end recess case decide thought final...\n7                                victory party slide","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cleaned_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>actually little disease magazine news show nat...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>know sure like talk touch lesson plan teach</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>life worth live</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>poll open end recess case decide thought final...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>victory party slide</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Bigrams\nUse `Gensim Phrases` package to automatically detect common phrases(bigrams) from the list of dialogues. https://radimrehurek.com/gensim/models/phrases.html\nThe main reason we are doing this here is to catch phrases such as \"mr_burns\", \"bart_simpson\""},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models.phrases import Phrases, Phraser","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training corpus must be a sequence (stream, generator) of sentences,\nwith each sentence a list of tokens"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nsent_list = [row.split() for row in df_clean['cleaned_text']]\n\n# create phrases using Gensim Phrases\nphrases = Phrases(sent_list, min_count=30, progress_per=10000)","execution_count":11,"outputs":[{"output_type":"stream","text":"CPU times: user 1.34 s, sys: 32.8 ms, total: 1.37 s\nWall time: 1.37 s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"The goal of `Phraser()` is to cut down memory consumption of `Phrases()`, by discarding model state not strictly needed for the bigram detection task:"},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram = Phraser(phrases)","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transform the corpus based on the bigrams detected:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = bigram[sent_list]","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigrams_created = []\nfor sent in sentences:\n    for word in sent:\n        if '_' in word:\n            bigrams_created.append(word)\nprint('bigrams created using Gensim Phrases: \\n\\n', set(bigrams_created))","execution_count":14,"outputs":[{"output_type":"stream","text":"bigrams created using Gensim Phrases: \n\n {'nyah_nyah', 'lady_gentleman', 'ha_ha', 'power_plant', 'credit_card', \"talkin_'\", 'high_school', \"gettin_'\", 'mom_dad', 'try_kill', 'doo_doo', 'ding_ding', 'year_ago', 'springfield_elementary', 't_shirt', 'tap_tap', 'santa_little', 'na_na', 'principal_skinner', 'montgomery_burn', 'capital_city', 'little_bit', 'young_lady', 'disco_stu', 'chief_wiggum', 'good_luck', 'tell_truth', 'homer_simpson', 'ice_cream', 'kwik_e', 'important_thing', 'da_da', 'new_york', 'mr_burns', 'big_deal', 'old_fashioned', 'world_war', 'hot_dog', 'sideshow_bob', 'oh_god', 'whoa_whoa', 'bart_simpson', 'mmm_hmm', 'dear_lord', 'fall_asleep', 'smell_like', 'old_man', 'wait_minute', 'sound_like', 'fat_tony', 'mrs_krabappel', 'la_la', 'yi_yi', 'u_s', 'uh_huh', 'got_to', 'mr_simpson', 'bad_news', \"'_n\", 'good_morning', 'cell_phone', 'ho_ho', 'patty_selma', \"'_til\", 'lisa_simpson', 'pork_chop', 'get_to', 'bye_bye', 'miss_hoover', 'wait_wait', 'van_houten', 'ow_ow', 'good_friend', 'mrs_simpson', 'happy_birthday', 'haw_haw', 'little_helper', 'field_trip', 'thirty_year', 'comic_book', 'super_bowl', 'rest_life', 'hee_hee', 'itchy_scratchy', 'nuclear_power', 'ooh_ooh', 'fourth_grade', 'god_bless', 'radioactive_man', 'frontline_infantry', 'watch_tv', 'go_to', \"tryin_'\", \"workin_'\", 'li_li', 'million_dollar', 'blah_blah', 'united_states', 'troy_mcclure', 'little_girl', 'ba_ba', 'young_man', \"makin_'\", 'ned_flander', 'yo_yo', 'yes_yes', 'woo_hoo', \"comin_'\", 'year_old', 'thank_god', 'thousand_dollar', 'krusty_clown', 'dr_hibbert', 'duck_duck', 'run_away', 'merry_christmas', 'kent_brockman', 'nuclear_plant', 'heh_heh', \"'_tis\", 'malibu_stacy', 'comic_strip', 'good_lord', \"lookin_'\", 'pay_attention'}\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Most frequent words\nSanity check for effectiveness of:\n- lemmatization\n- removal of stop words\n- addition of bigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"word_freq = defaultdict(int)\nfor sent in sentences:\n    for word in sent:\n        word_freq[word]+=1\n\nprint('Number of unique words in the corpus', len(word_freq))","execution_count":15,"outputs":[{"output_type":"stream","text":"Number of unique words in the corpus 30178\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Sort python dictionary. Refer this [link](https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value)\nThere are multiple ways:\n- for python 3: sorted(x.items(), lambda kv: kv[1]) - to sort by values\n- for python 3: sorted(x.items(), lambda kv: kv[0]) - to sort by key"},{"metadata":{"trusted":true},"cell_type":"code","source":"#sorted(word_freq.items(), key = lambda kv: kv[1])[::-1]\nprint(sorted(word_freq.items(), key = lambda kv: kv[1], reverse=True)[:10])\n\nprint('\\nthe most common words are, taking 10 for instance\\n', sorted(word_freq, key = word_freq.get, reverse=True)[:10])","execution_count":16,"outputs":[{"output_type":"stream","text":"[('oh', 6453), ('like', 5599), ('know', 4819), ('get', 4197), ('hey', 3620), ('think', 3594), ('right', 3406), ('look', 3375), ('want', 3181), ('come', 3161)]\n\nthe most common words are, taking 10 for instance\n ['oh', 'like', 'know', 'get', 'hey', 'think', 'right', 'look', 'want', 'come']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 3. Training the model\n\n### Gensim Word2Vec implementation\nImplementing Gensim word2vec as mentioned [here](https://radimrehurek.com/gensim/models/word2vec.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training of the model will be done in 3 steps:\n\n1. `Word2Vec`: \n    > setting up the parameters one by one without supplying the parameter `sentences` and therefore leave the model uninitialized, purposefully.\n2. `.build_vocab()`: \n    > builds vocabulary from the sentence sequence and thus initialize the model. With the loggings, I can follow the progress and even more important, the effect of min_count and sample on the word corpus. I noticed that these two parameters, and in particular sample, have a great influence over the performance of a model. Displaying both allows for a more accurate and an easier management of their influence.\n3. `.train()`: \n    > trains the model. The loggings here useful for monitoring and making sure not threads execute simultaneously"},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting the number of cores on the machine to find how many threads can be executed in parallel\ncores = multiprocessing.cpu_count()","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word2Vec parameters\n\n* `min_count`: ignore words with frequency less than this\n* `window`: the max distance between the current words and predicted word in a sentence\n* `size`: dimensionality of feature vector\n* `sample`: The threshold for configuring which higher-frequency words are randomly downsampled\n* `alpha`: The initial learning rate\n* `min_alpha` :Learning rate will linearly drop to `min_alpha` as training progresses\n* `max_vocab_size`: Limits the RAM during vocabulary building; if there are more unique words than this, then prune the infrequent ones\n* `negative` :  If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"should be drawn (usually between 5-20)\n* `workers`: number of threads to train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create an instance of Word2Vec without intializing it using sequence of sentences\n\nw2v_model = Word2Vec(alpha = 0.03, \n                     window = 2, \n                     min_count = 20, \n                     sample = 6e-5,\n                     min_alpha = 0.0007,\n                     negative = 20,\n                     workers = cores\n                     )","execution_count":19,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build the vocabulary table\n\nWord2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them):"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\n\nw2v_model.build_vocab(sentences, progress_per = 10000)\nprint('time taken to build vocabulary is {} min'.format((time.time() - start) / 60))","execution_count":20,"outputs":[{"output_type":"stream","text":"time taken to build vocabulary is 0.0460463007291158 min\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Train the model\n\n* total_examples = int - Count of sentences;\n* epochs = int - Number of iterations (epochs) over the corpus - [10, 20, 30]"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\n\nw2v_model.train(sentences, total_examples = w2v_model.corpus_count, epochs= 30, report_delay=10)\nprint('time taken to train the model is {} min'.format((time.time() - start) / 60))","execution_count":21,"outputs":[{"output_type":"stream","text":"time taken to train the model is 1.2311998883883157 min\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"As we do not plan to train the model any further, we are calling init_sims(), which will make the model much more memory-efficient.\nThis will replace the weights trained with their normalized values. This step is irreversible"},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.init_sims(replace=True)","execution_count":22,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Explore the model"},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Most similar to"},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar('homer')","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"[('rude', 0.7671548128128052),\n ('bongo', 0.7588388919830322),\n ('wife', 0.7429685592651367),\n ('marge', 0.7415707111358643),\n ('snuggle', 0.734971284866333),\n ('worry', 0.7316248416900635),\n ('attract', 0.7215455770492554),\n ('sweetheart', 0.7214065790176392),\n ('adopt', 0.7213941812515259),\n ('dr_hibbert', 0.7210967540740967)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar('homer_simpson')","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"[('recent', 0.756310224533081),\n ('hutz', 0.7376375198364258),\n ('easily', 0.737457275390625),\n ('governor', 0.7340445518493652),\n ('congratulation', 0.7321300506591797),\n ('simon', 0.7197277545928955),\n ('waylon', 0.7154708504676819),\n ('robert', 0.7027060985565186),\n ('erotic', 0.6951093673706055),\n ('council', 0.692208468914032)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar('bart')","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"[('lisa', 0.828191876411438),\n ('homework', 0.7757643461227417),\n ('mom', 0.7673578262329102),\n ('strangle', 0.7603084444999695),\n ('grown', 0.7595598697662354),\n ('substitute', 0.7528258562088013),\n ('convince', 0.7512975931167603),\n ('ralphie', 0.7400736808776855),\n ('surprised', 0.7386853694915771),\n ('badly', 0.7379380464553833)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Similarities between words"},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.similarity('bart_simpson', 'child')","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"0.57327795"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.similarity('maggie', 'baby')","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"0.73507625"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 Finding the odd one out\nFind the word that does not belong to the list"},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.doesnt_match(['homer', 'bart', 'maggie', 'hibbert'])","execution_count":28,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n","name":"stderr"},{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"'hibbert'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.doesnt_match(['bart', 'milhouse', 'nelson'])","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"'nelson'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### 4.4 Analogy difference\n\nWhich word is to woman as homer is to marge ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(positive=['woman', 'homer'], negative=['marge'], topn=3)","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"[('admire', 0.6312659978866577),\n ('wife', 0.6125058531761169),\n ('wonder', 0.6094111204147339)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What word is to woman as bart is to man ?\nw2v_model.wv.most_similar(positive=['woman', 'bart'], negative=['man'], topn=3)","execution_count":36,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"[('pregnant', 0.717870831489563),\n ('parent', 0.7156802415847778),\n ('lisa', 0.6839100122451782)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}